{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd3b7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created scraper with base URL: https://growthlab.hks.harvard.edu/publications\n",
      "Scraper configuration: {'base_url': 'https://growthlab.hks.harvard.edu/publications', 'scrape_delay': 1.0, 'concurrency_limit': 1, 'max_retries': 3, 'retry_base_delay': 1.0, 'retry_max_delay': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 1: Setup and Basic Configuration\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This allows running async code in Jupyter\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create a simple placeholder for GrowthLabPublication\n",
    "class SimpleGrowthLabPublication(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def generate_id(self):\n",
    "        # Simple hash of title\n",
    "        title = self.get(\"title\", \"\")\n",
    "        return f\"pub_{hash(title) % 10000}\"\n",
    "        \n",
    "    def generate_content_hash(self):\n",
    "        # Simple hash of content\n",
    "        content = json.dumps(self, sort_keys=True)\n",
    "        return f\"hash_{hash(content) % 10000}\"\n",
    "        \n",
    "    @property\n",
    "    def paper_id(self):\n",
    "        return self.get(\"paper_id\", \"\")\n",
    "        \n",
    "    @paper_id.setter\n",
    "    def paper_id(self, value):\n",
    "        self[\"paper_id\"] = value\n",
    "        \n",
    "    @property\n",
    "    def content_hash(self):\n",
    "        return self.get(\"content_hash\", \"\")\n",
    "        \n",
    "    @content_hash.setter\n",
    "    def content_hash(self, value):\n",
    "        self[\"content_hash\"] = value\n",
    "        \n",
    "    @property\n",
    "    def title(self):\n",
    "        return self.get(\"title\", \"\")\n",
    "        \n",
    "    @title.setter\n",
    "    def title(self, value):\n",
    "        self[\"title\"] = value\n",
    "        \n",
    "    @property\n",
    "    def authors(self):\n",
    "        return self.get(\"authors\", \"\")\n",
    "        \n",
    "    @authors.setter\n",
    "    def authors(self, value):\n",
    "        self[\"authors\"] = value\n",
    "        \n",
    "    @property\n",
    "    def abstract(self):\n",
    "        return self.get(\"abstract\", \"\")\n",
    "        \n",
    "    @abstract.setter\n",
    "    def abstract(self, value):\n",
    "        self[\"abstract\"] = value\n",
    "        \n",
    "    @property\n",
    "    def pub_url(self):\n",
    "        return self.get(\"pub_url\", \"\")\n",
    "        \n",
    "    @property\n",
    "    def file_urls(self):\n",
    "        return self.get(\"file_urls\", [])\n",
    "\n",
    "# Simplified retry function\n",
    "async def retry_with_backoff(func, *args, max_retries=3, base_delay=1.0, max_delay=30.0, retry_on=Exception, **kwargs):\n",
    "    \"\"\"Retry a function with exponential backoff\"\"\"\n",
    "    retries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            return await func(*args, **kwargs)\n",
    "        except retry_on as e:\n",
    "            if retries >= max_retries:\n",
    "                raise\n",
    "            delay = min(base_delay * (2 ** retries), max_delay)\n",
    "            print(f\"Retrying after error: {e}. Attempt {retries+1}/{max_retries}. Waiting {delay:.2f}s...\")\n",
    "            await asyncio.sleep(delay)\n",
    "            retries += 1\n",
    "\n",
    "# Create a simple scraper with just configuration\n",
    "class SimpleGrowthLabScraper:\n",
    "    \"\"\"Simplified version of the Growth Lab scraper for testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load default configuration\n",
    "        self.config = {\n",
    "            \"base_url\": \"https://growthlab.hks.harvard.edu/publications\",\n",
    "            \"scrape_delay\": 1.0,  # Reduced for testing\n",
    "            \"concurrency_limit\": 1,  # Reduced for testing\n",
    "            \"max_retries\": 3,\n",
    "            \"retry_base_delay\": 1.0,\n",
    "            \"retry_max_delay\": 10.0,\n",
    "        }\n",
    "        \n",
    "        self.base_url = self.config[\"base_url\"]\n",
    "        self.scrape_delay = self.config[\"scrape_delay\"]\n",
    "        self.concurrency_limit = self.config[\"concurrency_limit\"]\n",
    "        self.semaphore = asyncio.Semaphore(self.concurrency_limit)\n",
    "        \n",
    "        # Some sample year corrections (from original scraper)\n",
    "        self.year_corrections = {\n",
    "            \"https://growthlab.hks.harvard.edu/publications/economic-complexity-brief\": 2013,\n",
    "            \"https://growthlab.hks.harvard.edu/publications/colombia-atlas-economic-complexity-datlas\": 2014,\n",
    "        }\n",
    "        \n",
    "# Create our scraper instance\n",
    "scraper = SimpleGrowthLabScraper()\n",
    "print(f\"Created scraper with base URL: {scraper.base_url}\")\n",
    "print(f\"Scraper configuration: {scraper.config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d415c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching pagination information from: https://growthlab.hks.harvard.edu/publications\n",
      "Found pagination element on the page\n",
      "Found last page URL: https://growthlab.hks.harvard.edu/publications?page=21\n",
      "Extracted max page number: 21\n",
      "\n",
      "Maximum page number: 21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def get_max_page_num(url):\n",
    "    \"\"\"Get the maximum page number from pagination\"\"\"\n",
    "    print(f\"Fetching pagination information from: {url}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page: {response.status}\")\n",
    "                return 0\n",
    "                \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            pagination = soup.find(\"ul\", {\"class\": \"pager\"})\n",
    "            \n",
    "            if pagination:\n",
    "                print(\"Found pagination element on the page\")\n",
    "                last_page_link = pagination.find(\"li\", {\"class\": \"pager-last\"})\n",
    "                if last_page_link and last_page_link.find(\"a\"):\n",
    "                    last_page_url = last_page_link.find(\"a\").get(\"href\")\n",
    "                    print(f\"Found last page URL: {last_page_url}\")\n",
    "                    \n",
    "                    match = re.search(r\"\\d+\", last_page_url)\n",
    "                    if match:\n",
    "                        max_page = int(match.group())\n",
    "                        print(f\"Extracted max page number: {max_page}\")\n",
    "                        return max_page\n",
    "            \n",
    "            print(\"Could not find pagination or determine max page\")\n",
    "            return 0\n",
    "\n",
    "# Run the function\n",
    "max_page = await get_max_page_num(scraper.base_url)\n",
    "print(f\"\\nMaximum page number: {max_page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4061fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a publication element, parsing...\n",
      "\n",
      "Successfully parsed a publication:\n",
      "Title: Global Networks, Monetary Policy and Trade\n",
      "Authors: Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\n",
      "Year: 2025\n",
      "Abstract: We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n",
      "URL: https://growthlab.hks.harvard.edu/publications/global-networks-monetary-policy-and-trade\n",
      "File URLs: []\n",
      "ID: pub_6494\n",
      "Content Hash: hash_8040\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 3: Fetch and Parse a Publication\n",
    "\n",
    "async def fetch_and_parse_publication():\n",
    "    \"\"\"Fetch the first page and parse a single publication\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(scraper.base_url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page: {response.status}\")\n",
    "                return None\n",
    "            \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Find the first publication element\n",
    "            pub_element = soup.find(\"div\", {\"class\": \"biblio-entry\"})\n",
    "            if not pub_element:\n",
    "                print(\"No publication found\")\n",
    "                return None\n",
    "            \n",
    "            print(\"Found a publication element, parsing...\")\n",
    "            \n",
    "            # Extract data using the same CSS selectors as in parse_publication\n",
    "            title_element = pub_element.find(\"span\", {\"class\": \"biblio-title\"})\n",
    "            title = title_element.text.strip() if title_element else None\n",
    "            \n",
    "            title_link = title_element.find(\"a\") if title_element else None\n",
    "            pub_url = title_link.get(\"href\") if title_link else None\n",
    "            \n",
    "            # Ensure URL is absolute\n",
    "            if pub_url and not pub_url.startswith((\"http://\", \"https://\")):\n",
    "                pub_url = f\"{scraper.base_url.split('/publications')[0]}{pub_url}\"\n",
    "            \n",
    "            authors_element = pub_element.find(\"span\", {\"class\": \"biblio-authors\"})\n",
    "            authors = authors_element.text.strip() if authors_element else None\n",
    "            \n",
    "            # Extract year\n",
    "            year = None\n",
    "            if authors_element:\n",
    "                sibling_text = authors_element.next_sibling\n",
    "                if sibling_text:\n",
    "                    year_match = re.search(r\"\\b\\d{4}\\b\", sibling_text)\n",
    "                    if year_match:\n",
    "                        year = int(year_match.group())\n",
    "            \n",
    "            # Apply year correction if available\n",
    "            if pub_url in scraper.year_corrections:\n",
    "                year = scraper.year_corrections[pub_url]\n",
    "            \n",
    "            abstract_element = pub_element.find(\"div\", {\"class\": \"biblio-abstract-display\"})\n",
    "            abstract = abstract_element.text.strip() if abstract_element else None\n",
    "            \n",
    "            # Get file URLs\n",
    "            file_urls = []\n",
    "            for file_elem in pub_element.find_all(\"span\", {\"class\": \"file\"}):\n",
    "                file_link = file_elem.find(\"a\")\n",
    "                if file_link and file_link.get(\"href\"):\n",
    "                    file_url = file_link[\"href\"]\n",
    "                    # Ensure URL is absolute\n",
    "                    if not file_url.startswith((\"http://\", \"https://\")):\n",
    "                        file_url = f\"{scraper.base_url.split('/publications')[0]}{file_url}\"\n",
    "                    file_urls.append(file_url)\n",
    "            \n",
    "            # Create a publication object\n",
    "            pub = SimpleGrowthLabPublication(\n",
    "                title=title,\n",
    "                authors=authors,\n",
    "                year=year,\n",
    "                abstract=abstract,\n",
    "                pub_url=pub_url,\n",
    "                file_urls=file_urls,\n",
    "                source=\"GrowthLab\",\n",
    "            )\n",
    "            \n",
    "            # Generate stable ID and content hash\n",
    "            pub.paper_id = pub.generate_id()\n",
    "            pub.content_hash = pub.generate_content_hash()\n",
    "            \n",
    "            return pub\n",
    "\n",
    "# Run the function\n",
    "publication = await fetch_and_parse_publication()\n",
    "\n",
    "if publication:\n",
    "    print(\"\\nSuccessfully parsed a publication:\")\n",
    "    print(f\"Title: {publication['title']}\")\n",
    "    print(f\"Authors: {publication['authors']}\")\n",
    "    print(f\"Year: {publication['year']}\")\n",
    "    print(f\"Abstract: {publication['abstract'][:100]}...\" if publication['abstract'] else \"No abstract\")\n",
    "    print(f\"URL: {publication['pub_url']}\")\n",
    "    print(f\"File URLs: {publication['file_urls']}\")\n",
    "    print(f\"ID: {publication['paper_id']}\")\n",
    "    print(f\"Content Hash: {publication['content_hash']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f15c4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed: 42\n",
      "Fetching publications from page 20: https://growthlab.hks.harvard.edu/publications?page=20\n",
      "Found 20 publications on page 20\n",
      "Selected 3 random publications for analysis\n",
      "\n",
      "Publications fetched from random page:\n",
      "\n",
      "Publication 1:\n",
      "Title: Growth Diagnostic: Paraguay\n",
      "Authors: Hausmann, R. & Klinger, B.\n",
      "Abstract: Paraguay’s growth history is characterized by prol...\n",
      "\n",
      "Publication 2:\n",
      "Title: Uncertainty in the Search for New Exports\n",
      "Authors: Klinger, B.\n",
      "Abstract: This paper explores the role that uncertainty play...\n",
      "\n",
      "Publication 3:\n",
      "Title: Growth Collapses\n",
      "Authors: Hausmann, R., Rodríguez, F. & Wagner, R.\n",
      "Abstract: We study episodes where economic growth decelerate...\n",
      "Fetching publications from page 6: https://growthlab.hks.harvard.edu/publications?page=6\n",
      "Found 20 publications on page 6\n",
      "Selected 3 random publications for analysis\n",
      "\n",
      "Publications fetched from page 6:\n",
      "\n",
      "Publication 1:\n",
      "Title: Buscando virtudes en la lejanía: Recomendaciones de política para promover el crecimiento inclusivo y sostenible en Loreto, Peru\n",
      "Authors: Hausmann, R., et al.\n",
      "Abstract: Loreto es un lugar de contrastes. Es el departamen...\n",
      "\n",
      "Publication 2:\n",
      "Title: Knowledge Diffusion in the Network of International Business Travel\n",
      "Authors: Coscia, M., Neffke, F. & Hausmann, R.\n",
      "Abstract: We use aggregated and anonymized information based...\n",
      "\n",
      "Publication 3:\n",
      "Title: Accelerating Growth in Albania through Targeted Investment Promotion\n",
      "Authors: O'Brien, T., et al.\n",
      "Abstract: The investment promotion process in Albania is und...\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 4: Fetch Publications from a Random Page\n",
    "\n",
    "import random\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "print(f\"Using random seed: {RANDOM_SEED}\")\n",
    "\n",
    "async def fetch_page(page_num=None):\n",
    "    \"\"\"Fetch a single page of publications\n",
    "    \n",
    "    Args:\n",
    "        page_num: Page number to fetch, or None to choose a random page\n",
    "    \"\"\"\n",
    "    # If max_page was found in Chunk 2, use it as the upper bound for random selection\n",
    "    # Otherwise, just pick a number between 1 and 10 (reasonable guess)\n",
    "    upper_bound = max_page if 'max_page' in globals() and max_page > 0 else 10\n",
    "    \n",
    "    if page_num is None:\n",
    "        # Choose a random page between 0 and upper_bound\n",
    "        page_num = random.randint(0, upper_bound)\n",
    "    \n",
    "    url = scraper.base_url if page_num == 0 else f\"{scraper.base_url}?page={page_num}\"\n",
    "    print(f\"Fetching publications from page {page_num}: {url}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    }\n",
    "    \n",
    "    publications = []\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page {page_num}: {response.status}\")\n",
    "                return []\n",
    "            \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            pub_elements = soup.find_all(\"div\", {\"class\": \"biblio-entry\"})\n",
    "            \n",
    "            print(f\"Found {len(pub_elements)} publications on page {page_num}\")\n",
    "            \n",
    "            # Pick a random subset of publications (3 maximum, or all if fewer than 3)\n",
    "            sample_size = min(3, len(pub_elements))\n",
    "            if sample_size > 0:\n",
    "                selected_indices = random.sample(range(len(pub_elements)), sample_size)\n",
    "                selected_pubs = [pub_elements[i] for i in selected_indices]\n",
    "            else:\n",
    "                selected_pubs = []\n",
    "            \n",
    "            print(f\"Selected {len(selected_pubs)} random publications for analysis\")\n",
    "            \n",
    "            # Process the selected publications\n",
    "            for i, pub_element in enumerate(selected_pubs):\n",
    "                # Extract data (same as before)\n",
    "                title_element = pub_element.find(\"span\", {\"class\": \"biblio-title\"})\n",
    "                title = title_element.text.strip() if title_element else None\n",
    "                \n",
    "                title_link = title_element.find(\"a\") if title_element else None\n",
    "                pub_url = title_link.get(\"href\") if title_link else None\n",
    "                \n",
    "                # Ensure URL is absolute\n",
    "                if pub_url and not pub_url.startswith((\"http://\", \"https://\")):\n",
    "                    pub_url = f\"{scraper.base_url.split('/publications')[0]}{pub_url}\"\n",
    "                \n",
    "                authors_element = pub_element.find(\"span\", {\"class\": \"biblio-authors\"})\n",
    "                authors = authors_element.text.strip() if authors_element else None\n",
    "                \n",
    "                # Extract year\n",
    "                year = None\n",
    "                if authors_element:\n",
    "                    sibling_text = authors_element.next_sibling\n",
    "                    if sibling_text:\n",
    "                        year_match = re.search(r\"\\b\\d{4}\\b\", sibling_text)\n",
    "                        if year_match:\n",
    "                            year = int(year_match.group())\n",
    "                \n",
    "                abstract_element = pub_element.find(\"div\", {\"class\": \"biblio-abstract-display\"})\n",
    "                abstract = abstract_element.text.strip() if abstract_element else None\n",
    "                \n",
    "                # Get file URLs\n",
    "                file_urls = []\n",
    "                for file_elem in pub_element.find_all(\"span\", {\"class\": \"file\"}):\n",
    "                    file_link = file_elem.find(\"a\")\n",
    "                    if file_link and file_link.get(\"href\"):\n",
    "                        file_url = file_link[\"href\"]\n",
    "                        # Ensure URL is absolute\n",
    "                        if not file_url.startswith((\"http://\", \"https://\")):\n",
    "                            file_url = f\"{scraper.base_url.split('/publications')[0]}{file_url}\"\n",
    "                        file_urls.append(file_url)\n",
    "                \n",
    "                pub = SimpleGrowthLabPublication(\n",
    "                    title=title,\n",
    "                    authors=authors,\n",
    "                    year=year,\n",
    "                    abstract=abstract,\n",
    "                    pub_url=pub_url,\n",
    "                    file_urls=file_urls,\n",
    "                    source=\"GrowthLab\",\n",
    "                )\n",
    "                \n",
    "                # Generate stable ID and content hash\n",
    "                pub.paper_id = pub.generate_id()\n",
    "                pub.content_hash = pub.generate_content_hash()\n",
    "                \n",
    "                publications.append(pub)\n",
    "                \n",
    "            # In the real scraper, this would be controlled by semaphore\n",
    "            await asyncio.sleep(scraper.scrape_delay)\n",
    "            \n",
    "            return publications\n",
    "\n",
    "# Run the function to fetch from a random page\n",
    "random_page_publications = await fetch_page()\n",
    "\n",
    "print(\"\\nPublications fetched from random page:\")\n",
    "for i, pub in enumerate(random_page_publications):\n",
    "    print(f\"\\nPublication {i+1}:\")\n",
    "    print(f\"Title: {pub['title']}\")\n",
    "    print(f\"Authors: {pub['authors']}\")\n",
    "    print(f\"Abstract: {pub['abstract'][:50]}...\" if pub['abstract'] else \"No abstract\")\n",
    "\n",
    "# Also fetch a specific page (page 6) for comparison\n",
    "specific_page_publications = await fetch_page(6)\n",
    "\n",
    "print(\"\\nPublications fetched from page 6:\")\n",
    "for i, pub in enumerate(specific_page_publications):\n",
    "    print(f\"\\nPublication {i+1}:\")\n",
    "    print(f\"Title: {pub['title']}\")\n",
    "    print(f\"Authors: {pub['authors']}\")\n",
    "    print(f\"Abstract: {pub['abstract'][:50]}...\" if pub['abstract'] else \"No abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dec7ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Air Transportation and Regional Economic Development: A Case Study for the New Airport in South Albania',\n",
       "  'authors': 'Gadgin Matha, S., Goldstein, P. & Lu, J.',\n",
       "  'year': 2020,\n",
       "  'abstract': 'Considering the case of the proposed airport in Vlora, South Albania, this report analyzes the channels through which a new greenfield airport can contribute to regional economic development. In December 2019, the Government of Albania opened a call for offers to build a new airport in the south of the country. While there is evidence indicating that the airport could be commercially viable, this does not provide a grounded perspective on the channels by which the airport could boost the regional economy. To evaluate how the new airport would interact with existing and potential economic activities, this report evaluates three of the most important channels of impact by which the airport could serve as a promoter: (1) economic activities directly related to or promoted by airports, (2) the airport’s potential contribution to the region’s booming tourism sector and (3) the potential for the country’s development of air freight as a tool for export promotion. In each of these three cases, the report identifies complementary public goods or policies that could maximize the airport’s impact in the region.\\n\\n\\n\\tThe operation of the airport itself could stimulate a series of economic activities directly related to air traffic services. Airports have the ability to mold the economic structure of the places immediately around them, acting both as a consumer and as a supplier of air transport services. Not only activities related to transportation and logistics thrive around airports, but also a variety of manufacturing, trade and construction industries. Nevertheless, the agglomeration benefits of a successful aerotropolis are not guaranteed by the construction of an airport. For South Albania’s new airport to actualize its potential returns, integrated planning of the airport site will be required, with focus on real estate planification and provision of complementary infrastructure.\\n\\n\\n\\tEstablishing an airport in Vlora has the potential to spur regional development in South Albania through facilitating the growth of the tourism sector and its related activities. Albania’s tourism industry has seen strong growth in the last two decades, but still lags behind its potential. Albania only has a strong penetration in the tourism market of its neighboring markets, and the high seasonality of the tourism season further limits the sector’s growth. The establishment of an airport in South Albania would ease some of the tourism industry constraints tied to transportation into the country and region. Given the high reliance of the tourism industry on its many complementary inputs, more than one area of concern may have to be addressed for the impact of the new airport to be maximized. Facilitating transportation access around the South Albania region and specifically to tourist sites; preparing natural and cultural heritage sites for tourism use and expanding tourism infrastructure to accommodate potential growth are some of the interventions analyzed.\\n\\n\\n\\tAirfreight infrastructure could in theory provide opportunities to improve the competitiveness of Albanian exports but developing a successful air cargo cluster is no simple task. An airport can facilitate an alternative mode of transport for specific types of goods and hence promote a country’s exports. In Albania’s case, not only existing textile and agriculture products could be competitively exported through air freight, but also air freight itself could improve Albania’s position to diversify into “nearby” industries, identified by the theory of Economic Complexity. Nevertheless, an effective air freight strategy does not and cannot uniquely depend on the simple availability of a nearby airport. Air cargo operations require both traffic volume that Albania may not be able to provide, as well as complementary cargo-specific infrastructure. Although the potential for air freight in South Albania could be high, it is by no means a safe bet nor does it imply with certainty significant impact in the immediate future.',\n",
       "  'pub_url': 'https://growthlab.hks.harvard.edu/publications/air-transportation-and-regional-economic-development-case-study-new-airport',\n",
       "  'file_urls': ['https://growthlab.hks.harvard.edu/sites/projects.iq.harvard.edu/files/growthlab/files/2020-06-cid-fellows-wp-127-albania-air-transport.pdf'],\n",
       "  'source': 'GrowthLab',\n",
       "  'paper_id': 'pub_3032',\n",
       "  'content_hash': 'hash_3585'},\n",
       " {'title': 'The Value of Complementary Coworkers',\n",
       "  'authors': 'Neffke, F.',\n",
       "  'year': 2019,\n",
       "  'abstract': 'As individuals specialize in specific knowledge areas, a society’s know-how becomes distributed across different workers. To use this distributed know-how, workers must be coordinated into teams that, collectively, can cover a wide range of expertise. This paper studies the interdependencies among co-workers that result from this process in a population-wide dataset covering educational specializations of millions of workers and their co-workers in Sweden over a 10-year period. The analysis shows that the value of what a person knows depends on whom that person works with. Whereas having co-workers with qualifications similar to one’s own is costly, having co-workers with complementary qualifications is beneficial. This co-worker complementarity increases over a worker’s career and offers a unifying framework to explain seemingly disparate observations, answering questions such as “Why do returns to education differ so widely?” “Why do workers earn higher wages in large establishments?” “Why are wages so high in large cities?”\\n\\n\\n\\tAdditional resources: Website |\\xa0Podcast\\xa0| Video | Media Release',\n",
       "  'pub_url': 'https://growthlab.hks.harvard.edu/publications/value-complementary-coworkers',\n",
       "  'file_urls': [],\n",
       "  'source': 'GrowthLab',\n",
       "  'paper_id': 'pub_9873',\n",
       "  'content_hash': 'hash_4226'},\n",
       " {'title': 'Taxation, Innovation, and Entrepreneurship',\n",
       "  'authors': 'Schetter, U., Gersbach, H. & Schneider, M.T.',\n",
       "  'year': 2019,\n",
       "  'abstract': 'We explore optimal and politically feasible growth policies consisting of basic research investments and taxation. We show that the impact of basic research on the general economy rationalises a taxation pecking order with high labour taxes and low profit taxes. This scheme induces a significant proportion of agents to become entrepreneurs, thereby rationalising substantial investments in basic research fostering their innovation prospects. These entrepreneurial economies, however, may make a majority of workers worse off, giving rise to a conflict between efficiency and equality. We discuss ways of mitigating this conflict, and thus strengthening political support for growth policies.',\n",
       "  'pub_url': 'https://growthlab.hks.harvard.edu/publications/taxation-innovation-and-entrepreneurship',\n",
       "  'file_urls': [],\n",
       "  'source': 'GrowthLab',\n",
       "  'paper_id': 'pub_7639',\n",
       "  'content_hash': 'hash_5965'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_page_publications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "936f4d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:XPath selector failed: Invalid character '/' position 0\n",
      "  line 1:\n",
      "//span[contains(@class, 'file')]\n",
      "^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Testing configurable selectors system...\n",
      "Finding publication containers...\n",
      "Found 20 publication containers\n",
      "\n",
      "Testing element selectors within first publication:\n",
      "- Title: Global Networks, Monetary Policy and Trade\n",
      "- Authors: Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\n",
      "- Abstract: We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n",
      "- File URLs: []\n",
      "\n",
      "Testing pagination selectors:\n",
      "Found pagination container\n",
      "Found last page URL: https://growthlab.hks.harvard.edu/publications?page=21\n",
      "\n",
      "🔍 Testing publication parser with configurable selectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Found 20 publication containers\n",
      "WARNING:root:XPath selector failed: Invalid character '/' position 0\n",
      "  line 1:\n",
      "//span[contains(@class, 'file')]\n",
      "^\n",
      "WARNING:root:XPath selector failed: Invalid character '/' position 0\n",
      "  line 1:\n",
      "//span[contains(@class, 'file')]\n",
      "^\n",
      "WARNING:root:XPath selector failed: Invalid character '/' position 0\n",
      "  line 1:\n",
      "//span[contains(@class, 'file')]\n",
      "^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum page number: 21\n",
      "Parsed 20 publications\n",
      "\n",
      "First 3 parsed publications:\n",
      "\n",
      "1. Global Networks, Monetary Policy and Trade\n",
      "   Authors: Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\n",
      "   Year: 2025\n",
      "   Abstract: We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n",
      "   URL: https://growthlab.hks.harvard.edu/publications/global-networks-monetary-policy-and-trade\n",
      "   Files: []\n",
      "   ID: pub_6494\n",
      "\n",
      "2. Industrial policy for competitiveness in the energy transition\n",
      "   Authors: Ahuja, K. & Hausmann, R.\n",
      "   Year: 2025\n",
      "   Abstract: Green objectives have reshaped public policy worldwide since the signing in 2015 of the Paris Agreem...\n",
      "   URL: https://growthlab.hks.harvard.edu/publications/industrial-policy-competitiveness-energy-transition\n",
      "   Files: ['https://growthlab.hks.harvard.edu/sites/projects.iq.harvard.edu/files/bruegel_blueprint_34_0.pdf']\n",
      "   ID: pub_4859\n",
      "\n",
      "3. Public-Private Dialogs to Spur Export-led Growth: The Case of Productivity Taskforces in Namibia\n",
      "   Authors: Fortunato, A. & Santos, M.A.\n",
      "   Year: 2025\n",
      "   Abstract: This case study examines the implementation of Namibia's first Productivity Task Force focused on th...\n",
      "   URL: https://growthlab.hks.harvard.edu/publications/public-private-dialogs-spur-export-led-growth-case-productivity-taskforces\n",
      "   Files: ['https://growthlab.hks.harvard.edu/sites/projects.iq.harvard.edu/files/public-private-dialogs-to-spur-export-led-growth.pdf']\n",
      "   ID: pub_3727\n",
      "\n",
      "📡 Testing selector monitoring system...\n",
      "\n",
      "🔍 Selector Health Check:\n",
      "\n",
      "PUBLICATION Selectors:\n",
      "  - publication.container: ✅ GOOD (2/2, 100.0%)\n",
      "  - publication.title: ✅ GOOD (2/2, 100.0%)\n",
      "  - publication.authors: ⚪ NO DATA\n",
      "  - publication.abstract: ❌ FAILING (0/1, 0.0%)\n",
      "  - publication.file: ❌ FAILING (1/4, 25.0%)\n",
      "  - publication.year: ⚪ NO DATA\n",
      "\n",
      "PAGINATION Selectors:\n",
      "  - pagination.container: ⚪ NO DATA\n",
      "  - pagination.last_page: ⚪ NO DATA\n",
      "\n",
      "ENDNOTE Selectors:\n",
      "  - endnote.link: ⚪ NO DATA\n",
      "\n",
      "📊 Selector Performance Report\n",
      "Pages processed: 2\n",
      "Publications processed: 3\n",
      "Overall selector success rate: 55.56%\n",
      "\n",
      "Selector Performance:\n",
      "  - publication.abstract: 0.00% success (0/1)\n",
      "  - publication.container: 100.00% success (2/2)\n",
      "  - publication.file: 25.00% success (1/4)\n",
      "  - publication.title: 100.00% success (2/2)\n",
      "\n",
      "✅ No selector alerts detected\n",
      "\n",
      "🔄 Integration Example: Using the monitor with the parser\n",
      "\n",
      "# Example integration in production code:\n",
      "\n",
      "async def parse_with_monitoring(html, parser, monitor):\n",
      "    # Record that we're processing a page\n",
      "    monitor.record_page_processed()\n",
      "\n",
      "    # Parse the page\n",
      "    try:\n",
      "        publications = await parser.parse_page(html)\n",
      "        # Record successful parsing\n",
      "        monitor.record_success(\"publication\", \"container\")\n",
      "\n",
      "        # For each publication, record successful elements\n",
      "        for pub in publications:\n",
      "            monitor.record_publication_processed()\n",
      "            if pub.get(\"title\"):\n",
      "                monitor.record_success(\"publication\", \"title\")\n",
      "            else:\n",
      "                monitor.record_failure(\"publication\", \"title\")\n",
      "\n",
      "            # And so on for other elements...\n",
      "\n",
      "        return publications\n",
      "    except Exception as e:\n",
      "        # Record failure\n",
      "        monitor.record_failure(\"publication\", \"container\")\n",
      "        logging.error(f\"Error parsing page: {e}\")\n",
      "        return []\n",
      "\n",
      "\n",
      "✅ Implementation complete!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# CHUNK 10: Implement Configurable CSS Selectors with Fallbacks\n",
    "\n",
    "# Define a configuration class for selectors that includes primary and fallback options\n",
    "class SelectorConfig:\n",
    "    def __init__(self, primary=None, fallbacks=None, xpath=None, description=None):\n",
    "        self.primary = primary\n",
    "        self.fallbacks = fallbacks or []\n",
    "        self.xpath = xpath  # XPath alternative\n",
    "        self.description = description or \"Element\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.description} ({self.primary})\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"SelectorConfig(primary='{self.primary}', fallbacks={self.fallbacks}, xpath='{self.xpath}')\"\n",
    "\n",
    "# Define the selector configuration with primary selectors, fallbacks, and XPath alternatives\n",
    "SELECTOR_CONFIG = {\n",
    "    \"publication\": {\n",
    "        \"container\": SelectorConfig(\n",
    "            primary=\"div.biblio-entry\", \n",
    "            fallbacks=[\"div.publication-item\", \"article.publication\"],\n",
    "            xpath=\"//div[contains(@class, 'biblio') and contains(@class, 'entry')]\",\n",
    "            description=\"Publication container\"\n",
    "        ),\n",
    "        \"title\": SelectorConfig(\n",
    "            primary=\"span.biblio-title\", \n",
    "            fallbacks=[\"h2.publication-title\", \"h3.title\", \"div.title\"],\n",
    "            xpath=\"//span[contains(@class, 'title')]\",\n",
    "            description=\"Publication title\"\n",
    "        ),\n",
    "        \"authors\": SelectorConfig(\n",
    "            primary=\"span.biblio-authors\", \n",
    "            fallbacks=[\"div.authors\", \"p.author-list\", \"div.publication-authors\"],\n",
    "            xpath=\"//span[contains(@class, 'authors')]\",\n",
    "            description=\"Publication authors\"\n",
    "        ),\n",
    "        \"abstract\": SelectorConfig(\n",
    "            primary=\"div.biblio-abstract-display\", \n",
    "            fallbacks=[\"div.abstract\", \"div.publication-abstract\", \"p.abstract\"],\n",
    "            xpath=\"//div[contains(@class, 'abstract')]\",\n",
    "            description=\"Publication abstract\"\n",
    "        ),\n",
    "        \"file\": SelectorConfig(\n",
    "            primary=\"span.file\", \n",
    "            fallbacks=[\"div.download-links a\", \"div.publication-files a\"],\n",
    "            xpath=\"//span[contains(@class, 'file')]\",\n",
    "            description=\"Publication files\"\n",
    "        ),\n",
    "        \"year\": SelectorConfig(\n",
    "            primary=None,  # Extracted from text, not direct selector\n",
    "            xpath=None,\n",
    "            description=\"Publication year\"\n",
    "        )\n",
    "    },\n",
    "    \"pagination\": {\n",
    "        \"container\": SelectorConfig(\n",
    "            primary=\"ul.pager\",\n",
    "            fallbacks=[\"div.pagination\", \"nav.pagination\"],\n",
    "            xpath=\"//ul[contains(@class, 'pager')]\",\n",
    "            description=\"Pagination container\"\n",
    "        ),\n",
    "        \"last_page\": SelectorConfig(\n",
    "            primary=\"li.pager-last\",\n",
    "            fallbacks=[\"li.page-item:last-child\", \"a.page-link:last-child\"],\n",
    "            xpath=\"//li[contains(@class, 'pager-last')]\",\n",
    "            description=\"Last page link\"\n",
    "        )\n",
    "    },\n",
    "    \"endnote\": {\n",
    "        \"link\": SelectorConfig(\n",
    "            primary=\"li.biblio_tagged a\",\n",
    "            fallbacks=[\"a.endnote-link\", \"a[href*='endnote']\", \"a[href*='tagged']\"],\n",
    "            xpath=\"//li[contains(@class, 'biblio_tagged')]//a\",\n",
    "            description=\"Endnote link\"\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a selector finder that tries multiple methods to find elements\n",
    "class SelectorFinder:\n",
    "    def __init__(self, soup=None):\n",
    "        self.soup = soup\n",
    "    \n",
    "    def set_soup(self, soup):\n",
    "        self.soup = soup\n",
    "    \n",
    "    def find(self, selector_config, soup=None, base_element=None):\n",
    "        \"\"\"Find an element using the configured selectors with fallbacks\"\"\"\n",
    "        if soup is None and base_element is None:\n",
    "            soup = self.soup\n",
    "        \n",
    "        search_element = base_element or soup\n",
    "        if not search_element:\n",
    "            logging.warning(f\"No soup or base element provided to find {selector_config}\")\n",
    "            return None\n",
    "        \n",
    "        # Try primary selector\n",
    "        if selector_config.primary:\n",
    "            element = self._try_css_selector(search_element, selector_config.primary)\n",
    "            if element:\n",
    "                return element\n",
    "        \n",
    "        # Try fallback selectors\n",
    "        for fallback in selector_config.fallbacks:\n",
    "            element = self._try_css_selector(search_element, fallback)\n",
    "            if element:\n",
    "                logging.info(f\"Using fallback selector {fallback} for {selector_config.description}\")\n",
    "                return element\n",
    "        \n",
    "        # Try XPath as last resort\n",
    "        if selector_config.xpath and hasattr(search_element, 'select'):  # For lxml / html5lib parser\n",
    "            try:\n",
    "                elements = search_element.select(selector_config.xpath)\n",
    "                if elements:\n",
    "                    logging.info(f\"Using XPath selector for {selector_config.description}\")\n",
    "                    return elements[0]\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"XPath selector failed: {e}\")\n",
    "        \n",
    "        logging.warning(f\"All selectors failed for {selector_config.description}\")\n",
    "        return None\n",
    "    \n",
    "    def find_all(self, selector_config, soup=None, base_element=None):\n",
    "        \"\"\"Find all elements using the configured selectors with fallbacks\"\"\"\n",
    "        if soup is None and base_element is None:\n",
    "            soup = self.soup\n",
    "        \n",
    "        search_element = base_element or soup\n",
    "        if not search_element:\n",
    "            return []\n",
    "        \n",
    "        # Try primary selector\n",
    "        if selector_config.primary:\n",
    "            elements = self._try_css_selector_all(search_element, selector_config.primary)\n",
    "            if elements:\n",
    "                return elements\n",
    "        \n",
    "        # Try fallback selectors\n",
    "        for fallback in selector_config.fallbacks:\n",
    "            elements = self._try_css_selector_all(search_element, fallback)\n",
    "            if elements:\n",
    "                logging.info(f\"Using fallback selector {fallback} for {selector_config.description}\")\n",
    "                return elements\n",
    "        \n",
    "        # Try XPath as last resort\n",
    "        if selector_config.xpath and hasattr(search_element, 'select'):\n",
    "            try:\n",
    "                elements = search_element.select(selector_config.xpath)\n",
    "                if elements:\n",
    "                    logging.info(f\"Using XPath selector for {selector_config.description}\")\n",
    "                    return elements\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"XPath selector failed: {e}\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _try_css_selector(self, element, selector):\n",
    "        \"\"\"Try to find an element with a CSS selector, handling different formats\"\"\"\n",
    "        try:\n",
    "            if \".\" in selector:  # Class selector\n",
    "                parts = selector.split(\".\")\n",
    "                tag = parts[0] if parts[0] else None\n",
    "                classes = parts[1].split()\n",
    "                return element.find(tag, class_=classes)\n",
    "            else:  # Tag selector\n",
    "                return element.find(selector)\n",
    "        except Exception as e:\n",
    "            logging.debug(f\"Selector {selector} failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _try_css_selector_all(self, element, selector):\n",
    "        \"\"\"Try to find all elements with a CSS selector, handling different formats\"\"\"\n",
    "        try:\n",
    "            if \".\" in selector:  # Class selector\n",
    "                parts = selector.split(\".\")\n",
    "                tag = parts[0] if parts[0] else None\n",
    "                classes = parts[1].split()\n",
    "                return element.find_all(tag, class_=classes)\n",
    "            else:  # Tag selector\n",
    "                return element.find_all(selector)\n",
    "        except Exception as e:\n",
    "            logging.debug(f\"Selector {selector} failed: {e}\")\n",
    "            return []\n",
    "\n",
    "# Initialize the selector finder\n",
    "selector_finder = SelectorFinder()\n",
    "\n",
    "# Test the configurable selector system\n",
    "async def test_configurable_selectors():\n",
    "    \"\"\"Test the new configurable selector system\"\"\"\n",
    "    print(\"\\n📋 Testing configurable selectors system...\")\n",
    "    \n",
    "    # Fetch a page\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(scraper.base_url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page: {response.status}\")\n",
    "                return\n",
    "            \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Set the soup in our selector finder\n",
    "            selector_finder.set_soup(soup)\n",
    "            \n",
    "            # Test finding containers\n",
    "            print(\"Finding publication containers...\")\n",
    "            containers = selector_finder.find_all(\n",
    "                SELECTOR_CONFIG[\"publication\"][\"container\"]\n",
    "            )\n",
    "            print(f\"Found {len(containers)} publication containers\")\n",
    "            \n",
    "            if containers:\n",
    "                # Test finding elements within a container\n",
    "                print(\"\\nTesting element selectors within first publication:\")\n",
    "                container = containers[0]\n",
    "                \n",
    "                # Find and print title\n",
    "                title_element = selector_finder.find(\n",
    "                    SELECTOR_CONFIG[\"publication\"][\"title\"], \n",
    "                    base_element=container\n",
    "                )\n",
    "                title = title_element.text.strip() if title_element else \"Title not found\"\n",
    "                print(f\"- Title: {title}\")\n",
    "                \n",
    "                # Find and print authors\n",
    "                authors_element = selector_finder.find(\n",
    "                    SELECTOR_CONFIG[\"publication\"][\"authors\"], \n",
    "                    base_element=container\n",
    "                )\n",
    "                authors = authors_element.text.strip() if authors_element else \"Authors not found\"\n",
    "                print(f\"- Authors: {authors}\")\n",
    "                \n",
    "                # Find and print abstract\n",
    "                abstract_element = selector_finder.find(\n",
    "                    SELECTOR_CONFIG[\"publication\"][\"abstract\"], \n",
    "                    base_element=container\n",
    "                )\n",
    "                abstract = abstract_element.text.strip() if abstract_element else \"Abstract not found\"\n",
    "                print(f\"- Abstract: {abstract[:100]}...\" if len(abstract) > 100 else f\"- Abstract: {abstract}\")\n",
    "                \n",
    "                # Find and print files\n",
    "                file_elements = selector_finder.find_all(\n",
    "                    SELECTOR_CONFIG[\"publication\"][\"file\"], \n",
    "                    base_element=container\n",
    "                )\n",
    "                file_urls = []\n",
    "                for file_elem in file_elements:\n",
    "                    file_link = file_elem.find(\"a\")\n",
    "                    if file_link and file_link.get(\"href\"):\n",
    "                        file_url = file_link[\"href\"]\n",
    "                        # Ensure URL is absolute\n",
    "                        if not file_url.startswith((\"http://\", \"https://\")):\n",
    "                            file_url = f\"{scraper.base_url.split('/publications')[0]}{file_url}\"\n",
    "                        file_urls.append(file_url)\n",
    "                \n",
    "                print(f\"- File URLs: {file_urls}\")\n",
    "                \n",
    "                # Test finding pagination\n",
    "                print(\"\\nTesting pagination selectors:\")\n",
    "                pagination = selector_finder.find(\n",
    "                    SELECTOR_CONFIG[\"pagination\"][\"container\"]\n",
    "                )\n",
    "                if pagination:\n",
    "                    print(\"Found pagination container\")\n",
    "                    \n",
    "                    # Find last page\n",
    "                    last_page_element = selector_finder.find(\n",
    "                        SELECTOR_CONFIG[\"pagination\"][\"last_page\"], \n",
    "                        base_element=pagination\n",
    "                    )\n",
    "                    \n",
    "                    if last_page_element and last_page_element.find(\"a\"):\n",
    "                        last_page_url = last_page_element.find(\"a\").get(\"href\")\n",
    "                        print(f\"Found last page URL: {last_page_url}\")\n",
    "                    else:\n",
    "                        print(\"Last page element not found\")\n",
    "                else:\n",
    "                    print(\"Pagination container not found\")\n",
    "    \n",
    "# Run the test\n",
    "await test_configurable_selectors()\n",
    "\n",
    "# %%\n",
    "# CHUNK 11: Implement Publication Parser with Configurable Selectors\n",
    "\n",
    "class PublicationParser:\n",
    "    \"\"\"Class to parse publications using configurable selectors\"\"\"\n",
    "    \n",
    "    def __init__(self, selectors=None, base_url=None):\n",
    "        self.selectors = selectors or SELECTOR_CONFIG\n",
    "        self.base_url = base_url or \"https://growthlab.hks.harvard.edu\"\n",
    "        self.finder = SelectorFinder()\n",
    "    \n",
    "    async def parse_page(self, html, url=None):\n",
    "        \"\"\"Parse a page of publications\"\"\"\n",
    "        if not html:\n",
    "            logging.warning(\"No HTML content provided to parse\")\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        self.finder.set_soup(soup)\n",
    "        \n",
    "        # Find all publication containers\n",
    "        containers = self.finder.find_all(self.selectors[\"publication\"][\"container\"])\n",
    "        \n",
    "        if not containers:\n",
    "            logging.warning(\"No publication containers found on page\")\n",
    "            return []\n",
    "        \n",
    "        logging.info(f\"Found {len(containers)} publication containers\")\n",
    "        \n",
    "        # Parse each publication\n",
    "        publications = []\n",
    "        for container in containers:\n",
    "            try:\n",
    "                publication = await self.parse_publication(container)\n",
    "                if publication:\n",
    "                    publications.append(publication)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error parsing publication: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return publications\n",
    "    \n",
    "    async def parse_publication(self, container):\n",
    "        \"\"\"Parse a single publication from a container element\"\"\"\n",
    "        # Extract basic metadata using selectors\n",
    "        title_element = self.finder.find(self.selectors[\"publication\"][\"title\"], base_element=container)\n",
    "        if not title_element:\n",
    "            logging.warning(\"No title element found, skipping publication\")\n",
    "            return None\n",
    "        \n",
    "        # Get title\n",
    "        title = title_element.text.strip()\n",
    "        \n",
    "        # Get URL from title link\n",
    "        title_link = title_element.find(\"a\")\n",
    "        pub_url = title_link.get(\"href\") if title_link else None\n",
    "        \n",
    "        # Ensure URL is absolute\n",
    "        if pub_url and not pub_url.startswith((\"http://\", \"https://\")):\n",
    "            pub_url = f\"{self.base_url}{pub_url}\"\n",
    "        \n",
    "        # Get authors\n",
    "        authors_element = self.finder.find(self.selectors[\"publication\"][\"authors\"], base_element=container)\n",
    "        authors = authors_element.text.strip() if authors_element else None\n",
    "        \n",
    "        # Extract year - first try the year text\n",
    "        year = None\n",
    "        if authors_element:\n",
    "            # Look for year in the text following the authors\n",
    "            sibling_text = authors_element.next_sibling\n",
    "            if sibling_text:\n",
    "                year_match = re.search(r\"\\b(19|20)\\d{2}\\b\", sibling_text)\n",
    "                if year_match:\n",
    "                    year = int(year_match.group())\n",
    "        \n",
    "        # Get abstract\n",
    "        abstract_element = self.finder.find(self.selectors[\"publication\"][\"abstract\"], base_element=container)\n",
    "        abstract = abstract_element.text.strip() if abstract_element else None\n",
    "        \n",
    "        # Get file URLs\n",
    "        file_elements = self.finder.find_all(self.selectors[\"publication\"][\"file\"], base_element=container)\n",
    "        file_urls = []\n",
    "        for file_elem in file_elements:\n",
    "            file_link = file_elem.find(\"a\")\n",
    "            if file_link and file_link.get(\"href\"):\n",
    "                file_url = file_link[\"href\"]\n",
    "                # Ensure URL is absolute\n",
    "                if not file_url.startswith((\"http://\", \"https://\")):\n",
    "                    file_url = f\"{self.base_url}{file_url}\"\n",
    "                file_urls.append(file_url)\n",
    "        \n",
    "        # Create a publication object\n",
    "        pub = SimpleGrowthLabPublication(\n",
    "            title=title,\n",
    "            authors=authors,\n",
    "            year=year,\n",
    "            abstract=abstract,\n",
    "            pub_url=pub_url,\n",
    "            file_urls=file_urls,\n",
    "            source=\"GrowthLab\",\n",
    "        )\n",
    "        \n",
    "        # Generate stable ID and content hash\n",
    "        pub.paper_id = pub.generate_id()\n",
    "        pub.content_hash = pub.generate_content_hash()\n",
    "        \n",
    "        return pub\n",
    "    \n",
    "    async def get_max_page_num(self, html):\n",
    "        \"\"\"Get the maximum page number from pagination\"\"\"\n",
    "        if not html:\n",
    "            logging.warning(\"No HTML content provided to parse pagination\")\n",
    "            return 0\n",
    "        \n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        self.finder.set_soup(soup)\n",
    "        \n",
    "        # Find pagination container\n",
    "        pagination = self.finder.find(self.selectors[\"pagination\"][\"container\"])\n",
    "        \n",
    "        if not pagination:\n",
    "            logging.warning(\"No pagination container found\")\n",
    "            return 0\n",
    "        \n",
    "        # Find last page element\n",
    "        last_page_element = self.finder.find(\n",
    "            self.selectors[\"pagination\"][\"last_page\"], \n",
    "            base_element=pagination\n",
    "        )\n",
    "        \n",
    "        if not last_page_element or not last_page_element.find(\"a\"):\n",
    "            logging.warning(\"No last page link found\")\n",
    "            return 0\n",
    "        \n",
    "        # Get URL from last page link\n",
    "        last_page_url = last_page_element.find(\"a\").get(\"href\")\n",
    "        if not last_page_url:\n",
    "            return 0\n",
    "        \n",
    "        # Extract page number from URL\n",
    "        match = re.search(r\"page=(\\d+)\", last_page_url)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        \n",
    "        return 0\n",
    "\n",
    "# Test the publication parser with configurable selectors\n",
    "async def test_publication_parser():\n",
    "    \"\"\"Test the publication parser with configurable selectors\"\"\"\n",
    "    print(\"\\n🔍 Testing publication parser with configurable selectors...\")\n",
    "    \n",
    "    # Create a parser with the current selector configuration\n",
    "    parser = PublicationParser(SELECTOR_CONFIG, scraper.base_url.split('/publications')[0])\n",
    "    \n",
    "    # Fetch a page\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(scraper.base_url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page: {response.status}\")\n",
    "                return\n",
    "            \n",
    "            html = await response.text()\n",
    "            \n",
    "            # Get max page number\n",
    "            max_page = await parser.get_max_page_num(html)\n",
    "            print(f\"Maximum page number: {max_page}\")\n",
    "            \n",
    "            # Parse publications\n",
    "            publications = await parser.parse_page(html)\n",
    "            print(f\"Parsed {len(publications)} publications\")\n",
    "            \n",
    "            if publications:\n",
    "                print(\"\\nFirst 3 parsed publications:\")\n",
    "                for i, pub in enumerate(publications[:3]):\n",
    "                    print(f\"\\n{i+1}. {pub['title']}\")\n",
    "                    print(f\"   Authors: {pub['authors']}\")\n",
    "                    print(f\"   Year: {pub['year']}\")\n",
    "                    print(f\"   Abstract: {pub['abstract'][:100]}...\" if pub['abstract'] else \"   No abstract\")\n",
    "                    print(f\"   URL: {pub['pub_url']}\")\n",
    "                    print(f\"   Files: {pub['file_urls']}\")\n",
    "                    print(f\"   ID: {pub['paper_id']}\")\n",
    "\n",
    "# Run the test\n",
    "await test_publication_parser()\n",
    "\n",
    "# %%\n",
    "# CHUNK 12: Implement Selector Monitoring and Alert System\n",
    "\n",
    "class SelectorMonitor:\n",
    "    \"\"\"Class to monitor selector performance and detect failures\"\"\"\n",
    "    \n",
    "    def __init__(self, selectors=None):\n",
    "        self.selectors = selectors or SELECTOR_CONFIG\n",
    "        self.stats = {\n",
    "            \"total_pages\": 0,\n",
    "            \"total_publications\": 0,\n",
    "            \"selector_success\": {},\n",
    "            \"selector_failure\": {},\n",
    "            \"alerts\": []\n",
    "        }\n",
    "        \n",
    "        # Initialize stats for each selector\n",
    "        for section, config in self.selectors.items():\n",
    "            for name, selector in config.items():\n",
    "                key = f\"{section}.{name}\"\n",
    "                self.stats[\"selector_success\"][key] = 0\n",
    "                self.stats[\"selector_failure\"][key] = 0\n",
    "    \n",
    "    def record_success(self, section, name):\n",
    "        \"\"\"Record a successful selector use\"\"\"\n",
    "        key = f\"{section}.{name}\"\n",
    "        if key in self.stats[\"selector_success\"]:\n",
    "            self.stats[\"selector_success\"][key] += 1\n",
    "    \n",
    "    def record_failure(self, section, name):\n",
    "        \"\"\"Record a failed selector use\"\"\"\n",
    "        key = f\"{section}.{name}\"\n",
    "        if key in self.stats[\"selector_failure\"]:\n",
    "            self.stats[\"selector_failure\"][key] += 1\n",
    "            \n",
    "            # Check if failure rate is high enough to trigger alert\n",
    "            total = self.stats[\"selector_success\"][key] + self.stats[\"selector_failure\"][key]\n",
    "            if total >= 5:  # Only check after a minimum sample\n",
    "                failure_rate = self.stats[\"selector_failure\"][key] / total\n",
    "                if failure_rate > 0.5:  # Alert if more than 50% failure\n",
    "                    self.create_alert(section, name, failure_rate)\n",
    "    \n",
    "    def create_alert(self, section, name, failure_rate):\n",
    "        \"\"\"Create an alert for a failing selector\"\"\"\n",
    "        selector = self.selectors[section][name]\n",
    "        alert = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"selector\": f\"{section}.{name}\",\n",
    "            \"failure_rate\": failure_rate,\n",
    "            \"primary\": selector.primary,\n",
    "            \"fallbacks\": selector.fallbacks,\n",
    "            \"message\": f\"Selector {section}.{name} is failing at a rate of {failure_rate:.2%}\"\n",
    "        }\n",
    "        \n",
    "        # Check if we already have an alert for this selector\n",
    "        existing_alerts = [a for a in self.stats[\"alerts\"] if a[\"selector\"] == alert[\"selector\"]]\n",
    "        if not existing_alerts:\n",
    "            self.stats[\"alerts\"].append(alert)\n",
    "            logging.warning(f\"SELECTOR ALERT: {alert['message']}\")\n",
    "    \n",
    "    def record_page_processed(self):\n",
    "        \"\"\"Record that a page was processed\"\"\"\n",
    "        self.stats[\"total_pages\"] += 1\n",
    "    \n",
    "    def record_publication_processed(self):\n",
    "        \"\"\"Record that a publication was processed\"\"\"\n",
    "        self.stats[\"total_publications\"] += 1\n",
    "    \n",
    "    def check_selector_health(self):\n",
    "        \"\"\"Check the health of all selectors\"\"\"\n",
    "        print(\"\\n🔍 Selector Health Check:\")\n",
    "        \n",
    "        for section, config in self.selectors.items():\n",
    "            print(f\"\\n{section.upper()} Selectors:\")\n",
    "            \n",
    "            for name, selector in config.items():\n",
    "                key = f\"{section}.{name}\"\n",
    "                success = self.stats[\"selector_success\"].get(key, 0)\n",
    "                failure = self.stats[\"selector_failure\"].get(key, 0)\n",
    "                total = success + failure\n",
    "                \n",
    "                if total > 0:\n",
    "                    success_rate = success / total\n",
    "                    status = \"✅ GOOD\" if success_rate >= 0.9 else \"⚠️ WARNING\" if success_rate >= 0.5 else \"❌ FAILING\"\n",
    "                    print(f\"  - {key}: {status} ({success}/{total}, {success_rate:.1%})\")\n",
    "                else:\n",
    "                    print(f\"  - {key}: ⚪ NO DATA\")\n",
    "    \n",
    "    def get_alerts(self):\n",
    "        \"\"\"Get current selector alerts\"\"\"\n",
    "        return self.stats[\"alerts\"]\n",
    "    \n",
    "    def print_alerts(self):\n",
    "        \"\"\"Print current selector alerts\"\"\"\n",
    "        alerts = self.get_alerts()\n",
    "        if alerts:\n",
    "            print(\"\\n⚠️ Current Selector Alerts:\")\n",
    "            for alert in alerts:\n",
    "                print(f\"  - {alert['selector']}: Failing at {alert['failure_rate']:.1%}\")\n",
    "                print(f\"    Primary: {alert['primary']}\")\n",
    "                print(f\"    Fallbacks: {alert['fallbacks']}\")\n",
    "        else:\n",
    "            print(\"\\n✅ No selector alerts detected\")\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a full report of selector performance\"\"\"\n",
    "        print(\"\\n📊 Selector Performance Report\")\n",
    "        print(f\"Pages processed: {self.stats['total_pages']}\")\n",
    "        print(f\"Publications processed: {self.stats['total_publications']}\")\n",
    "        \n",
    "        # Calculate overall selector success rate\n",
    "        total_success = sum(self.stats[\"selector_success\"].values())\n",
    "        total_failure = sum(self.stats[\"selector_failure\"].values())\n",
    "        total_attempts = total_success + total_failure\n",
    "        \n",
    "        if total_attempts > 0:\n",
    "            overall_rate = total_success / total_attempts\n",
    "            print(f\"Overall selector success rate: {overall_rate:.2%}\")\n",
    "        \n",
    "        # Print selector-specific stats\n",
    "        print(\"\\nSelector Performance:\")\n",
    "        for key in sorted(self.stats[\"selector_success\"].keys()):\n",
    "            success = self.stats[\"selector_success\"][key]\n",
    "            failure = self.stats[\"selector_failure\"][key]\n",
    "            total = success + failure\n",
    "            \n",
    "            if total > 0:\n",
    "                rate = success / total\n",
    "                print(f\"  - {key}: {rate:.2%} success ({success}/{total})\")\n",
    "        \n",
    "        # Print alerts\n",
    "        self.print_alerts()\n",
    "\n",
    "# Create a simple demo for the selector monitor\n",
    "async def test_selector_monitor():\n",
    "    \"\"\"Demonstrate the selector monitoring system\"\"\"\n",
    "    print(\"\\n📡 Testing selector monitoring system...\")\n",
    "    \n",
    "    # Create a monitor\n",
    "    monitor = SelectorMonitor(SELECTOR_CONFIG)\n",
    "    \n",
    "    # Simulate some selector successes and failures\n",
    "    monitor.record_success(\"publication\", \"container\")\n",
    "    monitor.record_success(\"publication\", \"container\")\n",
    "    monitor.record_success(\"publication\", \"title\")\n",
    "    monitor.record_success(\"publication\", \"title\")\n",
    "    monitor.record_failure(\"publication\", \"abstract\")\n",
    "    \n",
    "    # Simulate multiple failures to trigger alert\n",
    "    monitor.record_failure(\"publication\", \"file\")\n",
    "    monitor.record_failure(\"publication\", \"file\")\n",
    "    monitor.record_failure(\"publication\", \"file\")\n",
    "    monitor.record_success(\"publication\", \"file\")\n",
    "    \n",
    "    # Record page and publication processing\n",
    "    monitor.record_page_processed()\n",
    "    monitor.record_page_processed()\n",
    "    monitor.record_publication_processed()\n",
    "    monitor.record_publication_processed()\n",
    "    monitor.record_publication_processed()\n",
    "    \n",
    "    # Check selector health\n",
    "    monitor.check_selector_health()\n",
    "    \n",
    "    # Generate full report\n",
    "    monitor.generate_report()\n",
    "    \n",
    "    return monitor\n",
    "\n",
    "# Run the test\n",
    "selector_monitor = await test_selector_monitor()\n",
    "\n",
    "# Show how to integrate the monitor with the parser\n",
    "print(\"\\n🔄 Integration Example: Using the monitor with the parser\")\n",
    "print(\"\"\"\n",
    "# Example integration in production code:\n",
    "\n",
    "async def parse_with_monitoring(html, parser, monitor):\n",
    "    # Record that we're processing a page\n",
    "    monitor.record_page_processed()\n",
    "    \n",
    "    # Parse the page\n",
    "    try:\n",
    "        publications = await parser.parse_page(html)\n",
    "        # Record successful parsing\n",
    "        monitor.record_success(\"publication\", \"container\")\n",
    "        \n",
    "        # For each publication, record successful elements\n",
    "        for pub in publications:\n",
    "            monitor.record_publication_processed()\n",
    "            if pub.get(\"title\"):\n",
    "                monitor.record_success(\"publication\", \"title\")\n",
    "            else:\n",
    "                monitor.record_failure(\"publication\", \"title\")\n",
    "                \n",
    "            # And so on for other elements...\n",
    "            \n",
    "        return publications\n",
    "    except Exception as e:\n",
    "        # Record failure\n",
    "        monitor.record_failure(\"publication\", \"container\")\n",
    "        logging.error(f\"Error parsing page: {e}\")\n",
    "        return []\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✅ Implementation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c17a18fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing selector parser:\n",
      "'div.biblio-entry' -> tag: 'div', attrs: {'class': 'biblio-entry'}\n",
      "'span.biblio-title' -> tag: 'span', attrs: {'class': 'biblio-title'}\n",
      "'li' -> tag: 'li', attrs: {}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# STEP 2: Create a simple selector configuration\n",
    "\n",
    "# Define selectors in a configuration dictionary\n",
    "SELECTORS = {\n",
    "    \"publication\": {\n",
    "        \"container\": \"div.biblio-entry\",\n",
    "        \"title\": \"span.biblio-title\",\n",
    "        \"authors\": \"span.biblio-authors\",\n",
    "        \"abstract\": \"div.biblio-abstract-display\",\n",
    "        \"file\": \"span.file\",\n",
    "    },\n",
    "    \"pagination\": {\n",
    "        \"container\": \"ul.pager\",\n",
    "        \"last_page\": \"li.pager-last\",\n",
    "    },\n",
    "    \"endnote\": {\n",
    "        \"link\": \"li.biblio_tagged a\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Helper function to parse CSS selectors into BeautifulSoup arguments\n",
    "def parse_selector(selector):\n",
    "    \"\"\"Parse a CSS selector string into tag and attributes for BeautifulSoup\"\"\"\n",
    "    if \".\" in selector:\n",
    "        parts = selector.split(\".\")\n",
    "        tag = parts[0] if parts[0] else None\n",
    "        class_name = parts[1]\n",
    "        return tag, {\"class\": class_name}\n",
    "    else:\n",
    "        return selector, {}\n",
    "\n",
    "# Test our selector parser with a few examples\n",
    "test_selectors = [\n",
    "    \"div.biblio-entry\",\n",
    "    \"span.biblio-title\",\n",
    "    \"li\"\n",
    "]\n",
    "\n",
    "print(\"Testing selector parser:\")\n",
    "for selector in test_selectors:\n",
    "    tag, attrs = parse_selector(selector)\n",
    "    print(f\"'{selector}' -> tag: '{tag}', attrs: {attrs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d9d50e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using selector: div.biblio-entry for publication.container\n",
      "Found publication container\n",
      "Using selector: span.biblio-title for publication.title\n",
      "Title: Global Networks, Monetary Policy and Trade\n",
      "Using selector: span.biblio-authors for publication.authors\n",
      "Authors: Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# STEP 3: Create a function that uses the selectors\n",
    "\n",
    "async def find_with_selector(soup, selector_key, base_element=None):\n",
    "    \"\"\"Find an element using the configured selector\"\"\"\n",
    "    # Get the correct selector string\n",
    "    parts = selector_key.split(\".\")\n",
    "    if len(parts) != 2:\n",
    "        print(f\"Invalid selector key format: {selector_key}\")\n",
    "        return None\n",
    "        \n",
    "    section, name = parts\n",
    "    if section not in SELECTORS or name not in SELECTORS[section]:\n",
    "        print(f\"Selector not found: {selector_key}\")\n",
    "        return None\n",
    "        \n",
    "    selector = SELECTORS[section][name]\n",
    "    print(f\"Using selector: {selector} for {selector_key}\")\n",
    "    \n",
    "    # Parse the selector\n",
    "    tag, attrs = parse_selector(selector)\n",
    "    \n",
    "    # Use the base element or soup\n",
    "    element = base_element if base_element else soup\n",
    "    \n",
    "    # Find the element\n",
    "    return element.find(tag, attrs)\n",
    "\n",
    "# Test the function with a simple example\n",
    "async def test_find_with_selector():\n",
    "    \"\"\"Test finding elements with our selector function\"\"\"\n",
    "    # Fetch a page\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(scraper.base_url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page: {response.status}\")\n",
    "                return\n",
    "                \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Find a publication container\n",
    "            container = await find_with_selector(soup, \"publication.container\")\n",
    "            if not container:\n",
    "                print(\"No publication container found\")\n",
    "                return\n",
    "                \n",
    "            print(\"Found publication container\")\n",
    "            \n",
    "            # Find title within container\n",
    "            title_element = await find_with_selector(soup, \"publication.title\", container)\n",
    "            if title_element:\n",
    "                print(f\"Title: {title_element.text.strip()}\")\n",
    "            else:\n",
    "                print(\"Title not found\")\n",
    "            \n",
    "            # Find authors within container\n",
    "            authors_element = await find_with_selector(soup, \"publication.authors\", container)\n",
    "            if authors_element:\n",
    "                print(f\"Authors: {authors_element.text.strip()}\")\n",
    "            else:\n",
    "                print(\"Authors not found\")\n",
    "\n",
    "# Run the test\n",
    "await test_find_with_selector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8842af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found with primary selector: div.biblio-entry\n",
      "Found publication container\n",
      "Found with primary selector: span.biblio-title\n",
      "Title: Global Networks, Monetary Policy and Trade\n",
      "Found with primary selector: span.biblio-authors\n",
      "Authors: Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\n",
      "Found with primary selector: div.biblio-abstract-display\n",
      "Abstract: We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# STEP 4: Add fallback selectors for robustness\n",
    "\n",
    "# Update our selector configuration to include fallbacks\n",
    "SELECTORS_WITH_FALLBACKS = {\n",
    "    \"publication\": {\n",
    "        \"container\": {\n",
    "            \"primary\": \"div.biblio-entry\",\n",
    "            \"fallbacks\": [\"div.publication-item\", \"article.publication\"]\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"primary\": \"span.biblio-title\",\n",
    "            \"fallbacks\": [\"h2.publication-title\", \"h3.title\", \"div.title\"]\n",
    "        },\n",
    "        \"authors\": {\n",
    "            \"primary\": \"span.biblio-authors\",\n",
    "            \"fallbacks\": [\"div.authors\", \"p.author-list\"]\n",
    "        },\n",
    "        \"abstract\": {\n",
    "            \"primary\": \"div.biblio-abstract-display\",\n",
    "            \"fallbacks\": [\"div.abstract\", \"p.abstract\"]\n",
    "        },\n",
    "        \"file\": {\n",
    "            \"primary\": \"span.file\",\n",
    "            \"fallbacks\": [\"a.file-download\", \"div.file-links a\"]\n",
    "        },\n",
    "    },\n",
    "    \"pagination\": {\n",
    "        \"container\": {\n",
    "            \"primary\": \"ul.pager\",\n",
    "            \"fallbacks\": [\"div.pagination\", \"nav.pagination\"]\n",
    "        },\n",
    "        \"last_page\": {\n",
    "            \"primary\": \"li.pager-last\",\n",
    "            \"fallbacks\": [\"li.page-item:last-child\", \"a.page-link:last-child\"]\n",
    "        }\n",
    "    },\n",
    "    \"endnote\": {\n",
    "        \"link\": {\n",
    "            \"primary\": \"li.biblio_tagged a\",\n",
    "            \"fallbacks\": [\"a.endnote-link\", \"a[href*='endnote']\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Update our function to try fallbacks\n",
    "async def find_with_fallbacks(soup, selector_key, base_element=None):\n",
    "    \"\"\"Find an element using the configured selector with fallbacks\"\"\"\n",
    "    # Get the correct selector configuration\n",
    "    parts = selector_key.split(\".\")\n",
    "    if len(parts) != 2:\n",
    "        print(f\"Invalid selector key format: {selector_key}\")\n",
    "        return None\n",
    "        \n",
    "    section, name = parts\n",
    "    if section not in SELECTORS_WITH_FALLBACKS or name not in SELECTORS_WITH_FALLBACKS[section]:\n",
    "        print(f\"Selector not found: {selector_key}\")\n",
    "        return None\n",
    "        \n",
    "    selector_config = SELECTORS_WITH_FALLBACKS[section][name]\n",
    "    \n",
    "    # Use the base element or soup\n",
    "    element = base_element if base_element else soup\n",
    "    \n",
    "    # Try the primary selector first\n",
    "    primary = selector_config[\"primary\"]\n",
    "    tag, attrs = parse_selector(primary)\n",
    "    result = element.find(tag, attrs)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"Found with primary selector: {primary}\")\n",
    "        return result\n",
    "    \n",
    "    # If primary fails, try fallbacks\n",
    "    for fallback in selector_config[\"fallbacks\"]:\n",
    "        print(f\"Trying fallback selector: {fallback}\")\n",
    "        tag, attrs = parse_selector(fallback)\n",
    "        result = element.find(tag, attrs)\n",
    "        if result:\n",
    "            print(f\"Found with fallback selector: {fallback}\")\n",
    "            return result\n",
    "    \n",
    "    print(f\"All selectors failed for {selector_key}\")\n",
    "    return None\n",
    "\n",
    "# Test the function with fallbacks\n",
    "async def test_find_with_fallbacks():\n",
    "    \"\"\"Test finding elements with our fallback selector function\"\"\"\n",
    "    # Fetch a page\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(scraper.base_url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page: {response.status}\")\n",
    "                return\n",
    "                \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Find a publication container\n",
    "            container = await find_with_fallbacks(soup, \"publication.container\")\n",
    "            if not container:\n",
    "                print(\"No publication container found\")\n",
    "                return\n",
    "                \n",
    "            print(\"Found publication container\")\n",
    "            \n",
    "            # Find title within container\n",
    "            title_element = await find_with_fallbacks(soup, \"publication.title\", container)\n",
    "            if title_element:\n",
    "                print(f\"Title: {title_element.text.strip()}\")\n",
    "            else:\n",
    "                print(\"Title not found\")\n",
    "            \n",
    "            # Find authors within container\n",
    "            authors_element = await find_with_fallbacks(soup, \"publication.authors\", container)\n",
    "            if authors_element:\n",
    "                print(f\"Authors: {authors_element.text.strip()}\")\n",
    "            else:\n",
    "                print(\"Authors not found\")\n",
    "            \n",
    "            # Find abstract within container (with real and fake selectors to test fallbacks)\n",
    "            abstract_element = await find_with_fallbacks(soup, \"publication.abstract\", container)\n",
    "            if abstract_element:\n",
    "                abstract = abstract_element.text.strip()\n",
    "                print(f\"Abstract: {abstract[:100]}...\" if len(abstract) > 100 else abstract)\n",
    "            else:\n",
    "                print(\"Abstract not found\")\n",
    "\n",
    "# Run the fallback test\n",
    "await test_find_with_fallbacks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9395fe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing selectors on real publications across multiple pages\n",
      "Fetching page 0: https://growthlab.hks.harvard.edu/publications\n",
      "Found 20 publications on page 0\n",
      "Waiting 1.2 seconds before next request...\n",
      "Fetching page 1: https://growthlab.hks.harvard.edu/publications?page=1\n",
      "Found 20 publications on page 1\n",
      "Waiting 1.1 seconds before next request...\n",
      "Fetching page 2: https://growthlab.hks.harvard.edu/publications?page=2\n",
      "Found 20 publications on page 2\n",
      "\n",
      "Collected 6 publication samples for testing\n",
      "\n",
      "📄 Testing publication sample 1 from page 0\n",
      "\n",
      "🔍 Testing selector publication.title\n",
      "Primary selector: span.biblio-title\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['biblio-title']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[5]/span[2]\n",
      "Text content: \"De Facto Openness to Immigration\"\n",
      "Direct children: 1\n",
      "  a (1)\n",
      "\n",
      "🔍 Testing selector publication.authors\n",
      "Primary selector: span.biblio-authors\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['biblio-authors']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[5]/span[1]\n",
      "Text content: \"Nedelkoska, L., et al.\"\n",
      "No children\n",
      "\n",
      "🔍 Testing selector publication.abstract\n",
      "Primary selector: div.biblio-abstract-display\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <div>\n",
      "Attributes:\n",
      "  class: ['biblio-abstract-display', 'os-slider']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[5]/div[1]\n",
      "Text content: \"Various factors influence why some countries are more open to immigration than others. Policy is ...\"\n",
      "Direct children: 2\n",
      "  p (2)\n",
      "\n",
      "🔍 Testing selector publication.file\n",
      "Primary selector: span.file\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['file']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[5]/span[4]/span\n",
      "Text content: \"2025-02-glwp-245-openness-to-immigration.pdf\"\n",
      "Direct children: 2\n",
      "  img (1), a (1)\n",
      "\n",
      "📄 Testing publication sample 2 from page 0\n",
      "\n",
      "🔍 Testing selector publication.title\n",
      "Primary selector: span.biblio-title\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['biblio-title']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[3]/div[12]/span[2]\n",
      "Text content: \"GLocal: A global development dataset of subnational administrative areas\"\n",
      "Direct children: 1\n",
      "  a (1)\n",
      "\n",
      "🔍 Testing selector publication.authors\n",
      "Primary selector: span.biblio-authors\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['biblio-authors']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[3]/div[12]/span[1]\n",
      "Text content: \"Morales-Arilla, J. & Gadgin Matha, S.\"\n",
      "No children\n",
      "\n",
      "🔍 Testing selector publication.abstract\n",
      "Primary selector: div.biblio-abstract-display\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <div>\n",
      "Attributes:\n",
      "  class: ['biblio-abstract-display', 'os-slider']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[3]/div[12]/div[2]\n",
      "Text content: \"The purpose of the GLocal dataset is to enable research in international development that require...\"\n",
      "Direct children: 1\n",
      "  p (1)\n",
      "\n",
      "🔍 Testing selector publication.file\n",
      "Primary selector: span.file\n",
      "❌ Primary selector failed\n",
      "\n",
      "Trying fallback 1: a.file-download\n",
      "❌ Fallback 1 failed\n",
      "\n",
      "Trying fallback 2: div.file-links a\n",
      "❌ Fallback 2 failed\n",
      "\n",
      "❌ All selectors failed\n",
      "\n",
      "📄 Testing publication sample 3 from page 1\n",
      "\n",
      "🔍 Testing selector publication.title\n",
      "Primary selector: span.biblio-title\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['biblio-title']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[3]/div[5]/span[2]\n",
      "Text content: \"Towards a Sustainable Recovery for Lebanon’s Economy\"\n",
      "Direct children: 1\n",
      "  a (1)\n",
      "\n",
      "🔍 Testing selector publication.authors\n",
      "Primary selector: span.biblio-authors\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['biblio-authors']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[3]/div[5]/span[1]\n",
      "Text content: \"Hausmann, R., et al.\"\n",
      "No children\n",
      "\n",
      "🔍 Testing selector publication.abstract\n",
      "Primary selector: div.biblio-abstract-display\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <div>\n",
      "Attributes:\n",
      "  class: ['biblio-abstract-display', 'os-slider']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[3]/div[5]/div[1]\n",
      "Text content: \"Lebanon’s current economic crisis ranks among the worst in recent history. GDP has collapsed by 3...\"\n",
      "Direct children: 4\n",
      "  p (4)\n",
      "\n",
      "🔍 Testing selector publication.file\n",
      "Primary selector: span.file\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['file']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[3]/div[5]/span[4]/span[1]\n",
      "Text content: \"2023-11-cid-wp-439-lebanon-sustainable-recovery.pdf\"\n",
      "Direct children: 2\n",
      "  img (1), a (1)\n",
      "\n",
      "📄 Testing publication sample 4 from page 1\n",
      "\n",
      "🔍 Testing selector publication.title\n",
      "Primary selector: span.biblio-title\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['biblio-title']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[3]/span[2]\n",
      "Text content: \"Supply-Side Economics of a Good Type: Supporting and Expanding South Africa’s Informal Economy\"\n",
      "Direct children: 1\n",
      "  a (1)\n",
      "\n",
      "🔍 Testing selector publication.authors\n",
      "Primary selector: span.biblio-authors\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['biblio-authors']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[3]/span[1]\n",
      "Text content: \"Bhorat, H., et al.\"\n",
      "No children\n",
      "\n",
      "🔍 Testing selector publication.abstract\n",
      "Primary selector: div.biblio-abstract-display\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <div>\n",
      "Attributes:\n",
      "  class: ['biblio-abstract-display', 'os-slider']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[3]/div[1]\n",
      "Text content: \"This paper argues that South Africa's persistently high unemployment is in part explained by abno...\"\n",
      "Direct children: 1\n",
      "  p (1)\n",
      "\n",
      "🔍 Testing selector publication.file\n",
      "Primary selector: span.file\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['file']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[3]/span[4]/span\n",
      "Text content: \"2024-04-wp-158-supply-side-economics-south-africa.pdf\"\n",
      "Direct children: 2\n",
      "  img (1), a (1)\n",
      "\n",
      "📄 Testing publication sample 5 from page 2\n",
      "\n",
      "🔍 Testing selector publication.title\n",
      "Primary selector: span.biblio-title\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['biblio-title']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[8]/span[2]\n",
      "Text content: \"What is South Africa’s Crop Production Potential?\"\n",
      "Direct children: 1\n",
      "  a (1)\n",
      "\n",
      "🔍 Testing selector publication.authors\n",
      "Primary selector: span.biblio-authors\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['biblio-authors']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[8]/span[1]\n",
      "Text content: \"Sturzenegger, F., Klinger, B. & Ordonez, I.\"\n",
      "No children\n",
      "\n",
      "🔍 Testing selector publication.abstract\n",
      "Primary selector: div.biblio-abstract-display\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <div>\n",
      "Attributes:\n",
      "  class: ['biblio-abstract-display', 'os-slider']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[8]/div[1]\n",
      "Text content: \"Combining satellite data with FAO potential yields we provide a new measure of South Africa's cur...\"\n",
      "Direct children: 2\n",
      "  p (2)\n",
      "\n",
      "🔍 Testing selector publication.file\n",
      "Primary selector: span.file\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['file']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[8]/span[4]/span\n",
      "Text content: \"2023-05-cid-fellows-wp-149-south-africa-crop-potential.pdf\"\n",
      "Direct children: 2\n",
      "  img (1), a (1)\n",
      "\n",
      "📄 Testing publication sample 6 from page 2\n",
      "\n",
      "🔍 Testing selector publication.title\n",
      "Primary selector: span.biblio-title\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['biblio-title']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[9]/span[2]\n",
      "Text content: \"Scaling Partnerships to Activate Idle Community Land in South Africa\"\n",
      "Direct children: 1\n",
      "  a (1)\n",
      "\n",
      "🔍 Testing selector publication.authors\n",
      "Primary selector: span.biblio-authors\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['biblio-authors']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[9]/span[1]\n",
      "Text content: \"Klinger, B., Ordonez, I. & Sturzenegger, F.\"\n",
      "No children\n",
      "\n",
      "🔍 Testing selector publication.abstract\n",
      "Primary selector: div.biblio-abstract-display\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <div>\n",
      "Attributes:\n",
      "  class: ['biblio-abstract-display', 'os-slider']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[9]/div[1]\n",
      "Text content: \"We discuss three cases of corporate-smallholder partnerships in South Africa’s former homelands, ...\"\n",
      "Direct children: 2\n",
      "  p (2)\n",
      "\n",
      "🔍 Testing selector publication.file\n",
      "Primary selector: span.file\n",
      "✅ Primary selector succeeded\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <span>\n",
      "Attributes:\n",
      "  class: ['file']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[9]/span[4]/span\n",
      "Text content: \"2023-05-cid-fellows-wp148-scaling-partnerships-south-africa.pdf\"\n",
      "Direct children: 2\n",
      "  img (1), a (1)\n",
      "\n",
      "📊 Selector Test Summary:\n",
      "publication.title: ✅ GOOD - 100.0% success (6/6)\n",
      "publication.authors: ✅ GOOD - 100.0% success (6/6)\n",
      "publication.abstract: ✅ GOOD - 100.0% success (6/6)\n",
      "publication.file: ⚠️ WARNING - 83.3% success (5/6)\n",
      "\n",
      "# 🔍 Website Element Inspector Tool\n",
      "\n",
      "Use this tool to inspect elements on the website directly. Examples:\n",
      "\n",
      "1. Get page overview:\n",
      "   await inspect_website_element()\n",
      "\n",
      "2. Inspect publication containers:\n",
      "   await inspect_website_element(css_selector=\"div.biblio-entry\")\n",
      "\n",
      "3. Inspect titles:\n",
      "   await inspect_website_element(css_selector=\"span.biblio-title\")\n",
      "\n",
      "4. Inspect a specific page:\n",
      "   await inspect_website_element(url=\"https://growthlab.hks.harvard.edu/publications?page=5\")\n",
      "\n",
      "5. Try the selector you're having trouble with:\n",
      "   await inspect_website_element(css_selector=\"your-selector-here\")\n",
      "\n",
      "Fetching: https://growthlab.hks.harvard.edu/publications\n",
      "\n",
      "Found 20 elements matching 'div.biblio-entry'\n",
      "\n",
      "📋 Element Inspection:\n",
      "Tag: <div>\n",
      "Attributes:\n",
      "  class: ['biblio-entry', 'node', 'node-biblio', 'node-teaser', 'contextual-links-region', 'article', 'clearfix']\n",
      "XPath: //body/div[2]/div[3]/div/div[2]/div/div[1]/div/section/div/div[2]/div[2]\n",
      "Text content: \"Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A., 2025. Global Networks, Monetary Policy and Trade. ...\"\n",
      "Direct children: 7\n",
      "  span (3), a (2), div (2)\n",
      "\n",
      "Summary of other 19 elements:\n",
      "\n",
      "Element 2:\n",
      "Tag: <div>\n",
      "Text: \"Ahuja, K. & Hausmann, R., 2025. Industrial poli...\"\n",
      "\n",
      "Element 3:\n",
      "Tag: <div>\n",
      "Text: \"Fortunato, A. & Santos, M.A., 2025. Public-Priv...\"\n",
      "\n",
      "Element 4:\n",
      "Tag: <div>\n",
      "Text: \"Nedelkoska, L., et al., 2025. De Facto Openness...\"\n",
      "\n",
      "Element 5:\n",
      "Tag: <div>\n",
      "Text: \"Arcay, G. & O'Brien, T., 2025. Serving From Her...\"\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# STEP 4: Enhanced Fallback Testing with Website Inspection Tools\n",
    "\n",
    "# First, let's add a function to extract XPath from an element\n",
    "def get_xpath(element):\n",
    "    \"\"\"Get the XPath of an element\"\"\"\n",
    "    components = []\n",
    "    child = element if element.name else element.parent\n",
    "    \n",
    "    for parent in child.parents:\n",
    "        siblings = parent.find_all(child.name, recursive=False)\n",
    "        if len(siblings) > 1:\n",
    "            # If there are multiple siblings, use position\n",
    "            components.append(f\"{child.name}[{siblings.index(child)+1}]\")\n",
    "        else:\n",
    "            components.append(child.name)\n",
    "        child = parent\n",
    "        \n",
    "        # Stop at the <html> level\n",
    "        if parent.name == \"html\":\n",
    "            break\n",
    "            \n",
    "    components.reverse()\n",
    "    return \"//\" + \"/\".join(components)\n",
    "\n",
    "# Add a function to inspect an element's attributes and structure\n",
    "def inspect_element(element, print_xpath=True):\n",
    "    \"\"\"Print detailed information about an element\"\"\"\n",
    "    if not element:\n",
    "        print(\"No element provided to inspect\")\n",
    "        return\n",
    "        \n",
    "    print(\"\\n📋 Element Inspection:\")\n",
    "    print(f\"Tag: <{element.name}>\")\n",
    "    \n",
    "    # Print attributes\n",
    "    if element.attrs:\n",
    "        print(\"Attributes:\")\n",
    "        for key, value in element.attrs.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(\"No attributes\")\n",
    "    \n",
    "    # Print XPath\n",
    "    if print_xpath:\n",
    "        try:\n",
    "            xpath = get_xpath(element)\n",
    "            print(f\"XPath: {xpath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting XPath: {e}\")\n",
    "    \n",
    "    # Print text content (truncated)\n",
    "    if element.text:\n",
    "        text = element.text.strip()\n",
    "        if len(text) > 100:\n",
    "            text = text[:97] + \"...\"\n",
    "        print(f\"Text content: \\\"{text}\\\"\")\n",
    "    else:\n",
    "        print(\"No text content\")\n",
    "    \n",
    "    # Print immediate children (summary)\n",
    "    children = element.find_all(recursive=False)\n",
    "    if children:\n",
    "        print(f\"Direct children: {len(children)}\")\n",
    "        child_tags = {}\n",
    "        for child in children:\n",
    "            if child.name:\n",
    "                child_tags[child.name] = child_tags.get(child.name, 0) + 1\n",
    "        print(\"  \" + \", \".join(f\"{tag} ({count})\" for tag, count in child_tags.items()))\n",
    "    else:\n",
    "        print(\"No children\")\n",
    "\n",
    "# Updated fallback function that can deliberately try fallbacks\n",
    "async def test_all_selectors(soup, selector_key, base_element=None, force_fallbacks=False):\n",
    "    \"\"\"Test all selectors (primary and fallbacks) for a given selector key\"\"\"\n",
    "    # Get the selector configuration\n",
    "    parts = selector_key.split(\".\")\n",
    "    if len(parts) != 2:\n",
    "        print(f\"Invalid selector key format: {selector_key}\")\n",
    "        return None\n",
    "        \n",
    "    section, name = parts\n",
    "    if section not in SELECTORS_WITH_FALLBACKS or name not in SELECTORS_WITH_FALLBACKS[section]:\n",
    "        print(f\"Selector not found: {selector_key}\")\n",
    "        return None\n",
    "        \n",
    "    selector_config = SELECTORS_WITH_FALLBACKS[section][name]\n",
    "    \n",
    "    # Use the base element or soup\n",
    "    element = base_element if base_element else soup\n",
    "    \n",
    "    # Test the primary selector\n",
    "    primary = selector_config[\"primary\"]\n",
    "    print(f\"\\n🔍 Testing selector {selector_key}\")\n",
    "    print(f\"Primary selector: {primary}\")\n",
    "    \n",
    "    tag, attrs = parse_selector(primary)\n",
    "    primary_result = element.find(tag, attrs)\n",
    "    \n",
    "    if primary_result and not force_fallbacks:\n",
    "        print(\"✅ Primary selector succeeded\")\n",
    "        inspect_element(primary_result)\n",
    "        return primary_result\n",
    "    else:\n",
    "        if not primary_result:\n",
    "            print(\"❌ Primary selector failed\")\n",
    "        else:\n",
    "            print(\"⚠️ Primary selector succeeded but testing fallbacks anyway\")\n",
    "    \n",
    "    # Test each fallback\n",
    "    for i, fallback in enumerate(selector_config[\"fallbacks\"]):\n",
    "        print(f\"\\nTrying fallback {i+1}: {fallback}\")\n",
    "        tag, attrs = parse_selector(fallback)\n",
    "        fallback_result = element.find(tag, attrs)\n",
    "        \n",
    "        if fallback_result:\n",
    "            print(f\"✅ Fallback {i+1} succeeded\")\n",
    "            inspect_element(fallback_result)\n",
    "            return fallback_result\n",
    "        else:\n",
    "            print(f\"❌ Fallback {i+1} failed\")\n",
    "    \n",
    "    print(\"\\n❌ All selectors failed\")\n",
    "    return None\n",
    "\n",
    "# Add a function to get a sample of publications for testing\n",
    "async def get_publication_samples(num_pages=2, pubs_per_page=3, random_delay=True):\n",
    "    \"\"\"Get a sample of publications from multiple pages for testing\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    }\n",
    "    \n",
    "    all_containers = []\n",
    "    \n",
    "    # Get publications from multiple pages\n",
    "    for page in range(num_pages):\n",
    "        page_url = scraper.base_url if page == 0 else f\"{scraper.base_url}?page={page}\"\n",
    "        print(f\"Fetching page {page}: {page_url}\")\n",
    "        \n",
    "        async with aiohttp.ClientSession(headers=headers) as session:\n",
    "            async with session.get(page_url) as response:\n",
    "                if response.status != 200:\n",
    "                    print(f\"Error fetching page {page}: {response.status}\")\n",
    "                    continue\n",
    "                \n",
    "                html = await response.text()\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                \n",
    "                # Find publication containers\n",
    "                tag, attrs = parse_selector(SELECTORS_WITH_FALLBACKS[\"publication\"][\"container\"][\"primary\"])\n",
    "                containers = soup.find_all(tag, attrs)\n",
    "                \n",
    "                if containers:\n",
    "                    print(f\"Found {len(containers)} publications on page {page}\")\n",
    "                    \n",
    "                    # Select a sample from this page\n",
    "                    import random\n",
    "                    sample_size = min(pubs_per_page, len(containers))\n",
    "                    if sample_size > 0:\n",
    "                        # Use random sampling to get diverse examples\n",
    "                        sampled_containers = random.sample(containers, sample_size)\n",
    "                        all_containers.extend([(soup, container, page) for container in sampled_containers])\n",
    "                else:\n",
    "                    print(f\"No publications found on page {page}\")\n",
    "        \n",
    "        # Add a random delay between page requests\n",
    "        if random_delay and page < num_pages - 1:\n",
    "            delay = random.uniform(1, 3)\n",
    "            print(f\"Waiting {delay:.1f} seconds before next request...\")\n",
    "            await asyncio.sleep(delay)\n",
    "    \n",
    "    return all_containers\n",
    "\n",
    "# Test function that thoroughly tests all selectors on multiple publications\n",
    "async def test_selectors_on_real_publications():\n",
    "    \"\"\"Test all selectors on a diverse sample of real publications\"\"\"\n",
    "    print(\"\\n🧪 Testing selectors on real publications across multiple pages\")\n",
    "    \n",
    "    # Get a sample of publications from different pages\n",
    "    samples = await get_publication_samples(num_pages=3, pubs_per_page=2)\n",
    "    \n",
    "    if not samples:\n",
    "        print(\"No publication samples found to test\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nCollected {len(samples)} publication samples for testing\")\n",
    "    \n",
    "    # Test all selectors on each publication\n",
    "    results = {\n",
    "        \"publication.title\": {\"success\": 0, \"failure\": 0},\n",
    "        \"publication.authors\": {\"success\": 0, \"failure\": 0},\n",
    "        \"publication.abstract\": {\"success\": 0, \"failure\": 0},\n",
    "        \"publication.file\": {\"success\": 0, \"failure\": 0}\n",
    "    }\n",
    "    \n",
    "    for i, (soup, container, page) in enumerate(samples):\n",
    "        print(f\"\\n📄 Testing publication sample {i+1} from page {page}\")\n",
    "        \n",
    "        # Test each selector\n",
    "        for selector_key in results.keys():\n",
    "            result = await test_all_selectors(soup, selector_key, container)\n",
    "            \n",
    "            if result:\n",
    "                results[selector_key][\"success\"] += 1\n",
    "            else:\n",
    "                results[selector_key][\"failure\"] += 1\n",
    "            \n",
    "            # Add a small delay between tests\n",
    "            await asyncio.sleep(0.5)\n",
    "    \n",
    "    # Print summary results\n",
    "    print(\"\\n📊 Selector Test Summary:\")\n",
    "    for selector_key, counts in results.items():\n",
    "        total = counts[\"success\"] + counts[\"failure\"]\n",
    "        success_rate = counts[\"success\"] / total if total > 0 else 0\n",
    "        status = \"✅ GOOD\" if success_rate == 1.0 else \"⚠️ WARNING\" if success_rate >= 0.5 else \"❌ FAILING\"\n",
    "        \n",
    "        print(f\"{selector_key}: {status} - {success_rate*100:.1f}% success ({counts['success']}/{total})\")\n",
    "\n",
    "# Run the real-world testing\n",
    "await test_selectors_on_real_publications()\n",
    "\n",
    "# Now, let's add a tool to help you inspect the HTML directly\n",
    "# %%\n",
    "# HTML Inspector Tool\n",
    "\n",
    "async def inspect_website_element(url=None, css_selector=None):\n",
    "    \"\"\"Fetch a website and inspect an element using CSS selector\"\"\"\n",
    "    if not url:\n",
    "        url = scraper.base_url\n",
    "    \n",
    "    print(f\"Fetching: {url}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching URL: {response.status}\")\n",
    "                return\n",
    "            \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            if not css_selector:\n",
    "                # If no selector provided, give page structure overview\n",
    "                print(\"\\n📄 Page Structure Overview:\")\n",
    "                \n",
    "                # Count tags by type\n",
    "                tag_counts = {}\n",
    "                for tag in soup.find_all(True):  # Find all tags\n",
    "                    tag_name = tag.name\n",
    "                    tag_counts[tag_name] = tag_counts.get(tag_name, 0) + 1\n",
    "                \n",
    "                print(\"Tags found:\")\n",
    "                for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:15]:\n",
    "                    print(f\"  {tag}: {count}\")\n",
    "                \n",
    "                # Count classes\n",
    "                class_counts = {}\n",
    "                for tag in soup.find_all(True):\n",
    "                    if \"class\" in tag.attrs:\n",
    "                        for cls in tag.attrs[\"class\"]:\n",
    "                            class_counts[cls] = class_counts.get(cls, 0) + 1\n",
    "                \n",
    "                print(\"\\nTop 15 classes:\")\n",
    "                for cls, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True)[:15]:\n",
    "                    print(f\"  {cls}: {count}\")\n",
    "                \n",
    "                return\n",
    "            \n",
    "            # Find the element using the provided CSS selector\n",
    "            try:\n",
    "                if \".\" in css_selector:\n",
    "                    parts = css_selector.split(\".\")\n",
    "                    tag = parts[0] if parts[0] else None\n",
    "                    cls = parts[1]\n",
    "                    elements = soup.find_all(tag, class_=cls)\n",
    "                else:\n",
    "                    elements = soup.find_all(css_selector)\n",
    "                \n",
    "                if elements:\n",
    "                    print(f\"\\nFound {len(elements)} elements matching '{css_selector}'\")\n",
    "                    \n",
    "                    # Inspect the first element in detail\n",
    "                    first_element = elements[0]\n",
    "                    inspect_element(first_element)\n",
    "                    \n",
    "                    # If there are multiple elements, show a summary of others\n",
    "                    if len(elements) > 1:\n",
    "                        print(f\"\\nSummary of other {len(elements)-1} elements:\")\n",
    "                        for i, element in enumerate(elements[1:5]):  # Show up to 4 more\n",
    "                            print(f\"\\nElement {i+2}:\")\n",
    "                            # Simplified inspection\n",
    "                            print(f\"Tag: <{element.name}>\")\n",
    "                            text = element.text.strip()\n",
    "                            if text:\n",
    "                                if len(text) > 50:\n",
    "                                    text = text[:47] + \"...\"\n",
    "                                print(f\"Text: \\\"{text}\\\"\")\n",
    "                    \n",
    "                    return first_element\n",
    "                else:\n",
    "                    print(f\"No elements found matching '{css_selector}'\")\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(f\"Error inspecting element with selector '{css_selector}': {e}\")\n",
    "                return None\n",
    "\n",
    "# Let's provide some examples of how to use the inspector\n",
    "print(\"\"\"\n",
    "# 🔍 Website Element Inspector Tool\n",
    "\n",
    "Use this tool to inspect elements on the website directly. Examples:\n",
    "\n",
    "1. Get page overview:\n",
    "   await inspect_website_element()\n",
    "\n",
    "2. Inspect publication containers:\n",
    "   await inspect_website_element(css_selector=\"div.biblio-entry\")\n",
    "\n",
    "3. Inspect titles:\n",
    "   await inspect_website_element(css_selector=\"span.biblio-title\")\n",
    "\n",
    "4. Inspect a specific page:\n",
    "   await inspect_website_element(url=\"https://growthlab.hks.harvard.edu/publications?page=5\")\n",
    "\n",
    "5. Try the selector you're having trouble with:\n",
    "   await inspect_website_element(css_selector=\"your-selector-here\")\n",
    "\"\"\")\n",
    "\n",
    "# Let's try it with the publication container\n",
    "element = await inspect_website_element(css_selector=\"div.biblio-entry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4780e6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Investigating publication with missing files\n",
      "Found 20 publications\n",
      "\n",
      "Publication 1 without file elements: Global Networks, Monetary Policy and Trade\n",
      "Found 3 links: ['Global Networks, Monetary Policy and Trade', \"Publisher's Version\", 'Abstract']\n",
      "\n",
      "Publication 15 without file elements: Leaving Home: Cumulative Climate Shocks and Migration in Sub-Saharan Africa\n",
      "Found 3 links: ['Leaving Home: Cumulative Climate Shocks and Migration in Sub-Saharan Africa', \"Publisher's Version\", 'Abstract']\n",
      "\n",
      "Publication 18 without file elements: GLocal: A global development dataset of subnational administrative areas\n",
      "Found 5 links: ['GLocal: A global development dataset of subnational administrative areas', \"Publisher's Version\", 'Abstract']\n",
      "\n",
      "Summary: 17/20 publications have file elements\n",
      "3 publications without file elements:\n",
      "1. Publication 1: Global Networks, Monetary Policy and Trade\n",
      "2. Publication 15: Leaving Home: Cumulative Climate Shocks and Migration in Sub-Saharan Africa\n",
      "3. Publication 18: GLocal: A global development dataset of subnational administrative areas\n",
      "\n",
      "Detailed inspection of publication without files:\n",
      "\n",
      "HTML structure:\n",
      "<div class=\"biblio-entry node node-biblio node-teaser contextual-links-region article clearfix\"><span class=\"biblio-authors\">Kalemli-Özcan, Ṣ., Soylu, C. &amp; Yildirim, M.A.</span>, 2025. <span class=\"biblio-title\"><a href=\"https://growthlab.hks.harvard.edu/publications/global-networks-monetary-policy-and-trade\">Global Networks, Monetary Policy and Trade</a></span>.<span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.title=Global+Networks%2C+M...\n",
      "\n",
      "Found 3 span elements:\n",
      "1. <span class='biblio-authors'>Kalemli-Özcan, Ṣ., Soylu, C. &...</span>\n",
      "2. <span class='biblio-title'>Global Networks, Monetary Poli...</span>\n",
      "3. <span class='Z3988'></span>\n",
      "\n",
      "🧪 Testing improved selectors\n",
      "Found 20 containers\n",
      "Testing with publication missing files: How would dollarisation affect Argentina’s competitiveness?\n",
      "\n",
      "Testing title selector:\n",
      "✅ Primary selector succeeded: span.biblio-title\n",
      "\n",
      "Testing authors selector:\n",
      "✅ Primary selector succeeded: span.biblio-authors\n",
      "\n",
      "Testing abstract selector:\n",
      "✅ Primary selector succeeded: div.biblio-abstract-display\n",
      "\n",
      "Testing file selector with fallbacks:\n",
      "❌ Primary selector failed: span.file\n",
      "Trying fallback 1: a.biblio-download\n",
      "Trying fallback 2: a[href$='.pdf']\n",
      "Trying fallback 3: a[href*='files']\n",
      "❌ All selectors failed\n",
      "\n",
      "📊 Improved Selector Results:\n",
      "Title: ✅ Found\n",
      "Authors: ✅ Found\n",
      "Abstract: ✅ Found\n",
      "File: ❌ Not found\n"
     ]
    }
   ],
   "source": [
    "# Improved selector configuration based on testing results\n",
    "\n",
    "# Let's first investigate the publication that had missing files\n",
    "async def investigate_missing_files():\n",
    "    \"\"\"Investigate the publication that had missing files\"\"\"\n",
    "    print(\"\\n🔍 Investigating publication with missing files\")\n",
    "    \n",
    "    # Let's fetch all publications and check their file elements\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(scraper.base_url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page: {response.status}\")\n",
    "                return\n",
    "                \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Get all publication containers\n",
    "            containers = soup.find_all(\"div\", class_=\"biblio-entry\")\n",
    "            print(f\"Found {len(containers)} publications\")\n",
    "            \n",
    "            # Check each publication for file elements\n",
    "            publications_with_files = 0\n",
    "            publications_without_files = []\n",
    "            \n",
    "            for i, container in enumerate(containers):\n",
    "                # Get title for reference\n",
    "                title_elem = container.find(\"span\", class_=\"biblio-title\")\n",
    "                title = title_elem.text.strip() if title_elem else f\"Publication {i+1}\"\n",
    "                \n",
    "                # Look for file elements\n",
    "                file_elems = container.find_all(\"span\", class_=\"file\")\n",
    "                \n",
    "                if file_elems:\n",
    "                    publications_with_files += 1\n",
    "                else:\n",
    "                    publications_without_files.append((i+1, title))\n",
    "                    # Let's look for any other link elements that might be files\n",
    "                    links = container.find_all(\"a\")\n",
    "                    link_texts = [link.text.strip() for link in links if link.text.strip()]\n",
    "                    print(f\"\\nPublication {i+1} without file elements: {title}\")\n",
    "                    print(f\"Found {len(links)} links: {link_texts[:3]}\")\n",
    "            \n",
    "            print(f\"\\nSummary: {publications_with_files}/{len(containers)} publications have file elements\")\n",
    "            if publications_without_files:\n",
    "                print(f\"{len(publications_without_files)} publications without file elements:\")\n",
    "                for i, (num, title) in enumerate(publications_without_files[:5]):\n",
    "                    print(f\"{i+1}. Publication {num}: {title}\")\n",
    "                \n",
    "                # Let's check one of these publications more thoroughly\n",
    "                if publications_without_files:\n",
    "                    pub_num = publications_without_files[0][0] - 1  # Zero-indexed\n",
    "                    problem_pub = containers[pub_num]\n",
    "                    print(\"\\nDetailed inspection of publication without files:\")\n",
    "                    \n",
    "                    # Print the raw HTML to see the structure\n",
    "                    html_snippet = str(problem_pub)[:500] + \"...\" if len(str(problem_pub)) > 500 else str(problem_pub)\n",
    "                    print(f\"\\nHTML structure:\\n{html_snippet}\")\n",
    "                    \n",
    "                    # Look for any spans that might contain file info\n",
    "                    spans = problem_pub.find_all(\"span\")\n",
    "                    print(f\"\\nFound {len(spans)} span elements:\")\n",
    "                    for i, span in enumerate(spans):\n",
    "                        cls = span.get(\"class\", [\"no-class\"])\n",
    "                        text = span.text.strip()\n",
    "                        print(f\"{i+1}. <span class='{' '.join(cls)}'>{text[:30]}{'...' if len(text) > 30 else ''}</span>\")\n",
    "\n",
    "# Let's run the investigation\n",
    "await investigate_missing_files()\n",
    "\n",
    "# Now, create an improved selector configuration based on our findings\n",
    "IMPROVED_SELECTORS = {\n",
    "    \"publication\": {\n",
    "        \"container\": {\n",
    "            \"primary\": \"div.biblio-entry\",\n",
    "            \"fallbacks\": [\"div.node-biblio\", \"article.node-biblio\"],\n",
    "            \"xpath\": \"//div[contains(@class, 'biblio-entry')]\"\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"primary\": \"span.biblio-title\",\n",
    "            \"fallbacks\": [\"h2.title\", \"div.title\", \"a.biblio-title-link\"],\n",
    "            \"xpath\": \"//span[contains(@class, 'biblio-title')]\"\n",
    "        },\n",
    "        \"authors\": {\n",
    "            \"primary\": \"span.biblio-authors\",\n",
    "            \"fallbacks\": [\"div.biblio-authors\", \"p.biblio-authors\"],\n",
    "            \"xpath\": \"//span[contains(@class, 'biblio-authors')]\"\n",
    "        },\n",
    "        \"abstract\": {\n",
    "            \"primary\": \"div.biblio-abstract-display\",\n",
    "            \"fallbacks\": [\"div.abstract\", \"div.field-biblio-abstract\"],\n",
    "            \"xpath\": \"//div[contains(@class, 'biblio-abstract')]\"\n",
    "        },\n",
    "        \"file\": {\n",
    "            \"primary\": \"span.file\",\n",
    "            \"fallbacks\": [\n",
    "                \"a.biblio-download\", \n",
    "                \"a[href$='.pdf']\",\n",
    "                \"a[href*='files']\"\n",
    "            ],\n",
    "            \"xpath\": \"//span[contains(@class, 'file')]\"\n",
    "        }\n",
    "    },\n",
    "    \"pagination\": {\n",
    "        \"container\": {\n",
    "            \"primary\": \"ul.pager\",\n",
    "            \"fallbacks\": [\"nav.pagination\", \"div.pagination\"],\n",
    "            \"xpath\": \"//ul[contains(@class, 'pager')]\"\n",
    "        },\n",
    "        \"last_page\": {\n",
    "            \"primary\": \"li.pager-last\",\n",
    "            \"fallbacks\": [\"li.page-item:last-child\", \"a.page-link:last-child\"],\n",
    "            \"xpath\": \"//li[contains(@class, 'pager-last')]\"\n",
    "        }\n",
    "    },\n",
    "    \"endnote\": {\n",
    "        \"link\": {\n",
    "            \"primary\": \"li.biblio_tagged a\",\n",
    "            \"fallbacks\": [\"a[href*='tagged=1']\", \"a[href*='endnote']\", \"a.endnote-link\"],\n",
    "            \"xpath\": \"//a[contains(@href, 'tagged=1')]\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a more robust implementation of parse_selector that handles complex CSS\n",
    "def parse_advanced_selector(selector):\n",
    "    \"\"\"Parse a CSS selector into BeautifulSoup find arguments, handling more complex cases\"\"\"\n",
    "    # Handle attribute selectors like a[href$='.pdf']\n",
    "    if \"[\" in selector and \"]\" in selector:\n",
    "        tag_part = selector.split(\"[\")[0]\n",
    "        attr_part = selector[selector.find(\"[\")+1:selector.find(\"]\")]\n",
    "        \n",
    "        # Process the tag part\n",
    "        if \".\" in tag_part:\n",
    "            tag, class_name = tag_part.split(\".\", 1)\n",
    "            attrs = {\"class\": class_name}\n",
    "        else:\n",
    "            tag = tag_part\n",
    "            attrs = {}\n",
    "        \n",
    "        # Process the attribute part\n",
    "        if \"=\" in attr_part:\n",
    "            attr_name, attr_value = attr_part.split(\"=\", 1)\n",
    "            \n",
    "            # Handle special attribute selectors\n",
    "            if attr_value.startswith(\"$\") and attr_value.endswith(\"'\"):\n",
    "                # Ends with selector - we'll handle this specially\n",
    "                return tag, attrs, attr_name, attr_value[2:-1], \"ends_with\"\n",
    "            elif attr_value.startswith(\"*\") and attr_value.endswith(\"'\"):\n",
    "                # Contains selector\n",
    "                return tag, attrs, attr_name, attr_value[2:-1], \"contains\"\n",
    "            else:\n",
    "                # Regular equals\n",
    "                attrs[attr_name] = attr_value.strip(\"'\\\"\")\n",
    "        \n",
    "        return tag or None, attrs\n",
    "    \n",
    "    # Handle simple class selectors\n",
    "    elif \".\" in selector:\n",
    "        parts = selector.split(\".\")\n",
    "        tag = parts[0] if parts[0] else None\n",
    "        class_name = parts[1]\n",
    "        return tag, {\"class\": class_name}\n",
    "    \n",
    "    # Handle just tag selectors\n",
    "    else:\n",
    "        return selector, {}\n",
    "\n",
    "# Create an improved finder function that handles the special attribute selectors\n",
    "async def find_with_improved_selectors(soup, selector_config, base_element=None):\n",
    "    \"\"\"Find elements using improved selector parsing\"\"\"\n",
    "    element = base_element if base_element else soup\n",
    "    \n",
    "    # Try primary selector\n",
    "    primary = selector_config[\"primary\"]\n",
    "    if primary:\n",
    "        # Parse and apply the selector\n",
    "        try:\n",
    "            parsed = parse_advanced_selector(primary)\n",
    "            \n",
    "            # Handle special attribute selectors\n",
    "            if len(parsed) > 2:\n",
    "                tag, attrs, attr_name, attr_value, match_type = parsed\n",
    "                \n",
    "                # Find all matching elements and filter\n",
    "                candidates = element.find_all(tag, attrs) if tag else element.find_all(True, attrs)\n",
    "                \n",
    "                if match_type == \"ends_with\":\n",
    "                    result = next((el for el in candidates if attr_name in el.attrs and \n",
    "                                  el[attr_name].endswith(attr_value)), None)\n",
    "                elif match_type == \"contains\":\n",
    "                    result = next((el for el in candidates if attr_name in el.attrs and \n",
    "                                  attr_value in el[attr_name]), None)\n",
    "                else:\n",
    "                    result = None\n",
    "            else:\n",
    "                tag, attrs = parsed\n",
    "                result = element.find(tag, attrs)\n",
    "                \n",
    "            if result:\n",
    "                print(f\"✅ Primary selector succeeded: {primary}\")\n",
    "                return result\n",
    "            else:\n",
    "                print(f\"❌ Primary selector failed: {primary}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with primary selector {primary}: {e}\")\n",
    "    \n",
    "    # Try fallbacks\n",
    "    for i, fallback in enumerate(selector_config[\"fallbacks\"]):\n",
    "        print(f\"Trying fallback {i+1}: {fallback}\")\n",
    "        \n",
    "        try:\n",
    "            parsed = parse_advanced_selector(fallback)\n",
    "            \n",
    "            # Handle special attribute selectors\n",
    "            if len(parsed) > 2:\n",
    "                tag, attrs, attr_name, attr_value, match_type = parsed\n",
    "                \n",
    "                # Find all matching elements and filter\n",
    "                candidates = element.find_all(tag, attrs) if tag else element.find_all(True, attrs)\n",
    "                \n",
    "                if match_type == \"ends_with\":\n",
    "                    result = next((el for el in candidates if attr_name in el.attrs and \n",
    "                                  el[attr_name].endswith(attr_value)), None)\n",
    "                elif match_type == \"contains\":\n",
    "                    result = next((el for el in candidates if attr_name in el.attrs and \n",
    "                                  attr_value in el[attr_name]), None)\n",
    "                else:\n",
    "                    result = None\n",
    "            else:\n",
    "                tag, attrs = parsed\n",
    "                result = element.find(tag, attrs)\n",
    "            \n",
    "            if result:\n",
    "                print(f\"✅ Fallback {i+1} succeeded: {fallback}\")\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error with fallback selector {fallback}: {e}\")\n",
    "    \n",
    "    # Try XPath as last resort\n",
    "    if selector_config.get(\"xpath\"):\n",
    "        try:\n",
    "            import lxml.html\n",
    "            from lxml import etree\n",
    "            \n",
    "            # Convert BeautifulSoup object to lxml for XPath support\n",
    "            dom = lxml.html.fromstring(str(element))\n",
    "            xpath_result = dom.xpath(selector_config[\"xpath\"])\n",
    "            \n",
    "            if xpath_result:\n",
    "                print(f\"✅ XPath selector succeeded: {selector_config['xpath']}\")\n",
    "                \n",
    "                # Convert back to soup to maintain consistency\n",
    "                result_html = etree.tostring(xpath_result[0])\n",
    "                result_soup = BeautifulSoup(result_html, \"html.parser\")\n",
    "                return result_soup.contents[0] if result_soup.contents else None\n",
    "        except ImportError:\n",
    "            print(\"lxml not available for XPath queries\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with XPath selector: {e}\")\n",
    "    \n",
    "    print(\"❌ All selectors failed\")\n",
    "    return None\n",
    "\n",
    "# Test our improved selectors\n",
    "async def test_improved_selectors():\n",
    "    \"\"\"Test our improved selectors on a publication that was missing files\"\"\"\n",
    "    print(\"\\n🧪 Testing improved selectors\")\n",
    "    \n",
    "    # Fetch a page\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        # Try the page where we had a file issue\n",
    "        async with session.get(f\"{scraper.base_url}?page=1\") as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page: {response.status}\")\n",
    "                return\n",
    "                \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Get all containers\n",
    "            containers = soup.find_all(\"div\", class_=\"biblio-entry\")\n",
    "            \n",
    "            if containers:\n",
    "                print(f\"Found {len(containers)} containers\")\n",
    "                \n",
    "                # Look for a container likely to be missing files\n",
    "                test_container = None\n",
    "                for container in containers:\n",
    "                    if not container.find(\"span\", class_=\"file\"):\n",
    "                        test_container = container\n",
    "                        title_elem = container.find(\"span\", class_=\"biblio-title\")\n",
    "                        title = title_elem.text.strip() if title_elem else \"Unknown\"\n",
    "                        print(f\"Testing with publication missing files: {title}\")\n",
    "                        break\n",
    "                \n",
    "                if not test_container:\n",
    "                    print(\"No publication without files found, using first publication\")\n",
    "                    test_container = containers[0]\n",
    "                \n",
    "                # Test our selectors\n",
    "                print(\"\\nTesting title selector:\")\n",
    "                title_result = await find_with_improved_selectors(\n",
    "                    soup, IMPROVED_SELECTORS[\"publication\"][\"title\"], test_container\n",
    "                )\n",
    "                \n",
    "                print(\"\\nTesting authors selector:\")\n",
    "                authors_result = await find_with_improved_selectors(\n",
    "                    soup, IMPROVED_SELECTORS[\"publication\"][\"authors\"], test_container\n",
    "                )\n",
    "                \n",
    "                print(\"\\nTesting abstract selector:\")\n",
    "                abstract_result = await find_with_improved_selectors(\n",
    "                    soup, IMPROVED_SELECTORS[\"publication\"][\"abstract\"], test_container\n",
    "                )\n",
    "                \n",
    "                print(\"\\nTesting file selector with fallbacks:\")\n",
    "                file_result = await find_with_improved_selectors(\n",
    "                    soup, IMPROVED_SELECTORS[\"publication\"][\"file\"], test_container\n",
    "                )\n",
    "                \n",
    "                # Print a summary of our results\n",
    "                print(\"\\n📊 Improved Selector Results:\")\n",
    "                print(f\"Title: {'✅ Found' if title_result else '❌ Not found'}\")\n",
    "                print(f\"Authors: {'✅ Found' if authors_result else '❌ Not found'}\")\n",
    "                print(f\"Abstract: {'✅ Found' if abstract_result else '❌ Not found'}\")\n",
    "                print(f\"File: {'✅ Found' if file_result else '❌ Not found'}\")\n",
    "                \n",
    "                if file_result:\n",
    "                    print(f\"\\nFile element found: {file_result.name}\")\n",
    "                    print(f\"File text: {file_result.text.strip()}\")\n",
    "                    if file_result.name == \"a\":\n",
    "                        print(f\"File URL: {file_result.get('href')}\")\n",
    "\n",
    "# Run the improved selector test\n",
    "await test_improved_selectors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a3392a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Testing new file selectors on specific publication\n",
      "Testing selectors on publication page: https://growthlab.hks.harvard.edu/publications/global-networks-monetary-policy-and-trade\n",
      "\n",
      "Testing file selectors individually:\n",
      "1. span.file: 0 elements found\n",
      "2. #pub-cover-content-wrapper a: 1 elements found\n",
      "   - https://www.nber.org/system/files/working_papers/w33686/w33686.pdf (Publisher's Version)\n",
      "3. .Z3988+ a: 0 elements found\n",
      "4. a[href$='.pdf']: 1 elements found\n",
      "   - https://www.nber.org/system/files/working_papers/w33686/w33686.pdf (Publisher's Version)\n",
      "5. Combined XPath: 1 elements found\n",
      "   - https://www.nber.org/system/files/working_papers/w33686/w33686.pdf (Publisher's Version)\n",
      "\n",
      "Testing with improved finder function:\n",
      "❌ Primary selector failed: span.file\n",
      "Trying fallback 1: a.biblio-download\n",
      "Trying fallback 2: #pub-cover-content-wrapper a\n",
      "Trying fallback 3: .Z3988+ a\n",
      "Trying fallback 4: a[href$='.pdf']\n",
      "Trying fallback 5: a[href*='files']\n",
      "✅ XPath selector succeeded: //*[(@id = 'pub-cover-content-wrapper')]//a | //*[contains(concat(' ', @class, ' '), ' Z3988 ')]//following-sibling::a | //span[contains(@class, 'file')]\n",
      "✅ Found file element using selectors\n",
      "Element: <a>\n",
      "URL: https://www.nber.org/system/files/working_papers/w33686/w33686.pdf\n",
      "Text: Publisher's Version\n",
      "\n",
      "🔍 Testing file selectors on publication listing page\n",
      "Found 20 publications on listing\n",
      "1. Export-led Growth: ✅ Found 3 files\n",
      "   - span.file: 1\n",
      "   - .Z3988+ a: 1\n",
      "   - a[href$='.pdf']: 1\n",
      "2. Supply-Side Economics of a Good Type: Supporting and Expanding South Africa’s Informal Economy: ✅ Found 3 files\n",
      "   - span.file: 1\n",
      "   - .Z3988+ a: 1\n",
      "   - a[href$='.pdf']: 1\n",
      "3. How would dollarisation affect Argentina’s competitiveness?: ✅ Found 1 files\n",
      "   - span.file: 0\n",
      "   - .Z3988+ a: 1\n",
      "   - a[href$='.pdf']: 0\n",
      "4. Harnessing Global Value Chains for Regional Development: How to Upgrade through Regional Policy, FDI, and Trade: ✅ Found 3 files\n",
      "   - span.file: 1\n",
      "   - .Z3988+ a: 1\n",
      "   - a[href$='.pdf']: 1\n",
      "5. What Economic Challenges Does Argentina Face Today?: ✅ Found 1 files\n",
      "   - span.file: 0\n",
      "   - .Z3988+ a: 1\n",
      "   - a[href$='.pdf']: 0\n",
      "6. Japan's Economic Puzzle: ✅ Found 3 files\n",
      "   - span.file: 1\n",
      "   - .Z3988+ a: 1\n",
      "   - a[href$='.pdf']: 1\n",
      "7. A journey through time: the story behind ‘eight decades of changes in occupational tasks, computerization and the gender pay gap’: ✅ Found 1 files\n",
      "   - span.file: 0\n",
      "   - .Z3988+ a: 1\n",
      "   - a[href$='.pdf']: 0\n",
      "8. Women Seeking Jobs with Limited Information: Evidence from Iraq: ✅ Found 3 files\n",
      "   - span.file: 1\n",
      "   - .Z3988+ a: 1\n",
      "   - a[href$='.pdf']: 1\n",
      "9. On the Design of Effective Sanctions: The Case of Bans on Exports to Russia: ✅ Found 1 files\n",
      "   - span.file: 0\n",
      "   - .Z3988+ a: 1\n",
      "   - a[href$='.pdf']: 0\n",
      "10. Grants in Wyoming: Constraints and Solutions: ✅ Found 3 files\n",
      "   - span.file: 1\n",
      "   - .Z3988+ a: 1\n",
      "   - a[href$='.pdf']: 1\n",
      "\n",
      "Summary: 10/10 publications have files using our selectors\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Testing the newly identified selectors for file links\n",
    "\n",
    "# Update our improved selector config with the new selectors\n",
    "IMPROVED_SELECTORS[\"publication\"][\"file\"][\"fallbacks\"] = [\n",
    "    \"a.biblio-download\", \n",
    "    \"#pub-cover-content-wrapper a\",  # New selector from SelectorGadget\n",
    "    \".Z3988+ a\",                     # New selector (element after Z3988)\n",
    "    \"a[href$='.pdf']\",\n",
    "    \"a[href*='files']\"\n",
    "]\n",
    "IMPROVED_SELECTORS[\"publication\"][\"file\"][\"xpath\"] = \"//*[(@id = 'pub-cover-content-wrapper')]//a | //*[contains(concat(' ', @class, ' '), ' Z3988 ')]//following-sibling::a | //span[contains(@class, 'file')]\"\n",
    "\n",
    "async def test_specific_publication_file_selectors():\n",
    "    \"\"\"Test the newly identified file selectors on a specific publication\"\"\"\n",
    "    print(\"\\n🔍 Testing new file selectors on specific publication\")\n",
    "    \n",
    "    # Fetch the specific publication page\n",
    "    url = \"https://growthlab.hks.harvard.edu/publications/global-networks-monetary-policy-and-trade\"\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page: {response.status}\")\n",
    "                return\n",
    "                \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            print(f\"Testing selectors on publication page: {url}\")\n",
    "            \n",
    "            # Test each file selector one by one\n",
    "            print(\"\\nTesting file selectors individually:\")\n",
    "            \n",
    "            # 1. Test span.file (original)\n",
    "            file_spans = soup.find_all(\"span\", class_=\"file\")\n",
    "            print(f\"1. span.file: {len(file_spans)} elements found\")\n",
    "            if file_spans:\n",
    "                for span in file_spans[:2]:  # Show first 2\n",
    "                    links = span.find_all(\"a\")\n",
    "                    for link in links:\n",
    "                        print(f\"   - {link.get('href', 'No href')} ({link.text.strip()})\")\n",
    "            \n",
    "            # 2. Test #pub-cover-content-wrapper a\n",
    "            wrapper = soup.find(id=\"pub-cover-content-wrapper\")\n",
    "            if wrapper:\n",
    "                cover_links = wrapper.find_all(\"a\")\n",
    "                print(f\"2. #pub-cover-content-wrapper a: {len(cover_links)} elements found\")\n",
    "                for link in cover_links[:2]:  # Show first 2\n",
    "                    print(f\"   - {link.get('href', 'No href')} ({link.text.strip()})\")\n",
    "            else:\n",
    "                print(\"2. #pub-cover-content-wrapper a: No wrapper found\")\n",
    "            \n",
    "            # 3. Test .Z3988+ a\n",
    "            z3988_elements = soup.find_all(class_=\"Z3988\")\n",
    "            z3988_next_links = []\n",
    "            for z3988 in z3988_elements:\n",
    "                next_element = z3988.find_next_sibling(\"a\")\n",
    "                if next_element:\n",
    "                    z3988_next_links.append(next_element)\n",
    "            \n",
    "            print(f\"3. .Z3988+ a: {len(z3988_next_links)} elements found\")\n",
    "            for link in z3988_next_links[:2]:  # Show first 2\n",
    "                print(f\"   - {link.get('href', 'No href')} ({link.text.strip()})\")\n",
    "            \n",
    "            # 4. Test a[href$='.pdf']\n",
    "            pdf_links = soup.find_all(\"a\", href=lambda h: h and h.endswith(\".pdf\"))\n",
    "            print(f\"4. a[href$='.pdf']: {len(pdf_links)} elements found\")\n",
    "            for link in pdf_links[:2]:  # Show first 2\n",
    "                print(f\"   - {link.get('href', 'No href')} ({link.text.strip()})\")\n",
    "            \n",
    "            # 5. Test combined XPath\n",
    "            try:\n",
    "                import lxml.html\n",
    "                dom = lxml.html.fromstring(html)\n",
    "                xpath_elements = dom.xpath(IMPROVED_SELECTORS[\"publication\"][\"file\"][\"xpath\"])\n",
    "                print(f\"5. Combined XPath: {len(xpath_elements)} elements found\")\n",
    "                for element in xpath_elements[:2]:  # Show first 2\n",
    "                    text = element.text_content().strip()\n",
    "                    href = element.get(\"href\", \"No href\")\n",
    "                    print(f\"   - {href} ({text})\")\n",
    "            except ImportError:\n",
    "                print(\"5. Combined XPath: lxml not available\")\n",
    "            except Exception as e:\n",
    "                print(f\"5. Combined XPath: Error - {e}\")\n",
    "            \n",
    "            # Now let's test our improved finder function\n",
    "            print(\"\\nTesting with improved finder function:\")\n",
    "            file_result = await find_with_improved_selectors(\n",
    "                soup, IMPROVED_SELECTORS[\"publication\"][\"file\"]\n",
    "            )\n",
    "            \n",
    "            if file_result:\n",
    "                print(f\"✅ Found file element using selectors\")\n",
    "                print(f\"Element: <{file_result.name}>\")\n",
    "                if file_result.name == \"a\":\n",
    "                    print(f\"URL: {file_result.get('href')}\")\n",
    "                    print(f\"Text: {file_result.text.strip()}\")\n",
    "                else:\n",
    "                    links = file_result.find_all(\"a\")\n",
    "                    print(f\"Contains {len(links)} links:\")\n",
    "                    for link in links:\n",
    "                        print(f\"- {link.get('href')} ({link.text.strip()})\")\n",
    "            else:\n",
    "                print(\"❌ No file element found with any selector\")\n",
    "\n",
    "# Let's also check the publication listings to see if our new selectors work there\n",
    "async def test_file_selectors_on_listings():\n",
    "    \"\"\"Test our updated file selectors on the publication listings\"\"\"\n",
    "    print(\"\\n🔍 Testing file selectors on publication listing page\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(f\"{scraper.base_url}?page=1\") as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page: {response.status}\")\n",
    "                return\n",
    "                \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Get all containers\n",
    "            containers = soup.find_all(\"div\", class_=\"biblio-entry\")\n",
    "            \n",
    "            if not containers:\n",
    "                print(\"No publications found on listing page\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Found {len(containers)} publications on listing\")\n",
    "            \n",
    "            # Count publications with and without files using our improved selectors\n",
    "            with_files = 0\n",
    "            without_files = []\n",
    "            \n",
    "            for i, container in enumerate(containers[:10]):  # Test first 10\n",
    "                title_elem = container.find(\"span\", class_=\"biblio-title\")\n",
    "                title = title_elem.text.strip() if title_elem else f\"Publication {i+1}\"\n",
    "                \n",
    "                # Try our original file selector\n",
    "                span_files = container.find_all(\"span\", class_=\"file\")\n",
    "                \n",
    "                # Try Z3988 selector\n",
    "                z3988_elements = container.find_all(class_=\"Z3988\")\n",
    "                z3988_next_links = []\n",
    "                for z3988 in z3988_elements:\n",
    "                    next_element = z3988.find_next_sibling(\"a\")\n",
    "                    if next_element:\n",
    "                        z3988_next_links.append(next_element)\n",
    "                \n",
    "                # Try PDF links\n",
    "                pdf_links = container.find_all(\"a\", href=lambda h: h and h.endswith(\".pdf\"))\n",
    "                \n",
    "                # Count total files found with either method\n",
    "                total_files = len(span_files) + len(z3988_next_links) + len(pdf_links)\n",
    "                \n",
    "                if total_files > 0:\n",
    "                    with_files += 1\n",
    "                    print(f\"{i+1}. {title}: ✅ Found {total_files} files\")\n",
    "                    print(f\"   - span.file: {len(span_files)}\")\n",
    "                    print(f\"   - .Z3988+ a: {len(z3988_next_links)}\")\n",
    "                    print(f\"   - a[href$='.pdf']: {len(pdf_links)}\")\n",
    "                else:\n",
    "                    without_files.append((i+1, title))\n",
    "                    print(f\"{i+1}. {title}: ❌ No files found with any selector\")\n",
    "            \n",
    "            print(f\"\\nSummary: {with_files}/{len(containers[:10])} publications have files using our selectors\")\n",
    "            \n",
    "            # For publications still without files, check if they have any links at all\n",
    "            if without_files:\n",
    "                print(\"\\nInspecting publications without files:\")\n",
    "                for num, title in without_files:\n",
    "                    container = containers[num-1]\n",
    "                    links = container.find_all(\"a\")\n",
    "                    print(f\"Publication {num}: {title}\")\n",
    "                    print(f\"Total links: {len(links)}\")\n",
    "                    \n",
    "                    # Show the first few links\n",
    "                    for i, link in enumerate(links[:3]):\n",
    "                        href = link.get(\"href\", \"No href\")\n",
    "                        text = link.text.strip()\n",
    "                        print(f\"- Link {i+1}: {href} ({text})\")\n",
    "\n",
    "# Run the tests\n",
    "await test_specific_publication_file_selectors()\n",
    "await test_file_selectors_on_listings()\n",
    "\n",
    "# Create the final selector configuration with all our findings\n",
    "FINAL_SELECTORS = {\n",
    "    \"publication\": {\n",
    "        \"container\": {\n",
    "            \"primary\": \"div.biblio-entry\",\n",
    "            \"fallbacks\": [\"div.node-biblio\", \"article.node-biblio\"],\n",
    "            \"xpath\": \"//div[contains(@class, 'biblio-entry')]\"\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"primary\": \"span.biblio-title\",\n",
    "            \"fallbacks\": [\"h2.title\", \"a.biblio-title-link\"],\n",
    "            \"xpath\": \"//span[contains(@class, 'biblio-title')]\"\n",
    "        },\n",
    "        \"authors\": {\n",
    "            \"primary\": \"span.biblio-authors\", \n",
    "            \"fallbacks\": [\"div.biblio-authors\", \"span.field-biblio-authors\"],\n",
    "            \"xpath\": \"//span[contains(@class, 'biblio-authors')]\"\n",
    "        },\n",
    "        \"abstract\": {\n",
    "            \"primary\": \"div.biblio-abstract-display\",\n",
    "            \"fallbacks\": [\"div.abstract\", \"div.field-biblio-abstract\"],\n",
    "            \"xpath\": \"//div[contains(@class, 'biblio-abstract')]\"\n",
    "        },\n",
    "        \"file\": {\n",
    "            \"primary\": \"span.file\",\n",
    "            \"fallbacks\": [\n",
    "                \".Z3988+ a\",                     # Element after Z3988\n",
    "                \"#pub-cover-content-wrapper a\",  # Cover content links\n",
    "                \"a.biblio-download\", \n",
    "                \"a[href$='.pdf']\",\n",
    "                \"a[href*='files']\"\n",
    "            ],\n",
    "            \"xpath\": \"//*[contains(@class, 'file')] | //*[contains(@class, 'Z3988')]/following-sibling::a | //*[@id='pub-cover-content-wrapper']//a | //a[contains(@href, '.pdf')]\"\n",
    "        }\n",
    "    },\n",
    "    \"pagination\": {\n",
    "        \"container\": {\n",
    "            \"primary\": \"ul.pager\",\n",
    "            \"fallbacks\": [\"nav.pagination\", \"div.pagination\"],\n",
    "            \"xpath\": \"//ul[contains(@class, 'pager')]\"\n",
    "        },\n",
    "        \"last_page\": {\n",
    "            \"primary\": \"li.pager-last\",\n",
    "            \"fallbacks\": [\"li:last-child\", \"a:last-child\"],\n",
    "            \"xpath\": \"//li[contains(@class, 'pager-last')]\"\n",
    "        }\n",
    "    },\n",
    "    \"endnote\": {\n",
    "        \"link\": {\n",
    "            \"primary\": \"li.biblio_tagged a\",\n",
    "            \"fallbacks\": [\"a[href*='tagged=1']\", \"a[href*='endnote']\"],\n",
    "            \"xpath\": \"//a[contains(@href, 'tagged=1')]\"\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd2b2aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Finding XPath alternatives for critical elements\n",
      "Found publication container, examining critical elements:\n",
      "\n",
      "1. TITLE ELEMENT:\n",
      "Tag: <span>\n",
      "Classes: ['biblio-title']\n",
      "Selenium not available for XPath suggestions\n",
      "Found 20 title elements on page\n",
      "First few title texts:\n",
      "  \"Global Networks, Monetary Policy and Trade\"\n",
      "  \"Industrial policy for competitiveness in the energy transition\"\n",
      "  \"Public-Private Dialogs to Spur Export-led Growth: The Case of Productivity Taskforces in Namibia\"\n",
      "\n",
      "2. AUTHORS ELEMENT:\n",
      "Tag: <span>\n",
      "Classes: ['biblio-authors']\n",
      "Potential XPath expressions:\n",
      "  //span[@class='biblio-authors']\n",
      "  //span[contains(@class, 'biblio-authors')]\n",
      "  //span[contains(@class, 'authors')]\n",
      "Found 20 author elements on page\n",
      "First few author texts:\n",
      "  \"Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\"\n",
      "  \"Ahuja, K. & Hausmann, R.\"\n",
      "  \"Fortunato, A. & Santos, M.A.\"\n",
      "\n",
      "3. ABSTRACT ELEMENT:\n",
      "Tag: <div>\n",
      "Classes: ['biblio-abstract-display', 'os-slider']\n",
      "Potential XPath expressions:\n",
      "  //div[@class='biblio-abstract-display']\n",
      "  //div[contains(@class, 'biblio-abstract-display')]\n",
      "  //div[contains(@class, 'abstract')]\n",
      "Found 20 abstract elements on page\n",
      "First abstract text (truncated):\n",
      "  \"We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\"\n",
      "\n",
      "🧪 Testing selectors on publications from all 21 pages\n",
      "(Sampling 3 publications per page)\n",
      "\n",
      "Fetching page 0: https://growthlab.hks.harvard.edu/publications\n",
      "Found 20 publications on page 0\n",
      "\n",
      "Testing publication 1 from page 0:\n",
      "✅ Title found using span.biblio-title: \"Innovation Policies Under Economic Complexity\"\n",
      "✅ Authors found using span.biblio-authors: \"Chacua, C., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Recent geopolitical challenges have revived the im...\"\n",
      "\n",
      "Testing publication 2 from page 0:\n",
      "✅ Title found using span.biblio-title: \"Global Networks, Monetary Policy and Trade\"\n",
      "✅ Authors found using span.biblio-authors: \"Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"We develop a novel framework to study the interact...\"\n",
      "\n",
      "Testing publication 3 from page 0:\n",
      "✅ Title found using span.biblio-title: \"GLocal: A global development dataset of subnationa\"\n",
      "✅ Authors found using span.biblio-authors: \"Morales-Arilla, J. & Gadgin Matha, S.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"The purpose of the GLocal dataset is to enable res...\"\n",
      "\n",
      "Fetching page 1: https://growthlab.hks.harvard.edu/publications?page=1\n",
      "Found 20 publications on page 1\n",
      "\n",
      "Testing publication 1 from page 1:\n",
      "✅ Title found using span.biblio-title: \"A journey through time: the story behind ‘eight de\"\n",
      "❌ Authors not found\n",
      "✅ Abstract found using div.biblio-abstract-display: \"In this interview article, we embark on a fascinat...\"\n",
      "\n",
      "Testing publication 2 from page 1:\n",
      "✅ Title found using span.biblio-title: \"Food for Growth: A Diagnostics of Namibia’s Agricu\"\n",
      "✅ Authors found using span.biblio-authors: \"Fortunato, A. & Enciso, S.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"This growth diagnostic report analyzes the economi...\"\n",
      "\n",
      "Testing publication 3 from page 1:\n",
      "✅ Title found using span.biblio-title: \"Towards a Sustainable Recovery for Lebanon’s Econo\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Lebanon’s current economic crisis ranks among the ...\"\n",
      "\n",
      "Fetching page 2: https://growthlab.hks.harvard.edu/publications?page=2\n",
      "Found 20 publications on page 2\n",
      "\n",
      "Testing publication 1 from page 2:\n",
      "✅ Title found using span.biblio-title: \"Scaling Partnerships to Activate Idle Community La\"\n",
      "✅ Authors found using span.biblio-authors: \"Klinger, B., Ordonez, I. & Sturzenegger, F.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"We discuss three cases of corporate-smallholder pa...\"\n",
      "\n",
      "Testing publication 2 from page 2:\n",
      "✅ Title found using span.biblio-title: \"A Growth Perspective on Wyoming\"\n",
      "✅ Authors found using span.biblio-authors: \"Bùi, T.-N., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"This report sets out to understand if the economy ...\"\n",
      "\n",
      "Testing publication 3 from page 2:\n",
      "✅ Title found using span.biblio-title: \"Una historia de la economía de dos Amazonias: Lecc\"\n",
      "✅ Authors found using span.biblio-authors: \"Cheston, T. & Rueda-Sanz, A.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"A menudo se piensa que alcanzar la prosperidad eco...\"\n",
      "\n",
      "Fetching page 3: https://growthlab.hks.harvard.edu/publications?page=3\n",
      "Found 20 publications on page 3\n",
      "\n",
      "Testing publication 1 from page 3:\n",
      "✅ Title found using span.biblio-title: \"The Value of Early-Career Skills\"\n",
      "✅ Authors found using span.biblio-authors: \"Langer, C. & Wiederhold, S.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"We develop novel measures of early-career skills t...\"\n",
      "\n",
      "Testing publication 2 from page 3:\n",
      "✅ Title found using span.biblio-title: \"Birthplace diversity and economic complexity: Cros\"\n",
      "✅ Authors found using span.biblio-authors: \"Bahar, D., Rapoport, H. & Turati, R.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"We empirically investigate the relationship betwee...\"\n",
      "\n",
      "Testing publication 3 from page 3:\n",
      "✅ Title found using span.biblio-title: \"Yet it Endures: The Persistence of Original Sin\"\n",
      "✅ Authors found using span.biblio-authors: \"Eichengreen, B., Hausmann, R. & Panizza, U.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Notwithstanding announcements of progress, \"intern...\"\n",
      "\n",
      "Fetching page 4: https://growthlab.hks.harvard.edu/publications?page=4\n",
      "Found 20 publications on page 4\n",
      "\n",
      "Testing publication 1 from page 4:\n",
      "✅ Title found using span.biblio-title: \"The Economic Geography of the War in Ukraine\"\n",
      "✅ Authors found using span.biblio-authors: \"Neffke, F., Hartog, M. & Li, Y.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"The war in Ukraine has been waging for a month now...\"\n",
      "\n",
      "Testing publication 2 from page 4:\n",
      "✅ Title found using span.biblio-title: \"What Will It Take for Jordan to Grow?\"\n",
      "✅ Authors found using span.biblio-authors: \"O'Brien, T., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"This report aims to answer the critical but diffic...\"\n",
      "\n",
      "Testing publication 3 from page 4:\n",
      "✅ Title found using span.biblio-title: \"A Simple Theory of Economic Development at the Ext\"\n",
      "✅ Authors found using span.biblio-authors: \"Diodato, D., Hausmann, R. & Schetter, U.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"We revisit the well-known fact that richer countri...\"\n",
      "\n",
      "Fetching page 5: https://growthlab.hks.harvard.edu/publications?page=5\n",
      "Found 20 publications on page 5\n",
      "\n",
      "Testing publication 1 from page 5:\n",
      "✅ Title found using span.biblio-title: \"The Quest for Increased Saudization: Labor Market \"\n",
      "✅ Authors found using span.biblio-authors: \"Lopesciolo, M., Muhaj, D. & Pan, C.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Few countries have embraced active labor market po...\"\n",
      "\n",
      "Testing publication 2 from page 5:\n",
      "✅ Title found using span.biblio-title: \"Loreto’s Hidden Wealth: Economic Complexity Analys\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"This report has three main objectives. Firstly, to...\"\n",
      "\n",
      "Testing publication 3 from page 5:\n",
      "✅ Title found using span.biblio-title: \"Estimating the drivers of urban economic complexit\"\n",
      "✅ Authors found using span.biblio-authors: \"Gomez-Lievano, A. & Patterson-Lomba, O.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Estimating the capabilities, or inputs of producti...\"\n",
      "\n",
      "Fetching page 6: https://growthlab.hks.harvard.edu/publications?page=6\n",
      "Found 20 publications on page 6\n",
      "\n",
      "Testing publication 1 from page 6:\n",
      "✅ Title found using span.biblio-title: \"The Economic Case for Global Vaccinations: An Epid\"\n",
      "✅ Authors found using span.biblio-authors: \"Cakmakli, C., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"COVID-19 pandemic had a devastating effect on both...\"\n",
      "\n",
      "Testing publication 2 from page 6:\n",
      "✅ Title found using span.biblio-title: \"Diagnóstico de Crecimiento de Loreto: Principales \"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Sembrado en el flanco oeste de la selva amazónica,...\"\n",
      "\n",
      "Testing publication 3 from page 6:\n",
      "✅ Title found using span.biblio-title: \"Place-specific determinants of income gaps: New su\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R., Pietrobelli, C. & Santos, M.A.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"The literature on wage gaps between Chiapas and th...\"\n",
      "\n",
      "Fetching page 7: https://growthlab.hks.harvard.edu/publications?page=7\n",
      "Found 20 publications on page 7\n",
      "\n",
      "Testing publication 1 from page 7:\n",
      "✅ Title found using span.biblio-title: \"You Get What You Pay for: Sources and Consequences\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R., Nedelkoska, L. & Noor, S.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"We study the factors behind the public sector prem...\"\n",
      "\n",
      "Testing publication 2 from page 7:\n",
      "✅ Title found using span.biblio-title: \"Migration and Post-conflict Reconstruction: The Ef\"\n",
      "✅ Authors found using span.biblio-authors: \"Bahar, D., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"During the early 1990s Germany offered temporary p...\"\n",
      "\n",
      "Testing publication 3 from page 7:\n",
      "✅ Title found using span.biblio-title: \"Does Birthplace Diversity Affect Economic Complexi\"\n",
      "✅ Authors found using span.biblio-authors: \"Bahar, D., Rapoport, H. & Turati, R.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"We empirically investigate the relationship betwee...\"\n",
      "\n",
      "Fetching page 8: https://growthlab.hks.harvard.edu/publications?page=8\n",
      "Found 20 publications on page 8\n",
      "\n",
      "Testing publication 1 from page 8:\n",
      "✅ Title found using span.biblio-title: \"Female Labor in Jordan: A Systematic Approach to t\"\n",
      "✅ Authors found using span.biblio-authors: \"Kasoolu, S., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Women in Jordan are excluded from labor market opp...\"\n",
      "\n",
      "Testing publication 2 from page 8:\n",
      "✅ Title found using span.biblio-title: \"Minimizing Smuggling and Restoring Public Trust in\"\n",
      "✅ Authors found using span.biblio-authors: \"Abante, K.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"This policy analysis attempts to answer three ques...\"\n",
      "\n",
      "Testing publication 3 from page 8:\n",
      "✅ Title found using span.biblio-title: \"Production Ability and Economic Growth\"\n",
      "✅ Authors found using span.biblio-authors: \"Bustos, S. & Yıldırım, M.A.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Production is shaped by capability requirements of...\"\n",
      "\n",
      "Fetching page 9: https://growthlab.hks.harvard.edu/publications?page=9\n",
      "Found 20 publications on page 9\n",
      "\n",
      "Testing publication 1 from page 9:\n",
      "✅ Title found using span.biblio-title: \"Sri Lanka's North Central Province: A Growth Diagn\"\n",
      "✅ Authors found using span.biblio-authors: \"O'Brien, T.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"In 2018, the Growth Lab team turned attention to d...\"\n",
      "\n",
      "Testing publication 2 from page 9:\n",
      "✅ Title found using span.biblio-title: \"Tabasco: Insumos para el desarrollo de recomendaci\"\n",
      "✅ Authors found using span.biblio-authors: \"Barrios, D., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"En este documento se presentaron una serie de insu...\"\n",
      "\n",
      "Testing publication 3 from page 9:\n",
      "✅ Title found using span.biblio-title: \"Targeting Sectors For Investment and Export Promot\"\n",
      "✅ Authors found using span.biblio-authors: \"Malalgoda, C., Samaraweera, P. & Stock, D.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"In August 2016, the Government of Sri Lanka (GoSL)...\"\n",
      "\n",
      "Fetching page 10: https://growthlab.hks.harvard.edu/publications?page=10\n",
      "Found 20 publications on page 10\n",
      "\n",
      "Testing publication 1 from page 10:\n",
      "✅ Title found using span.biblio-title: \"The Exposure of U.S. Manufacturing Industries to E\"\n",
      "✅ Authors found using span.biblio-authors: \"Thornbecke, W.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Safe asset demand and currency manipulation increa...\"\n",
      "\n",
      "Testing publication 2 from page 10:\n",
      "✅ Title found using span.biblio-title: \"On the Scaling Patterns of Infectious Disease Inci\"\n",
      "✅ Authors found using span.biblio-authors: \"Patterson-Lomba, O. & Gomez-Lievano, A.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Urban areas with larger and more connected populat...\"\n",
      "\n",
      "Testing publication 3 from page 10:\n",
      "✅ Title found using span.biblio-title: \"Place-specific Determinants of Income Gaps: New Su\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R., Pietrobelli, C. & Santos, M.A.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"The literature on income gaps between Chiapas and ...\"\n",
      "\n",
      "Fetching page 11: https://growthlab.hks.harvard.edu/publications?page=11\n",
      "Found 20 publications on page 11\n",
      "\n",
      "Testing publication 1 from page 11:\n",
      "✅ Title found using span.biblio-title: \"Increasing Your Chances of Success while Leaving Y\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"View Ricardo Hausmann's presentation to the Minist...\"\n",
      "\n",
      "Testing publication 2 from page 11:\n",
      "✅ Title found using span.biblio-title: \"Macroeconomic Adjustment in the Euro Area\"\n",
      "✅ Authors found using span.biblio-authors: \"Terzi, A.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Macroeconomic adjustment in the euro area peripher...\"\n",
      "\n",
      "Testing publication 3 from page 11:\n",
      "✅ Title found using span.biblio-title: \"Mapping the International Health Aid Community usi\"\n",
      "✅ Authors found using span.biblio-authors: \"Coscia, M., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"International aid is a complex system: it involves...\"\n",
      "\n",
      "Fetching page 12: https://growthlab.hks.harvard.edu/publications?page=12\n",
      "Found 20 publications on page 12\n",
      "\n",
      "Testing publication 1 from page 12:\n",
      "✅ Title found using span.biblio-title: \"What is different about urbanization in rich and p\"\n",
      "✅ Authors found using span.biblio-authors: \"Chauvin, J.P., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Are the well-known facts about urbanization in the...\"\n",
      "\n",
      "Testing publication 2 from page 12:\n",
      "✅ Title found using span.biblio-title: \"How to Cope with Volatile Commodity Export Prices:\"\n",
      "✅ Authors found using span.biblio-authors: \"Frankel, J.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Countries that specialize in commodities have in r...\"\n",
      "\n",
      "Testing publication 3 from page 12:\n",
      "✅ Title found using span.biblio-title: \"International Emigrant Selection on Occupational S\"\n",
      "✅ Authors found using span.biblio-authors: \"Patt, A., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"We present the first evidence that international e...\"\n",
      "\n",
      "Fetching page 13: https://growthlab.hks.harvard.edu/publications?page=13\n",
      "Found 20 publications on page 13\n",
      "\n",
      "Testing publication 1 from page 13:\n",
      "✅ Title found using span.biblio-title: \"Constraints to Sustained and Inclusive Growth in S\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"In late 2015, CID was requested to conduct an init...\"\n",
      "\n",
      "Testing publication 2 from page 13:\n",
      "✅ Title found using span.biblio-title: \"Do Political Connections Reduce Job Creation? Evid\"\n",
      "✅ Authors found using span.biblio-authors: \"Diwan, I. & Haidar, J.I.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Using firm-level census data, we determine how pol...\"\n",
      "\n",
      "Testing publication 3 from page 13:\n",
      "✅ Title found using span.biblio-title: \"Sri Lanka’s Edible Oils Exports\"\n",
      "❌ Authors not found\n",
      "✅ Abstract found using div.biblio-abstract-display: \"By request of the Government of Sri Lanka, the Gro...\"\n",
      "\n",
      "Fetching page 14: https://growthlab.hks.harvard.edu/publications?page=14\n",
      "Found 20 publications on page 14\n",
      "\n",
      "Testing publication 1 from page 14:\n",
      "✅ Title found using span.biblio-title: \"What Bangs for Your Bucks? Assessing the Design an\"\n",
      "✅ Authors found using span.biblio-authors: \"Janssen, M.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"After an era of generic support for economic devel...\"\n",
      "\n",
      "Testing publication 2 from page 14:\n",
      "✅ Title found using span.biblio-title: \"Bringing Home the Gold? A Review of the Economic I\"\n",
      "✅ Authors found using span.biblio-authors: \"Barrios, D., Russell, S. & Andrews, M.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"There is perhaps no larger sports policy decision ...\"\n",
      "\n",
      "Testing publication 3 from page 14:\n",
      "✅ Title found using span.biblio-title: \"One More Resource Curse: Dutch Disease and Export \"\n",
      "✅ Authors found using span.biblio-authors: \"Bahar, D. & Santos, M.A.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Economists have long discussed the negative effect...\"\n",
      "\n",
      "Fetching page 15: https://growthlab.hks.harvard.edu/publications?page=15\n",
      "Found 20 publications on page 15\n",
      "\n",
      "Testing publication 1 from page 15:\n",
      "✅ Title found using span.biblio-title: \"What are the Challenges of Economic Growth?\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R.\"\n",
      "❌ Abstract not found\n",
      "\n",
      "Testing publication 2 from page 15:\n",
      "✅ Title found using span.biblio-title: \"The Right Fit for the Wrong Reasons: Real Business\"\n",
      "✅ Authors found using span.biblio-authors: \"Santos, M.A.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Venezuela is an oil-dependent economy subject to l...\"\n",
      "\n",
      "Testing publication 3 from page 15:\n",
      "✅ Title found using span.biblio-title: \"Moving to the Adjacent Possible: Discovering Paths\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R. & Chauvin, J.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"How can Rwanda, which currently has one of the low...\"\n",
      "\n",
      "Fetching page 16: https://growthlab.hks.harvard.edu/publications?page=16\n",
      "Found 20 publications on page 16\n",
      "\n",
      "Testing publication 1 from page 16:\n",
      "✅ Title found using span.biblio-title: \"Implied Comparative Advantage\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"The comparative advantage of a location shapes its...\"\n",
      "\n",
      "Testing publication 2 from page 16:\n",
      "✅ Title found using span.biblio-title: \"Expropriation risk and housing prices: Evidence fr\"\n",
      "✅ Authors found using span.biblio-authors: \"Contreras, V., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"This paper examines the microeconomic determinants...\"\n",
      "\n",
      "Testing publication 3 from page 16:\n",
      "✅ Title found using span.biblio-title: \"Integration in the Balkans: Albania and Kosovo\"\n",
      "✅ Authors found using span.biblio-authors: \"Lawrence, R., Frasheri, E. & Qazi, M.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Despite their historic and ethnic ties, trade and ...\"\n",
      "\n",
      "Fetching page 17: https://growthlab.hks.harvard.edu/publications?page=17\n",
      "Found 20 publications on page 17\n",
      "\n",
      "Testing publication 1 from page 17:\n",
      "✅ Title found using span.biblio-title: \"The State, Socialization, and Private Schooling: W\"\n",
      "✅ Authors found using span.biblio-authors: \"Pritchett, L. & Viarengo, M.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Understanding the institutional features that can ...\"\n",
      "\n",
      "Testing publication 2 from page 17:\n",
      "✅ Title found using span.biblio-title: \"Structural Transformation in Egypt, Morocco, and T\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R. & Bustos, S.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Countries seldom grow rich by producing more of th...\"\n",
      "\n",
      "Testing publication 3 from page 17:\n",
      "✅ Title found using span.biblio-title: \"The Natural Resource Curse: A Survey of Diagnoses \"\n",
      "✅ Authors found using span.biblio-authors: \"Frankel, J.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Countries with oil, mineral or other natural resou...\"\n",
      "\n",
      "Fetching page 18: https://growthlab.hks.harvard.edu/publications?page=18\n",
      "Found 20 publications on page 18\n",
      "\n",
      "Testing publication 1 from page 18:\n",
      "✅ Title found using span.biblio-title: \"The network structure of economic output\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R. & Hidalgo, C.A.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Much of the analysis of economic growth has focuse...\"\n",
      "\n",
      "Testing publication 2 from page 18:\n",
      "✅ Title found using span.biblio-title: \"The Atlas of Economic Complexity: Mapping Paths to\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R., et al.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Over the past two centuries, mankind has accomplis...\"\n",
      "\n",
      "Testing publication 3 from page 18:\n",
      "✅ Title found using span.biblio-title: \"Development Policy and Development Economics: An I\"\n",
      "✅ Authors found using span.biblio-authors: \"Rodrik, D. & Rosenzweig, M.R.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Anyone who undertakes to produce a volume of surve...\"\n",
      "\n",
      "Fetching page 19: https://growthlab.hks.harvard.edu/publications?page=19\n",
      "Found 20 publications on page 19\n",
      "\n",
      "Testing publication 1 from page 19:\n",
      "✅ Title found using span.biblio-title: \"South Africa's Export Predicament\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R. & Klinger, B.\"\n",
      "❌ Abstract not found\n",
      "\n",
      "Testing publication 2 from page 19:\n",
      "✅ Title found using span.biblio-title: \"Growth Diagnostics\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R., Rodrik, D. & Velasco, A.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Most well-trained economists would agree that the ...\"\n",
      "\n",
      "Testing publication 3 from page 19:\n",
      "✅ Title found using span.biblio-title: \"What You Export Matters\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R., Hwang, J. & Rodrik, D.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"When local cost discovery generates knowledge spil...\"\n",
      "\n",
      "Fetching page 20: https://growthlab.hks.harvard.edu/publications?page=20\n",
      "Found 20 publications on page 20\n",
      "\n",
      "Testing publication 1 from page 20:\n",
      "✅ Title found using span.biblio-title: \"The Implications of Dark Matter for Assessing the \"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R. & Sturzenegger, F.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"This paper clarifies how dark matter changes our a...\"\n",
      "\n",
      "Testing publication 2 from page 20:\n",
      "✅ Title found using span.biblio-title: \"The Binding Constraints to Growth in Morocco\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"This book identifies the binding constraints to gr...\"\n",
      "\n",
      "Testing publication 3 from page 20:\n",
      "✅ Title found using span.biblio-title: \"Growth Diagnostic: Belize\"\n",
      "✅ Authors found using span.biblio-authors: \"Hausmann, R. & Klinger, B.\"\n",
      "✅ Abstract found using div.biblio-abstract-display: \"Belize’s economic history shows marked periods of ...\"\n",
      "\n",
      "📊 Selector Test Results:\n",
      "✅ GOOD TITLE: 100.0% success (63/63)\n",
      "  Selectors used:\n",
      "  - span.biblio-title: 100.0% (63/63)\n",
      "⚠️ WARNING AUTHORS: 96.8% success (61/63)\n",
      "  Selectors used:\n",
      "  - span.biblio-authors: 100.0% (61/61)\n",
      "⚠️ WARNING ABSTRACT: 96.8% success (61/63)\n",
      "  Selectors used:\n",
      "  - div.biblio-abstract-display: 100.0% (61/61)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Find proper XPath alternatives for all critical elements\n",
    "\n",
    "async def find_xpath_alternatives():\n",
    "    \"\"\"Find proper XPath alternatives for title, authors, and abstract\"\"\"\n",
    "    print(\"\\n🔍 Finding XPath alternatives for critical elements\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(scraper.base_url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page: {response.status}\")\n",
    "                return\n",
    "                \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Find the first publication container\n",
    "            container = soup.find(\"div\", class_=\"biblio-entry\")\n",
    "            if not container:\n",
    "                print(\"No publication container found\")\n",
    "                return\n",
    "            \n",
    "            print(\"Found publication container, examining critical elements:\")\n",
    "            \n",
    "            # Examine title element\n",
    "            title_element = container.find(\"span\", class_=\"biblio-title\")\n",
    "            if title_element:\n",
    "                print(\"\\n1. TITLE ELEMENT:\")\n",
    "                print(f\"Tag: <{title_element.name}>\")\n",
    "                print(f\"Classes: {title_element.get('class')}\")\n",
    "                try:\n",
    "                    # Trying to get accurate XPath\n",
    "                    from selenium.webdriver.common.by import By\n",
    "                    # This is just for demonstration - we're not actually using Selenium here\n",
    "                    xpath_options = [\n",
    "                        \"//span[@class='biblio-title']\",\n",
    "                        \"//span[contains(@class, 'biblio-title')]\",\n",
    "                        \"//span[contains(@class, 'title')]\"\n",
    "                    ]\n",
    "                    print(\"Potential XPath expressions:\")\n",
    "                    for xpath in xpath_options:\n",
    "                        print(f\"  {xpath}\")\n",
    "                except ImportError:\n",
    "                    print(\"Selenium not available for XPath suggestions\")\n",
    "                \n",
    "                # Check for other similar elements\n",
    "                all_titles = soup.find_all(\"span\", class_=\"biblio-title\")\n",
    "                print(f\"Found {len(all_titles)} title elements on page\")\n",
    "                print(\"First few title texts:\")\n",
    "                for title in all_titles[:3]:\n",
    "                    print(f\"  \\\"{title.text.strip()}\\\"\")\n",
    "            else:\n",
    "                print(\"No title element found\")\n",
    "            \n",
    "            # Examine authors element\n",
    "            authors_element = container.find(\"span\", class_=\"biblio-authors\")\n",
    "            if authors_element:\n",
    "                print(\"\\n2. AUTHORS ELEMENT:\")\n",
    "                print(f\"Tag: <{authors_element.name}>\")\n",
    "                print(f\"Classes: {authors_element.get('class')}\")\n",
    "                xpath_options = [\n",
    "                    \"//span[@class='biblio-authors']\",\n",
    "                    \"//span[contains(@class, 'biblio-authors')]\",\n",
    "                    \"//span[contains(@class, 'authors')]\"\n",
    "                ]\n",
    "                print(\"Potential XPath expressions:\")\n",
    "                for xpath in xpath_options:\n",
    "                    print(f\"  {xpath}\")\n",
    "                \n",
    "                # Check for other similar elements\n",
    "                all_authors = soup.find_all(\"span\", class_=\"biblio-authors\")\n",
    "                print(f\"Found {len(all_authors)} author elements on page\")\n",
    "                print(\"First few author texts:\")\n",
    "                for author in all_authors[:3]:\n",
    "                    print(f\"  \\\"{author.text.strip()}\\\"\")\n",
    "            else:\n",
    "                print(\"No authors element found\")\n",
    "            \n",
    "            # Examine abstract element\n",
    "            abstract_element = container.find(\"div\", class_=\"biblio-abstract-display\")\n",
    "            if abstract_element:\n",
    "                print(\"\\n3. ABSTRACT ELEMENT:\")\n",
    "                print(f\"Tag: <{abstract_element.name}>\")\n",
    "                print(f\"Classes: {abstract_element.get('class')}\")\n",
    "                xpath_options = [\n",
    "                    \"//div[@class='biblio-abstract-display']\",\n",
    "                    \"//div[contains(@class, 'biblio-abstract-display')]\",\n",
    "                    \"//div[contains(@class, 'abstract')]\"\n",
    "                ]\n",
    "                print(\"Potential XPath expressions:\")\n",
    "                for xpath in xpath_options:\n",
    "                    print(f\"  {xpath}\")\n",
    "                \n",
    "                # Check for other similar elements\n",
    "                all_abstracts = soup.find_all(\"div\", class_=\"biblio-abstract-display\")\n",
    "                print(f\"Found {len(all_abstracts)} abstract elements on page\")\n",
    "                print(\"First abstract text (truncated):\")\n",
    "                if all_abstracts:\n",
    "                    abstract_text = all_abstracts[0].text.strip()\n",
    "                    print(f\"  \\\"{abstract_text[:100]}...\\\"\")\n",
    "            else:\n",
    "                print(\"No abstract element found\")\n",
    "\n",
    "# Run the function to find XPath alternatives\n",
    "await find_xpath_alternatives()\n",
    "\n",
    "# %%\n",
    "# Create a simple selector configuration dictionary\n",
    "\n",
    "# This is a simplified version focused on the original task\n",
    "SELECTORS = {\n",
    "    \"title\": {\n",
    "        \"primary\": \"span.biblio-title\",\n",
    "        \"fallbacks\": [\"h2.title\", \"div.title\", \"span.field-biblio-title\"],\n",
    "        \"xpath\": \"//span[contains(@class, 'biblio-title')]\"\n",
    "    },\n",
    "    \"authors\": {\n",
    "        \"primary\": \"span.biblio-authors\", \n",
    "        \"fallbacks\": [\"div.authors\", \"span.field-biblio-authors\"],\n",
    "        \"xpath\": \"//span[contains(@class, 'biblio-authors')]\"\n",
    "    },\n",
    "    \"abstract\": {\n",
    "        \"primary\": \"div.biblio-abstract-display\",\n",
    "        \"fallbacks\": [\"div.abstract\", \"div.field-biblio-abstract\"],\n",
    "        \"xpath\": \"//div[contains(@class, 'abstract')]\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Simple function to try a selector with fallbacks\n",
    "def try_selector_with_fallbacks(element, selector_config):\n",
    "    \"\"\"Try a selector with fallbacks and return the first match\"\"\"\n",
    "    # Try primary\n",
    "    if selector_config[\"primary\"]:\n",
    "        if \".\" in selector_config[\"primary\"]:\n",
    "            tag, class_name = selector_config[\"primary\"].split(\".\", 1)\n",
    "            tag = tag if tag else None\n",
    "            result = element.find(tag, class_=class_name)\n",
    "            if result:\n",
    "                return result, selector_config[\"primary\"]\n",
    "    \n",
    "    # Try fallbacks\n",
    "    for fallback in selector_config[\"fallbacks\"]:\n",
    "        if \".\" in fallback:\n",
    "            tag, class_name = fallback.split(\".\", 1)\n",
    "            tag = tag if tag else None\n",
    "            result = element.find(tag, class_=class_name)\n",
    "            if result:\n",
    "                return result, fallback\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# %%\n",
    "# Create a comprehensive testing function that can check all pages\n",
    "\n",
    "async def test_selectors_on_all_pages(max_pages=21, publications_per_page=3):\n",
    "    \"\"\"Test selectors on publications from all pages\"\"\"\n",
    "    print(f\"\\n🧪 Testing selectors on publications from all {max_pages} pages\")\n",
    "    print(f\"(Sampling {publications_per_page} publications per page)\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    # Track success/failure of selectors\n",
    "    results = {\n",
    "        \"title\": {\"success\": 0, \"failure\": 0, \"fallback_used\": {}},\n",
    "        \"authors\": {\"success\": 0, \"failure\": 0, \"fallback_used\": {}},\n",
    "        \"abstract\": {\"success\": 0, \"failure\": 0, \"fallback_used\": {}}\n",
    "    }\n",
    "    \n",
    "    # Process each page\n",
    "    for page in range(max_pages):\n",
    "        # Construct page URL\n",
    "        page_url = scraper.base_url if page == 0 else f\"{scraper.base_url}?page={page}\"\n",
    "        print(f\"\\nFetching page {page}: {page_url}\")\n",
    "        \n",
    "        try:\n",
    "            async with aiohttp.ClientSession(headers=headers) as session:\n",
    "                async with session.get(page_url) as response:\n",
    "                    if response.status != 200:\n",
    "                        print(f\"Error fetching page {page}: {response.status}\")\n",
    "                        continue\n",
    "                    \n",
    "                    html = await response.text()\n",
    "                    soup = BeautifulSoup(html, \"html.parser\")\n",
    "                    \n",
    "                    # Find all publication containers\n",
    "                    containers = soup.find_all(\"div\", class_=\"biblio-entry\")\n",
    "                    \n",
    "                    if not containers:\n",
    "                        print(f\"No publications found on page {page}\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"Found {len(containers)} publications on page {page}\")\n",
    "                    \n",
    "                    # Select a sample of publications to test\n",
    "                    import random\n",
    "                    sample_size = min(publications_per_page, len(containers))\n",
    "                    if sample_size > 0:\n",
    "                        sample_indices = random.sample(range(len(containers)), sample_size)\n",
    "                        sampled_containers = [containers[i] for i in sample_indices]\n",
    "                    else:\n",
    "                        sampled_containers = []\n",
    "                    \n",
    "                    # Test each sampled publication\n",
    "                    for i, container in enumerate(sampled_containers):\n",
    "                        print(f\"\\nTesting publication {i+1} from page {page}:\")\n",
    "                        \n",
    "                        # Try to find title\n",
    "                        title_element, title_selector = try_selector_with_fallbacks(container, SELECTORS[\"title\"])\n",
    "                        if title_element:\n",
    "                            results[\"title\"][\"success\"] += 1\n",
    "                            results[\"title\"][\"fallback_used\"][title_selector] = results[\"title\"][\"fallback_used\"].get(title_selector, 0) + 1\n",
    "                            print(f\"✅ Title found using {title_selector}: \\\"{title_element.text.strip()[:50]}\\\"\")\n",
    "                        else:\n",
    "                            results[\"title\"][\"failure\"] += 1\n",
    "                            print(\"❌ Title not found\")\n",
    "                        \n",
    "                        # Try to find authors\n",
    "                        authors_element, authors_selector = try_selector_with_fallbacks(container, SELECTORS[\"authors\"])\n",
    "                        if authors_element:\n",
    "                            results[\"authors\"][\"success\"] += 1\n",
    "                            results[\"authors\"][\"fallback_used\"][authors_selector] = results[\"authors\"][\"fallback_used\"].get(authors_selector, 0) + 1\n",
    "                            print(f\"✅ Authors found using {authors_selector}: \\\"{authors_element.text.strip()}\\\"\")\n",
    "                        else:\n",
    "                            results[\"authors\"][\"failure\"] += 1\n",
    "                            print(\"❌ Authors not found\")\n",
    "                        \n",
    "                        # Try to find abstract\n",
    "                        abstract_element, abstract_selector = try_selector_with_fallbacks(container, SELECTORS[\"abstract\"])\n",
    "                        if abstract_element:\n",
    "                            results[\"abstract\"][\"success\"] += 1\n",
    "                            results[\"abstract\"][\"fallback_used\"][abstract_selector] = results[\"abstract\"][\"fallback_used\"].get(abstract_selector, 0) + 1\n",
    "                            abstract_text = abstract_element.text.strip()\n",
    "                            print(f\"✅ Abstract found using {abstract_selector}: \\\"{abstract_text[:50]}...\\\"\")\n",
    "                        else:\n",
    "                            results[\"abstract\"][\"failure\"] += 1\n",
    "                            print(\"❌ Abstract not found\")\n",
    "            \n",
    "            # Add a short delay between pages\n",
    "            await asyncio.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {page}: {e}\")\n",
    "    \n",
    "    # Print summary results\n",
    "    print(\"\\n📊 Selector Test Results:\")\n",
    "    for element_type, stats in results.items():\n",
    "        total = stats[\"success\"] + stats[\"failure\"]\n",
    "        if total > 0:\n",
    "            success_rate = stats[\"success\"] / total * 100\n",
    "            status = \"✅ GOOD\" if success_rate == 100 else \"⚠️ WARNING\" if success_rate >= 80 else \"❌ FAILING\"\n",
    "            print(f\"{status} {element_type.upper()}: {success_rate:.1f}% success ({stats['success']}/{total})\")\n",
    "            \n",
    "            # Show which selectors were used\n",
    "            if stats[\"success\"] > 0:\n",
    "                print(\"  Selectors used:\")\n",
    "                for selector, count in sorted(stats[\"fallback_used\"].items(), key=lambda x: x[1], reverse=True):\n",
    "                    percentage = count / stats[\"success\"] * 100\n",
    "                    print(f\"  - {selector}: {percentage:.1f}% ({count}/{stats['success']})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the test on all pages\n",
    "test_results = await test_selectors_on_all_pages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "916fb5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Testing more specific selectors from SelectorGadget\n",
      "\n",
      "Testing authors selector with SelectorGadget recommendation:\n",
      "Standard selector (span.biblio-authors): ✅ Found\n",
      "Text: \"Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\"\n",
      "Complex selector (.biblio-separator-bar+ .clearfix .biblio-authors): ❌ Not found\n",
      "XPath selector: ❌ Error - Invalid expression\n",
      "\n",
      "Testing citation selector on individual publication page:\n",
      "Citation selector (.biblio-citation): ✅ Found\n",
      "Citation text: \"Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A., 2025. Global Networks, Monetary Policy and Trade. Copy at http://www.tinyurl.com/27pjk6pp\"\n",
      "Extracted author: \"Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.,\"\n",
      "Extracted year: 2025\n",
      "Extracted title: \"Global Networks, Monetary Policy and Trade\"\n"
     ]
    }
   ],
   "source": [
    "# Testing the more specific selectors found with SelectorGadget\n",
    "\n",
    "async def test_more_specific_selectors():\n",
    "    \"\"\"Test the more specific selectors for authors and citation\"\"\"\n",
    "    print(\"\\n🔍 Testing more specific selectors from SelectorGadget\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    base_url = \"https://growthlab.hks.harvard.edu\"\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        # Test on the main publications page\n",
    "        async with session.get(f\"{base_url}/publications\") as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Get the first publication container\n",
    "            container = soup.find(\"div\", class_=\"biblio-entry\")\n",
    "            if not container:\n",
    "                print(\"No publication found\")\n",
    "                return\n",
    "            \n",
    "            print(\"\\nTesting authors selector with SelectorGadget recommendation:\")\n",
    "            \n",
    "            # Test the standard selector\n",
    "            standard_authors = container.find(\"span\", class_=\"biblio-authors\")\n",
    "            print(f\"Standard selector (span.biblio-authors): {'✅ Found' if standard_authors else '❌ Not found'}\")\n",
    "            if standard_authors:\n",
    "                print(f\"Text: \\\"{standard_authors.text.strip()}\\\"\")\n",
    "            \n",
    "            # Test the complex selector from SelectorGadget\n",
    "            # .biblio-separator-bar+ .clearfix .biblio-authors\n",
    "            separator = container.find(class_=\"biblio-separator-bar\")\n",
    "            complex_authors = None\n",
    "            \n",
    "            if separator:\n",
    "                # Find next sibling with class clearfix\n",
    "                sibling = separator.find_next_sibling(class_=\"clearfix\")\n",
    "                if sibling:\n",
    "                    # Within this, find element with class biblio-authors\n",
    "                    complex_authors = sibling.find(class_=\"biblio-authors\")\n",
    "            \n",
    "            print(f\"Complex selector (.biblio-separator-bar+ .clearfix .biblio-authors): {'✅ Found' if complex_authors else '❌ Not found'}\")\n",
    "            if complex_authors:\n",
    "                print(f\"Text: \\\"{complex_authors.text.strip()}\\\"\")\n",
    "            \n",
    "            # Test XPath selector\n",
    "            try:\n",
    "                import lxml.html\n",
    "                from lxml import etree\n",
    "                \n",
    "                dom = lxml.html.fromstring(str(container))\n",
    "                xpath = \"//*+[contains(concat( \\\" \\\", @class, \\\" \\\" ), concat( \\\" \\\", \\\"biblio-separator-bar\\\", \\\" \\\" ))]//*[contains(concat( \\\" \\\", @class, \\\" \\\" ), concat( \\\" \\\", \\\"clearfix\\\", \\\" \\\" ))]//*[contains(concat( \\\" \\\", @class, \\\" \\\" ), concat( \\\" \\\", \\\"biblio-authors\\\", \\\" \\\" ))]\"\n",
    "                xpath_results = dom.xpath(xpath)\n",
    "                \n",
    "                print(f\"XPath selector: {'✅ Found' if xpath_results else '❌ Not found'}\")\n",
    "                if xpath_results:\n",
    "                    # Convert to string to get text\n",
    "                    text = xpath_results[0].text_content().strip()\n",
    "                    print(f\"Text: \\\"{text}\\\"\")\n",
    "            except ImportError:\n",
    "                print(\"XPath selector: ❌ lxml not available\")\n",
    "            except Exception as e:\n",
    "                print(f\"XPath selector: ❌ Error - {e}\")\n",
    "        \n",
    "        # Test on individual publication page\n",
    "        async with session.get(f\"{base_url}/publications/global-networks-monetary-policy-and-trade\") as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            print(\"\\nTesting citation selector on individual publication page:\")\n",
    "            \n",
    "            # Test for biblio-citation\n",
    "            citation = soup.find(class_=\"biblio-citation\")\n",
    "            print(f\"Citation selector (.biblio-citation): {'✅ Found' if citation else '❌ Not found'}\")\n",
    "            if citation:\n",
    "                citation_text = citation.text.strip()\n",
    "                print(f\"Citation text: \\\"{citation_text}\\\"\")\n",
    "                \n",
    "                # Demonstrate parsing author from citation\n",
    "                if citation_text and \",\" in citation_text:\n",
    "                    # Split by year (after author, before title)\n",
    "                    year_match = re.search(r\"\\b(19|20)\\d{2}\\b\", citation_text)\n",
    "                    if year_match:\n",
    "                        year_index = year_match.start()\n",
    "                        author_part = citation_text[:year_index].strip()\n",
    "                        # Remove trailing comma if present\n",
    "                        if author_part.endswith(\".\"):\n",
    "                            author_part = author_part[:-1].strip()\n",
    "                        print(f\"Extracted author: \\\"{author_part}\\\"\")\n",
    "                        \n",
    "                        # Extract year\n",
    "                        year = int(year_match.group())\n",
    "                        print(f\"Extracted year: {year}\")\n",
    "                        \n",
    "                        # Extract title (after year and dot)\n",
    "                        title_start = year_match.end() + 1  # Skip the period after year\n",
    "                        rest = citation_text[title_start:].strip()\n",
    "                        if \".\" in rest:\n",
    "                            title_end = rest.find(\".\")\n",
    "                            title = rest[:title_end].strip()\n",
    "                            print(f\"Extracted title: \\\"{title}\\\"\")\n",
    "\n",
    "# Run the test\n",
    "await test_more_specific_selectors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "399c512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Testing more specific selectors from SelectorGadget\n",
      "\n",
      "Testing authors selector with SelectorGadget recommendation:\n",
      "Standard selector (span.biblio-authors): ✅ Found\n",
      "Text: \"Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\"\n",
      "Complex selector (.biblio-separator-bar+ .clearfix .biblio-authors): ❌ Not found\n",
      "XPath selector: ❌ Error - Invalid expression\n",
      "\n",
      "Testing citation selector on individual publication page:\n",
      "Citation selector (.biblio-citation): ✅ Found\n",
      "Citation text: \"Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A., 2025. Global Networks, Monetary Policy and Trade. Copy at http://www.tinyurl.com/27pjk6pp\"\n",
      "Extracted author: \"Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.,\"\n",
      "Extracted year: 2025\n",
      "Extracted title: \"Global Networks, Monetary Policy and Trade\"\n",
      "\n",
      "🧪 Testing improved parser with updated selectors\n",
      "\n",
      "No problematic publications found, testing first publication\n",
      "Parsed with improved parser:\n",
      "Title: Global Networks, Monetary Policy and Trade\n",
      "Authors: Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\n",
      "Year: 2025\n",
      "Abstract: We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n",
      "\n",
      "Testing on individual publication page:\n",
      "Parsed with improved parser:\n",
      "Title: Global Networks, Monetary Policy and Trade\n",
      "Authors: Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\n",
      "Year: 2025\n",
      "No abstract\n",
      "File URLs: ['https://www.nber.org/system/files/working_papers/w33686/w33686.pdf']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Testing the more specific selectors found with SelectorGadget\n",
    "\n",
    "async def test_more_specific_selectors():\n",
    "    \"\"\"Test the more specific selectors for authors and citation\"\"\"\n",
    "    print(\"\\n🔍 Testing more specific selectors from SelectorGadget\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    base_url = \"https://growthlab.hks.harvard.edu\"\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        # Test on the main publications page\n",
    "        async with session.get(f\"{base_url}/publications\") as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Get the first publication container\n",
    "            container = soup.find(\"div\", class_=\"biblio-entry\")\n",
    "            if not container:\n",
    "                print(\"No publication found\")\n",
    "                return\n",
    "            \n",
    "            print(\"\\nTesting authors selector with SelectorGadget recommendation:\")\n",
    "            \n",
    "            # Test the standard selector\n",
    "            standard_authors = container.find(\"span\", class_=\"biblio-authors\")\n",
    "            print(f\"Standard selector (span.biblio-authors): {'✅ Found' if standard_authors else '❌ Not found'}\")\n",
    "            if standard_authors:\n",
    "                print(f\"Text: \\\"{standard_authors.text.strip()}\\\"\")\n",
    "            \n",
    "            # Test the complex selector from SelectorGadget\n",
    "            # .biblio-separator-bar+ .clearfix .biblio-authors\n",
    "            separator = container.find(class_=\"biblio-separator-bar\")\n",
    "            complex_authors = None\n",
    "            \n",
    "            if separator:\n",
    "                # Find next sibling with class clearfix\n",
    "                sibling = separator.find_next_sibling(class_=\"clearfix\")\n",
    "                if sibling:\n",
    "                    # Within this, find element with class biblio-authors\n",
    "                    complex_authors = sibling.find(class_=\"biblio-authors\")\n",
    "            \n",
    "            print(f\"Complex selector (.biblio-separator-bar+ .clearfix .biblio-authors): {'✅ Found' if complex_authors else '❌ Not found'}\")\n",
    "            if complex_authors:\n",
    "                print(f\"Text: \\\"{complex_authors.text.strip()}\\\"\")\n",
    "            \n",
    "            # Test XPath selector\n",
    "            try:\n",
    "                import lxml.html\n",
    "                from lxml import etree\n",
    "                \n",
    "                dom = lxml.html.fromstring(str(container))\n",
    "                xpath = \"//*+[contains(concat( \\\" \\\", @class, \\\" \\\" ), concat( \\\" \\\", \\\"biblio-separator-bar\\\", \\\" \\\" ))]//*[contains(concat( \\\" \\\", @class, \\\" \\\" ), concat( \\\" \\\", \\\"clearfix\\\", \\\" \\\" ))]//*[contains(concat( \\\" \\\", @class, \\\" \\\" ), concat( \\\" \\\", \\\"biblio-authors\\\", \\\" \\\" ))]\"\n",
    "                xpath_results = dom.xpath(xpath)\n",
    "                \n",
    "                print(f\"XPath selector: {'✅ Found' if xpath_results else '❌ Not found'}\")\n",
    "                if xpath_results:\n",
    "                    # Convert to string to get text\n",
    "                    text = xpath_results[0].text_content().strip()\n",
    "                    print(f\"Text: \\\"{text}\\\"\")\n",
    "            except ImportError:\n",
    "                print(\"XPath selector: ❌ lxml not available\")\n",
    "            except Exception as e:\n",
    "                print(f\"XPath selector: ❌ Error - {e}\")\n",
    "        \n",
    "        # Test on individual publication page\n",
    "        async with session.get(f\"{base_url}/publications/global-networks-monetary-policy-and-trade\") as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            print(\"\\nTesting citation selector on individual publication page:\")\n",
    "            \n",
    "            # Test for biblio-citation\n",
    "            citation = soup.find(class_=\"biblio-citation\")\n",
    "            print(f\"Citation selector (.biblio-citation): {'✅ Found' if citation else '❌ Not found'}\")\n",
    "            if citation:\n",
    "                citation_text = citation.text.strip()\n",
    "                print(f\"Citation text: \\\"{citation_text}\\\"\")\n",
    "                \n",
    "                # Demonstrate parsing author from citation\n",
    "                if citation_text and \",\" in citation_text:\n",
    "                    # Split by year (after author, before title)\n",
    "                    year_match = re.search(r\"\\b(19|20)\\d{2}\\b\", citation_text)\n",
    "                    if year_match:\n",
    "                        year_index = year_match.start()\n",
    "                        author_part = citation_text[:year_index].strip()\n",
    "                        # Remove trailing comma if present\n",
    "                        if author_part.endswith(\".\"):\n",
    "                            author_part = author_part[:-1].strip()\n",
    "                        print(f\"Extracted author: \\\"{author_part}\\\"\")\n",
    "                        \n",
    "                        # Extract year\n",
    "                        year = int(year_match.group())\n",
    "                        print(f\"Extracted year: {year}\")\n",
    "                        \n",
    "                        # Extract title (after year and dot)\n",
    "                        title_start = year_match.end() + 1  # Skip the period after year\n",
    "                        rest = citation_text[title_start:].strip()\n",
    "                        if \".\" in rest:\n",
    "                            title_end = rest.find(\".\")\n",
    "                            title = rest[:title_end].strip()\n",
    "                            print(f\"Extracted title: \\\"{title}\\\"\")\n",
    "\n",
    "# Run the test\n",
    "await test_more_specific_selectors()\n",
    "\n",
    "# %%\n",
    "# Update our selector configuration with the new findings\n",
    "\n",
    "# Updated selector configuration with SelectorGadget findings\n",
    "UPDATED_SELECTORS = {\n",
    "    \"publication\": {\n",
    "        \"container\": \"div.biblio-entry\",\n",
    "        \"title\": {\n",
    "            \"primary\": \"span.biblio-title\",\n",
    "            \"fallbacks\": [\"h2.title\", \"div.title\", \"span.field-biblio-title\", \".biblio-citation\"],\n",
    "            \"xpath\": \"//span[contains(@class, 'biblio-title')]\"\n",
    "        },\n",
    "        \"authors\": {\n",
    "            \"primary\": \"span.biblio-authors\", \n",
    "            \"fallbacks\": [\n",
    "                \".biblio-separator-bar+ .clearfix .biblio-authors\",  # New selector from SelectorGadget\n",
    "                \"div.authors\", \n",
    "                \"span.field-biblio-authors\",\n",
    "                \".biblio-citation\"  # We can extract authors from the citation\n",
    "            ],\n",
    "            \"xpath\": \"//*[contains(@class, 'biblio-authors')] | //*[contains(@class, 'biblio-separator-bar')]/following-sibling::*[contains(@class, 'clearfix')]//*[contains(@class, 'biblio-authors')]\"\n",
    "        },\n",
    "        \"abstract\": {\n",
    "            \"primary\": \"div.biblio-abstract-display\",\n",
    "            \"fallbacks\": [\"div.abstract\", \"div.field-biblio-abstract\", \"div.field-name-field-abstract\"],\n",
    "            \"xpath\": \"//div[contains(@class, 'abstract')]\"\n",
    "        },\n",
    "        \"file\": {\n",
    "            \"primary\": \"span.file\",\n",
    "            \"fallbacks\": [\".Z3988+ a\", \"#pub-cover-content-wrapper a\", \"a[href$='.pdf']\"],\n",
    "            \"xpath\": \"//span[contains(@class, 'file')] | //*[contains(@class, 'Z3988')]/following-sibling::a | //*[@id='pub-cover-content-wrapper']//a\"\n",
    "        }\n",
    "    },\n",
    "    \"pagination\": {\n",
    "        \"container\": \"ul.pager\",\n",
    "        \"last_page\": \"li.pager-last\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Improved helper function to handle more complex selectors\n",
    "def improved_find_with_fallbacks(element, config):\n",
    "    \"\"\"Find an element using primary selector with fallbacks, supporting complex selectors\n",
    "    \n",
    "    Args:\n",
    "        element: BeautifulSoup element to search within\n",
    "        config: Selector configuration with primary and fallbacks\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup element or None if not found\n",
    "    \"\"\"\n",
    "    # Try primary selector\n",
    "    result = try_complex_selector(element, config[\"primary\"])\n",
    "    if result:\n",
    "        return result\n",
    "    \n",
    "    # Try fallbacks if provided\n",
    "    if \"fallbacks\" in config:\n",
    "        for fallback in config[\"fallbacks\"]:\n",
    "            # Special case for citation\n",
    "            if fallback == \".biblio-citation\":\n",
    "                citation = element.find(class_=\"biblio-citation\")\n",
    "                if citation:\n",
    "                    return citation\n",
    "                    \n",
    "            result = try_complex_selector(element, fallback)\n",
    "            if result:\n",
    "                return result\n",
    "    \n",
    "    # Try XPath as last resort\n",
    "    if \"xpath\" in config:\n",
    "        try:\n",
    "            import lxml.html\n",
    "            from lxml import etree\n",
    "            \n",
    "            # We need to parse the HTML of the element\n",
    "            dom = lxml.html.fromstring(str(element))\n",
    "            xpath_result = dom.xpath(config[\"xpath\"])\n",
    "            \n",
    "            if xpath_result:\n",
    "                # Convert back to BeautifulSoup for consistency\n",
    "                result_html = etree.tostring(xpath_result[0])\n",
    "                result_soup = BeautifulSoup(result_html, \"html.parser\")\n",
    "                return result_soup.contents[0] if result_soup.contents else None\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def try_complex_selector(element, selector):\n",
    "    \"\"\"Try a potentially complex CSS selector\n",
    "    \n",
    "    Handles:\n",
    "    - Simple class selectors (div.class)\n",
    "    - Sibling selectors (a+ b)\n",
    "    - Descendant selectors (a b)\n",
    "    \"\"\"\n",
    "    if not selector:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Handle adjacent sibling selector (e.g. .biblio-separator-bar+ .clearfix .biblio-authors)\n",
    "        if \"+ \" in selector:\n",
    "            parts = selector.split(\"+ \", 1)\n",
    "            first_part = parts[0].strip()\n",
    "            second_part = parts[1].strip()\n",
    "            \n",
    "            # Find the first element\n",
    "            first_element = None\n",
    "            if \".\" in first_part:\n",
    "                tag, class_name = first_part.split(\".\", 1)\n",
    "                tag = tag if tag else None\n",
    "                first_element = element.find(tag, class_=class_name)\n",
    "            else:\n",
    "                first_element = element.find(first_part)\n",
    "                \n",
    "            if not first_element:\n",
    "                return None\n",
    "                \n",
    "            # Find the next sibling element\n",
    "            sibling = first_element.find_next_sibling()\n",
    "            \n",
    "            # If second part has more complex rules (like a descendant selector)\n",
    "            if \" \" in second_part:\n",
    "                sibling_parts = second_part.split(\" \", 1)\n",
    "                sibling_class = sibling_parts[0]\n",
    "                descendant_selector = sibling_parts[1]\n",
    "                \n",
    "                # Check if sibling matches the first part of second_part\n",
    "                if \".\" in sibling_class:\n",
    "                    tag, class_name = sibling_class.split(\".\", 1)\n",
    "                    tag = tag if tag else None\n",
    "                    if (not tag or sibling.name == tag) and class_name in sibling.get(\"class\", []):\n",
    "                        # Now find the descendant\n",
    "                        return try_complex_selector(sibling, descendant_selector)\n",
    "                elif sibling.name == sibling_class:\n",
    "                    # Now find the descendant\n",
    "                    return try_complex_selector(sibling, descendant_selector)\n",
    "                \n",
    "                return None\n",
    "            else:\n",
    "                # Simple sibling check\n",
    "                if \".\" in second_part:\n",
    "                    tag, class_name = second_part.split(\".\", 1)\n",
    "                    tag = tag if tag else None\n",
    "                    if (not tag or sibling.name == tag) and class_name in sibling.get(\"class\", []):\n",
    "                        return sibling\n",
    "                elif sibling.name == second_part:\n",
    "                    return sibling\n",
    "                \n",
    "                return None\n",
    "        \n",
    "        # Handle descendant selector (e.g. .clearfix .biblio-authors)\n",
    "        elif \" \" in selector and not selector.startswith(\"#\"):\n",
    "            parts = selector.split(\" \", 1)\n",
    "            parent_selector = parts[0].strip()\n",
    "            child_selector = parts[1].strip()\n",
    "            \n",
    "            # Find the parent element\n",
    "            parent = None\n",
    "            if \".\" in parent_selector:\n",
    "                tag, class_name = parent_selector.split(\".\", 1)\n",
    "                tag = tag if tag else None\n",
    "                parent = element.find(tag, class_=class_name)\n",
    "            else:\n",
    "                parent = element.find(parent_selector)\n",
    "                \n",
    "            if not parent:\n",
    "                return None\n",
    "                \n",
    "            # Find the child within the parent\n",
    "            return try_complex_selector(parent, child_selector)\n",
    "            \n",
    "        # Handle ID selector\n",
    "        elif \"#\" in selector:\n",
    "            parts = selector.split(\"#\", 1)\n",
    "            element_id = parts[1]\n",
    "            \n",
    "            if \".\" in parts[0]:\n",
    "                tag, class_name = parts[0].split(\".\", 1)\n",
    "                tag = tag if tag else None\n",
    "                return element.find(tag, id=element_id, class_=class_name)\n",
    "            else:\n",
    "                tag = parts[0] if parts[0] else None\n",
    "                return element.find(tag, id=element_id)\n",
    "                \n",
    "        # Handle simple class selector\n",
    "        elif \".\" in selector:\n",
    "            tag, class_name = selector.split(\".\", 1)\n",
    "            tag = tag if tag else None\n",
    "            return element.find(tag, class_=class_name)\n",
    "            \n",
    "        # Handle simple tag selector\n",
    "        else:\n",
    "            return element.find(selector)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with selector '{selector}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Improved publication parsing to handle complex selectors and citation extraction\n",
    "def improved_parse_publication(pub_element, base_url):\n",
    "    \"\"\"Parse a publication using improved selector handling\"\"\"\n",
    "    # Find title element\n",
    "    title_element = improved_find_with_fallbacks(pub_element, UPDATED_SELECTORS[\"publication\"][\"title\"])\n",
    "    \n",
    "    title = None\n",
    "    if title_element:\n",
    "        if \"biblio-citation\" in title_element.get(\"class\", []):\n",
    "            # Extract title from citation\n",
    "            citation_text = title_element.text.strip()\n",
    "            year_match = re.search(r\"\\b(19|20)\\d{2}\\b\", citation_text)\n",
    "            if year_match:\n",
    "                title_start = year_match.end() + 1  # Skip the period after year\n",
    "                rest = citation_text[title_start:].strip()\n",
    "                if \".\" in rest:\n",
    "                    title_end = rest.find(\".\")\n",
    "                    title = rest[:title_end].strip()\n",
    "        else:\n",
    "            title = title_element.text.strip()\n",
    "    \n",
    "    # Get publication URL\n",
    "    pub_url = None\n",
    "    if title_element:\n",
    "        title_link = title_element.find(\"a\")\n",
    "        if title_link and title_link.get(\"href\"):\n",
    "            pub_url = title_link[\"href\"]\n",
    "            if not pub_url.startswith((\"http://\", \"https://\")):\n",
    "                pub_url = f\"{base_url}{pub_url}\"\n",
    "    \n",
    "    # Find authors element\n",
    "    authors_element = improved_find_with_fallbacks(pub_element, UPDATED_SELECTORS[\"publication\"][\"authors\"])\n",
    "    \n",
    "    authors = None\n",
    "    if authors_element:\n",
    "        if \"biblio-citation\" in authors_element.get(\"class\", []):\n",
    "            # Extract authors from citation\n",
    "            citation_text = authors_element.text.strip()\n",
    "            year_match = re.search(r\"\\b(19|20)\\d{2}\\b\", citation_text)\n",
    "            if year_match:\n",
    "                year_index = year_match.start()\n",
    "                authors = citation_text[:year_index].strip()\n",
    "                # Remove trailing comma or period if present\n",
    "                if authors.endswith(\".\") or authors.endswith(\",\"):\n",
    "                    authors = authors[:-1].strip()\n",
    "        else:\n",
    "            authors = authors_element.text.strip()\n",
    "    \n",
    "    # Extract year from text after authors or from citation\n",
    "    year = None\n",
    "    if authors_element:\n",
    "        # Check if we have a citation\n",
    "        if \"biblio-citation\" in authors_element.get(\"class\", []):\n",
    "            citation_text = authors_element.text.strip()\n",
    "            year_match = re.search(r\"\\b(19|20)\\d{2}\\b\", citation_text)\n",
    "            if year_match:\n",
    "                year = int(year_match.group())\n",
    "        else:\n",
    "            sibling_text = authors_element.next_sibling\n",
    "            if sibling_text:\n",
    "                year_match = re.search(r\"\\b(19|20)\\d{2}\\b\", sibling_text)\n",
    "                if year_match:\n",
    "                    year = int(year_match.group())\n",
    "    \n",
    "    # Find abstract element\n",
    "    abstract_element = improved_find_with_fallbacks(pub_element, UPDATED_SELECTORS[\"publication\"][\"abstract\"])\n",
    "    abstract = abstract_element.text.strip() if abstract_element else None\n",
    "    \n",
    "    # Find file elements (with specialized handling due to multiple potential formats)\n",
    "    file_urls = []\n",
    "    file_element = improved_find_with_fallbacks(pub_element, UPDATED_SELECTORS[\"publication\"][\"file\"])\n",
    "    \n",
    "    if file_element:\n",
    "        if file_element.name == \"a\" and file_element.get(\"href\"):\n",
    "            # Direct link\n",
    "            file_url = file_element[\"href\"]\n",
    "            if not file_url.startswith((\"http://\", \"https://\")):\n",
    "                file_url = f\"{base_url}{file_url}\"\n",
    "            file_urls.append(file_url)\n",
    "        else:\n",
    "            # Container with links inside\n",
    "            for link in file_element.find_all(\"a\"):\n",
    "                if link.get(\"href\"):\n",
    "                    file_url = link[\"href\"]\n",
    "                    if not file_url.startswith((\"http://\", \"https://\")):\n",
    "                        file_url = f\"{base_url}{file_url}\"\n",
    "                    file_urls.append(file_url)\n",
    "    \n",
    "    # Create publication object\n",
    "    publication = {\n",
    "        \"title\": title,\n",
    "        \"authors\": authors,\n",
    "        \"year\": year,\n",
    "        \"abstract\": abstract,\n",
    "        \"pub_url\": pub_url,\n",
    "        \"file_urls\": file_urls,\n",
    "        \"source\": \"GrowthLab\"\n",
    "    }\n",
    "    \n",
    "    return publication\n",
    "\n",
    "# Test our improved parser with new selectors\n",
    "async def test_improved_parser():\n",
    "    \"\"\"Test our improved parser with the updated selectors\"\"\"\n",
    "    print(\"\\n🧪 Testing improved parser with updated selectors\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    base_url = \"https://growthlab.hks.harvard.edu\"\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        # Test on the main publications page\n",
    "        async with session.get(f\"{base_url}/publications\") as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Find publications with missing authors from previous tests\n",
    "            found_problematic = False\n",
    "            containers = soup.find_all(\"div\", class_=\"biblio-entry\")\n",
    "            \n",
    "            for i, container in enumerate(containers[:10]):  # Check first 10\n",
    "                # Quick check if this has the standard authors element\n",
    "                std_authors = container.find(\"span\", class_=\"biblio-authors\")\n",
    "                \n",
    "                if not std_authors:\n",
    "                    found_problematic = True\n",
    "                    print(f\"\\nFound publication {i+1} with missing standard authors element\")\n",
    "                    \n",
    "                    # Try parsing with improved parser\n",
    "                    publication = improved_parse_publication(container, base_url)\n",
    "                    \n",
    "                    print(\"Parsed with improved parser:\")\n",
    "                    print(f\"Title: {publication['title']}\")\n",
    "                    print(f\"Authors: {publication['authors']}\")\n",
    "                    print(f\"Year: {publication['year']}\")\n",
    "                    print(f\"Abstract: {publication['abstract'][:100]}...\" if publication['abstract'] else \"No abstract\")\n",
    "            \n",
    "            if not found_problematic:\n",
    "                # Just test the first publication\n",
    "                print(\"\\nNo problematic publications found, testing first publication\")\n",
    "                container = containers[0]\n",
    "                \n",
    "                # Try parsing with improved parser\n",
    "                publication = improved_parse_publication(container, base_url)\n",
    "                \n",
    "                print(\"Parsed with improved parser:\")\n",
    "                print(f\"Title: {publication['title']}\")\n",
    "                print(f\"Authors: {publication['authors']}\")\n",
    "                print(f\"Year: {publication['year']}\")\n",
    "                print(f\"Abstract: {publication['abstract'][:100]}...\" if publication['abstract'] else \"No abstract\")\n",
    "                \n",
    "        # Test on a known problematic publication\n",
    "        async with session.get(f\"{base_url}/publications/global-networks-monetary-policy-and-trade\") as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            print(\"\\nTesting on individual publication page:\")\n",
    "            \n",
    "            # For individual pages, we need to create a mock container\n",
    "            mock_container = BeautifulSoup(\"<div></div>\", \"html.parser\").div\n",
    "            \n",
    "            # Add citation if available\n",
    "            citation = soup.find(class_=\"biblio-citation\")\n",
    "            if citation:\n",
    "                mock_container.append(citation)\n",
    "                \n",
    "            # Add abstract if available\n",
    "            abstract = soup.find(class_=\"field-name-field-abstract\")\n",
    "            if abstract:\n",
    "                mock_container.append(abstract)\n",
    "                \n",
    "            # Add file links\n",
    "            cover_wrapper = soup.find(id=\"pub-cover-content-wrapper\")\n",
    "            if cover_wrapper:\n",
    "                mock_container.append(cover_wrapper)\n",
    "            \n",
    "            # Parse with improved parser\n",
    "            publication = improved_parse_publication(mock_container, base_url)\n",
    "            \n",
    "            print(\"Parsed with improved parser:\")\n",
    "            print(f\"Title: {publication['title']}\")\n",
    "            print(f\"Authors: {publication['authors']}\")\n",
    "            print(f\"Year: {publication['year']}\")\n",
    "            print(f\"Abstract: {publication['abstract'][:100]}...\" if publication['abstract'] else \"No abstract\")\n",
    "            print(f\"File URLs: {publication['file_urls']}\")\n",
    "\n",
    "# Run the test\n",
    "await test_improved_parser()\n",
    "\n",
    "# %%\n",
    "# Final solution with the improved selectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adea53c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing selector-focused parser\n",
      "\n",
      "Parsed publication from listing page:\n",
      "Title: Global Networks, Monetary Policy and Trade\n",
      "Authors: Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\n",
      "Year: 2025\n",
      "Abstract: We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n",
      "URL: https://growthlab.hks.harvard.edu/publications/global-networks-monetary-policy-and-trade\n",
      "File URLs: ['https://www.nber.org/system/files/working_papers/w33686/w33686.pdf']\n",
      "\n",
      "Parsed problematic publication page:\n",
      "Title: Global Networks, Monetary Policy and Trade\n",
      "Authors: Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\n",
      "Year: 2025\n",
      "No abstract\n",
      "URL: https://growthlab.hks.harvard.edu/publications/global-networks-monetary-policy-and-trade\n",
      "File URLs: ['https://www.nber.org/system/files/working_papers/w33686/w33686.pdf']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# FINAL FOCUSED IMPLEMENTATION: SELECTORS WITH CITATION FALLBACK\n",
    "\n",
    "# Refined selector configuration prioritizing direct selectors\n",
    "SELECTOR_CONFIG = {\n",
    "    \"publication\": {\n",
    "        \"container\": \"div.biblio-entry\",\n",
    "        \"title\": {\n",
    "            \"primary\": \"span.biblio-title\",\n",
    "            \"fallbacks\": [\"h1.page-title\", \"h2.title\", \"div.title\", \"span.field-biblio-title\"],\n",
    "            \"xpath\": \"//span[contains(@class, 'biblio-title')] | //h1[contains(@class, 'page-title')]\"\n",
    "        },\n",
    "        \"authors\": {\n",
    "            \"primary\": \"span.biblio-authors\", \n",
    "            \"fallbacks\": [\n",
    "                \"div.field-name-field-biblio-authors .field-item\",   # Individual page\n",
    "                \"div.authors\", \n",
    "                \"span.field-biblio-authors\"\n",
    "            ],\n",
    "            \"xpath\": \"//span[contains(@class, 'biblio-authors')] | //div[contains(@class, 'field-name-field-biblio-authors')]//div[@class='field-item']\"\n",
    "        },\n",
    "        \"abstract\": {\n",
    "            \"primary\": \"div.biblio-abstract-display\",\n",
    "            \"fallbacks\": [\n",
    "                \"div.field-name-field-abstract\",  # Individual page\n",
    "                \"div.abstract\", \n",
    "                \"div.field-biblio-abstract\"\n",
    "            ],\n",
    "            \"xpath\": \"//div[contains(@class, 'biblio-abstract-display')] | //div[contains(@class, 'field-name-field-abstract')]\"\n",
    "        },\n",
    "        \"file\": {\n",
    "            \"primary\": \"span.file\",\n",
    "            \"fallbacks\": [\n",
    "                \"#pub-cover-content-wrapper a\",  # SelectorGadget discovery for individual pages\n",
    "                \".Z3988+ a\",                     # Element after Z3988\n",
    "                \"a[href$='.pdf']\",               # Any PDF link\n",
    "                \"a[href*='files']\"               # Links containing \"files\"\n",
    "            ],\n",
    "            \"xpath\": \"//span[contains(@class, 'file')] | //*[@id='pub-cover-content-wrapper']//a | //*[contains(@class, 'Z3988')]/following-sibling::a | //a[contains(@href, '.pdf')]\"\n",
    "        },\n",
    "        \"citation\": {\n",
    "            \"primary\": \".biblio-citation\",       # Only used as fallback when other selectors fail\n",
    "            \"fallbacks\": [\n",
    "                \"div.field-name-field-citation\",\n",
    "                \"div.citation\"\n",
    "            ],\n",
    "            \"xpath\": \"//div[contains(@class, 'biblio-citation')] | //div[contains(@class, 'field-name-field-citation')]\"\n",
    "        }\n",
    "    },\n",
    "    \"pagination\": {\n",
    "        \"container\": \"ul.pager\",\n",
    "        \"last_page\": \"li.pager-last\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Helper function to find elements with fallbacks\n",
    "def find_with_fallbacks(element, config):\n",
    "    \"\"\"Find an element using primary selector with fallbacks\n",
    "    \n",
    "    Args:\n",
    "        element: BeautifulSoup element to search within\n",
    "        config: Selector configuration with primary and fallbacks\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup element or None if not found\n",
    "    \"\"\"\n",
    "    # Try primary selector\n",
    "    if \"primary\" in config and config[\"primary\"]:\n",
    "        try:\n",
    "            results = element.select(config[\"primary\"])\n",
    "            if results:\n",
    "                return results[0]\n",
    "        except:\n",
    "            # Fallback to simpler parsing for basic selectors\n",
    "            if \".\" in config[\"primary\"]:\n",
    "                tag, class_name = config[\"primary\"].split(\".\", 1)\n",
    "                tag = tag if tag else None\n",
    "                result = element.find(tag, class_=class_name)\n",
    "                if result:\n",
    "                    return result\n",
    "    \n",
    "    # Try fallbacks if provided\n",
    "    if \"fallbacks\" in config:\n",
    "        for fallback in config[\"fallbacks\"]:\n",
    "            try:\n",
    "                # Try using CSS selector API first (handles complex selectors)\n",
    "                results = element.select(fallback)\n",
    "                if results:\n",
    "                    return results[0]\n",
    "            except:\n",
    "                # Fallback to simpler parsing for basic selectors\n",
    "                if \".\" in fallback:\n",
    "                    tag, class_name = fallback.split(\".\", 1)\n",
    "                    tag = tag if tag else None\n",
    "                    result = element.find(tag, class_=class_name)\n",
    "                    if result:\n",
    "                        return result\n",
    "    \n",
    "    # Try XPath as last resort\n",
    "    if \"xpath\" in config:\n",
    "        try:\n",
    "            import lxml.html\n",
    "            from lxml import etree\n",
    "            \n",
    "            # We need to parse the HTML of the element\n",
    "            dom = lxml.html.fromstring(str(element))\n",
    "            xpath_result = dom.xpath(config[\"xpath\"])\n",
    "            \n",
    "            if xpath_result:\n",
    "                # Convert back to BeautifulSoup for consistency\n",
    "                result_html = etree.tostring(xpath_result[0])\n",
    "                result_soup = BeautifulSoup(result_html, \"html.parser\")\n",
    "                return result_soup.contents[0] if result_soup.contents else None\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Helper function to find all elements matching a selector with fallbacks\n",
    "def find_all_with_fallbacks(element, config):\n",
    "    \"\"\"Find all elements using primary selector with fallbacks\n",
    "    \n",
    "    Args:\n",
    "        element: BeautifulSoup element to search within\n",
    "        config: Selector configuration with primary and fallbacks\n",
    "    \n",
    "    Returns:\n",
    "        List of BeautifulSoup elements\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Try primary selector\n",
    "    if \"primary\" in config and config[\"primary\"]:\n",
    "        try:\n",
    "            found = element.select(config[\"primary\"])\n",
    "            if found:\n",
    "                results.extend(found)\n",
    "        except:\n",
    "            # Fallback to simpler parsing\n",
    "            if \".\" in config[\"primary\"]:\n",
    "                tag, class_name = config[\"primary\"].split(\".\", 1)\n",
    "                tag = tag if tag else None\n",
    "                found = element.find_all(tag, class_=class_name)\n",
    "                if found:\n",
    "                    results.extend(found)\n",
    "    \n",
    "    # If nothing found and fallbacks are provided, try them\n",
    "    if not results and \"fallbacks\" in config:\n",
    "        for fallback in config[\"fallbacks\"]:\n",
    "            try:\n",
    "                found = element.select(fallback)\n",
    "                if found:\n",
    "                    results.extend(found)\n",
    "                    break  # Stop at first successful fallback\n",
    "            except:\n",
    "                # Fallback to simpler parsing\n",
    "                if \".\" in fallback:\n",
    "                    tag, class_name = fallback.split(\".\", 1)\n",
    "                    tag = tag if tag else None\n",
    "                    found = element.find_all(tag, class_=class_name)\n",
    "                    if found:\n",
    "                        results.extend(found)\n",
    "                        break  # Stop at first successful fallback\n",
    "    \n",
    "    # If still nothing found and XPath is provided, try it as last resort\n",
    "    if not results and \"xpath\" in config:\n",
    "        try:\n",
    "            import lxml.html\n",
    "            from lxml import etree\n",
    "            \n",
    "            dom = lxml.html.fromstring(str(element))\n",
    "            xpath_results = dom.xpath(config[\"xpath\"])\n",
    "            \n",
    "            if xpath_results:\n",
    "                for result in xpath_results:\n",
    "                    result_html = etree.tostring(result)\n",
    "                    result_soup = BeautifulSoup(result_html, \"html.parser\")\n",
    "                    if result_soup.contents:\n",
    "                        results.append(result_soup.contents[0])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Helper function to extract components from citation when needed\n",
    "def extract_from_citation(citation_text):\n",
    "    \"\"\"Extract author, year, and title from a citation string\n",
    "    \n",
    "    Note: Only used as fallback when direct selectors fail\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    if not citation_text:\n",
    "        return result\n",
    "    \n",
    "    # Try to extract year\n",
    "    year_match = re.search(r'(\\d{4})\\. ', citation_text)\n",
    "    if year_match:\n",
    "        year_pos = year_match.start()\n",
    "        after_year = year_match.end()\n",
    "        \n",
    "        # Author is everything before the year\n",
    "        result[\"authors\"] = citation_text[:year_pos].strip()\n",
    "        if result[\"authors\"].endswith(','):\n",
    "            result[\"authors\"] = result[\"authors\"][:-1]\n",
    "        \n",
    "        # Title is everything after the year until next period or end\n",
    "        title_end = citation_text.find('.', after_year)\n",
    "        if title_end > after_year:\n",
    "            result[\"title\"] = citation_text[after_year:title_end].strip()\n",
    "        else:\n",
    "            result[\"title\"] = citation_text[after_year:].strip()\n",
    "        \n",
    "        # Extract year as integer\n",
    "        try:\n",
    "            result[\"year\"] = int(year_match.group(1))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Main parse_publication function focused on selectors first, citation as fallback\n",
    "def parse_publication(element, base_url):\n",
    "    \"\"\"Parse a publication using configurable selectors with fallbacks\n",
    "    \n",
    "    This function prioritizes direct selectors and only falls back to citation\n",
    "    extraction when direct selectors fail.\n",
    "    \n",
    "    Args:\n",
    "        element: BeautifulSoup element containing the publication\n",
    "        base_url: Base URL for resolving relative URLs\n",
    "        \n",
    "    Returns:\n",
    "        dict: Publication data\n",
    "    \"\"\"\n",
    "    # Initialize the publication object\n",
    "    publication = {\n",
    "        \"title\": None,\n",
    "        \"authors\": None,\n",
    "        \"year\": None,\n",
    "        \"abstract\": None,\n",
    "        \"pub_url\": None,\n",
    "        \"file_urls\": [],\n",
    "        \"source\": \"GrowthLab\"\n",
    "    }\n",
    "    \n",
    "    # 1. Find title using direct selectors\n",
    "    title_element = find_with_fallbacks(element, SELECTOR_CONFIG[\"publication\"][\"title\"])\n",
    "    if title_element:\n",
    "        publication[\"title\"] = title_element.text.strip()\n",
    "        \n",
    "        # Get URL from title link\n",
    "        title_link = title_element.find(\"a\")\n",
    "        if title_link and title_link.get(\"href\"):\n",
    "            pub_url = title_link[\"href\"]\n",
    "            if not pub_url.startswith((\"http://\", \"https://\")):\n",
    "                pub_url = f\"{base_url}{pub_url}\"\n",
    "            publication[\"pub_url\"] = pub_url\n",
    "    \n",
    "    # 2. Find authors using direct selectors\n",
    "    authors_element = find_with_fallbacks(element, SELECTOR_CONFIG[\"publication\"][\"authors\"])\n",
    "    if authors_element:\n",
    "        publication[\"authors\"] = authors_element.text.strip()\n",
    "    \n",
    "    # 3. Extract year from text after authors\n",
    "    if authors_element:\n",
    "        sibling_text = authors_element.next_sibling\n",
    "        if sibling_text:\n",
    "            year_match = re.search(r\"\\b(19|20)\\d{2}\\b\", sibling_text)\n",
    "            if year_match:\n",
    "                publication[\"year\"] = int(year_match.group())\n",
    "    \n",
    "    # 4. Find abstract using direct selectors\n",
    "    abstract_element = find_with_fallbacks(element, SELECTOR_CONFIG[\"publication\"][\"abstract\"])\n",
    "    if abstract_element:\n",
    "        publication[\"abstract\"] = abstract_element.text.strip()\n",
    "    \n",
    "    # 5. Find file URLs using direct selectors and multiple methods\n",
    "    file_urls = set()  # Use a set to avoid duplicates\n",
    "    \n",
    "    # Try all file selectors\n",
    "    for selector_type in [\"primary\", \"fallbacks\"]:\n",
    "        if selector_type == \"primary\":\n",
    "            selectors = [SELECTOR_CONFIG[\"publication\"][\"file\"][\"primary\"]]\n",
    "        else:\n",
    "            selectors = SELECTOR_CONFIG[\"publication\"][\"file\"].get(\"fallbacks\", [])\n",
    "            \n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                elements = element.select(selector)\n",
    "                for elem in elements:\n",
    "                    if elem.name == \"a\" and elem.get(\"href\"):\n",
    "                        # Direct link\n",
    "                        file_url = elem[\"href\"]\n",
    "                        if not file_url.startswith((\"http://\", \"https://\")):\n",
    "                            file_url = f\"{base_url}{file_url}\"\n",
    "                        file_urls.add(file_url)\n",
    "                    else:\n",
    "                        # Container with links\n",
    "                        for link in elem.find_all(\"a\"):\n",
    "                            if link.get(\"href\"):\n",
    "                                file_url = link[\"href\"]\n",
    "                                if not file_url.startswith((\"http://\", \"https://\")):\n",
    "                                    file_url = f\"{base_url}{file_url}\"\n",
    "                                file_urls.add(file_url)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Convert set back to list\n",
    "    publication[\"file_urls\"] = list(file_urls)\n",
    "    \n",
    "    # 6. FALLBACK: If critical fields are missing, try citation extraction\n",
    "    if not publication[\"title\"] or not publication[\"authors\"] or not publication[\"year\"]:\n",
    "        citation_element = find_with_fallbacks(element, SELECTOR_CONFIG[\"publication\"][\"citation\"])\n",
    "        if citation_element:\n",
    "            citation_text = citation_element.text.strip()\n",
    "            citation_data = extract_from_citation(citation_text)\n",
    "            \n",
    "            # Only use citation data for missing fields\n",
    "            if not publication[\"title\"] and \"title\" in citation_data:\n",
    "                publication[\"title\"] = citation_data[\"title\"]\n",
    "            if not publication[\"authors\"] and \"authors\" in citation_data:\n",
    "                publication[\"authors\"] = citation_data[\"authors\"]\n",
    "            if not publication[\"year\"] and \"year\" in citation_data:\n",
    "                publication[\"year\"] = citation_data[\"year\"]\n",
    "    \n",
    "    return publication\n",
    "\n",
    "# Demo using the revised selector-focused parser\n",
    "async def demo_selector_focused_parser():\n",
    "    \"\"\"Demonstrate the selector-focused parser with citation fallbacks\"\"\"\n",
    "    print(\"\\n🧪 Testing selector-focused parser\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    base_url = \"https://growthlab.hks.harvard.edu\"\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        # Test on the main publications page\n",
    "        async with session.get(f\"{base_url}/publications\") as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Get the first publication container\n",
    "            container = soup.find(\"div\", class_=\"biblio-entry\")\n",
    "            if not container:\n",
    "                print(\"No publication found\")\n",
    "                return\n",
    "            \n",
    "            # Parse the publication with our selector-focused parser\n",
    "            publication = parse_publication(container, base_url)\n",
    "            \n",
    "            print(\"\\nParsed publication from listing page:\")\n",
    "            print(f\"Title: {publication['title']}\")\n",
    "            print(f\"Authors: {publication['authors']}\")\n",
    "            print(f\"Year: {publication['year']}\")\n",
    "            print(f\"Abstract: {publication['abstract'][:100]}...\" if publication['abstract'] else \"No abstract\")\n",
    "            print(f\"URL: {publication['pub_url']}\")\n",
    "            print(f\"File URLs: {publication['file_urls']}\")\n",
    "            \n",
    "        # Test on the problematic publication page we identified\n",
    "        test_url = \"https://growthlab.hks.harvard.edu/publications/global-networks-monetary-policy-and-trade\"\n",
    "        async with session.get(test_url) as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Parse the publication with our selector-focused parser\n",
    "            publication = parse_publication(soup, base_url)\n",
    "            \n",
    "            print(\"\\nParsed problematic publication page:\")\n",
    "            print(f\"Title: {publication['title']}\")\n",
    "            print(f\"Authors: {publication['authors']}\")\n",
    "            print(f\"Year: {publication['year']}\")\n",
    "            print(f\"Abstract: {publication['abstract'][:100]}...\" if publication['abstract'] else \"No abstract\")\n",
    "            print(f\"URL: {publication['pub_url'] or test_url}\")\n",
    "            print(f\"File URLs: {publication['file_urls']}\")\n",
    "\n",
    "# Run the demo\n",
    "await demo_selector_focused_parser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1de0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This allows running async code in Jupyter\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the selector configuration with primary selectors, fallbacks, and XPath alternatives\n",
    "SELECTOR_CONFIG = {\n",
    "    \"publication\": {\n",
    "        \"container\": {\n",
    "            \"primary\": \"div.biblio-entry\", \n",
    "            \"fallbacks\": [\"div.node-biblio\", \"article.publication\", \"div.publication-item\"],\n",
    "            \"xpath\": \"//div[contains(@class, 'biblio-entry')]\",\n",
    "            \"description\": \"Publication container\"\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"primary\": \"span.biblio-title\", \n",
    "            \"fallbacks\": [\"h1.page-title\", \"h2.publication-title\", \"h3.title\", \"div.title\"],\n",
    "            \"xpath\": \"//span[contains(@class, 'biblio-title')] | //h1[contains(@class, 'page-title')]\",\n",
    "            \"description\": \"Publication title\"\n",
    "        },\n",
    "        \"authors\": {\n",
    "            \"primary\": \"span.biblio-authors\", \n",
    "            \"fallbacks\": [\n",
    "                \"div.field-name-field-biblio-authors .field-item\",\n",
    "                \"div.authors\", \n",
    "                \"p.author-list\", \n",
    "                \"div.publication-authors\"\n",
    "            ],\n",
    "            \"xpath\": \"//span[contains(@class, 'biblio-authors')] | //div[contains(@class, 'field-name-field-biblio-authors')]//div[@class='field-item']\",\n",
    "            \"description\": \"Publication authors\"\n",
    "        },\n",
    "        \"abstract\": {\n",
    "            \"primary\": \"div.biblio-abstract-display\", \n",
    "            \"fallbacks\": [\n",
    "                \"div.field-name-field-abstract\",\n",
    "                \"div.abstract\", \n",
    "                \"div.publication-abstract\", \n",
    "                \"p.abstract\"\n",
    "            ],\n",
    "            \"xpath\": \"//div[contains(@class, 'biblio-abstract-display')] | //div[contains(@class, 'field-name-field-abstract')]\",\n",
    "            \"description\": \"Publication abstract\"\n",
    "        },\n",
    "        \"file\": {\n",
    "            \"primary\": \"span.file\", \n",
    "            \"fallbacks\": [\n",
    "                \"#pub-cover-content-wrapper a\",  # SelectorGadget discovery\n",
    "                \".Z3988+ a\",                     # Element after Z3988\n",
    "                \"a.biblio-download\", \n",
    "                \"a[href$='.pdf']\",\n",
    "                \"a[href*='files']\"\n",
    "            ],\n",
    "            \"xpath\": \"//span[contains(@class, 'file')] | //*[@id='pub-cover-content-wrapper']//a | //*[contains(@class, 'Z3988')]/following-sibling::a | //a[contains(@href, '.pdf')]\",\n",
    "            \"description\": \"Publication files\"\n",
    "        },\n",
    "        \"citation\": {\n",
    "            \"primary\": \".biblio-citation\",\n",
    "            \"fallbacks\": [\"div.field-name-field-citation\", \"div.citation\"],\n",
    "            \"xpath\": \"//div[contains(@class, 'biblio-citation')] | //div[contains(@class, 'field-name-field-citation')]\",\n",
    "            \"description\": \"Citation information\"\n",
    "        }\n",
    "    },\n",
    "    \"pagination\": {\n",
    "        \"container\": {\n",
    "            \"primary\": \"ul.pager\",\n",
    "            \"fallbacks\": [\"div.pagination\", \"nav.pagination\"],\n",
    "            \"xpath\": \"//ul[contains(@class, 'pager')]\",\n",
    "            \"description\": \"Pagination container\"\n",
    "        },\n",
    "        \"last_page\": {\n",
    "            \"primary\": \"li.pager-last\",\n",
    "            \"fallbacks\": [\"li.page-item:last-child\", \"a.page-link:last-child\"],\n",
    "            \"xpath\": \"//li[contains(@class, 'pager-last')]\",\n",
    "            \"description\": \"Last page link\"\n",
    "        }\n",
    "    },\n",
    "    \"endnote\": {\n",
    "        \"link\": {\n",
    "            \"primary\": \"li.biblio_tagged a\",\n",
    "            \"fallbacks\": [\"a[href*='tagged=1']\", \"a[href*='endnote']\", \"a.endnote-link\"],\n",
    "            \"xpath\": \"//a[contains(@href, 'tagged=1')] | //a[contains(@href, 'endnote')]\",\n",
    "            \"description\": \"Endnote link\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a simple placeholder for GrowthLabPublication\n",
    "class SimpleGrowthLabPublication(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def generate_id(self):\n",
    "        # Simple hash based on title and authors\n",
    "        title = self.get(\"title\", \"\")\n",
    "        authors = self.get(\"authors\", \"\")\n",
    "        return f\"pub_{hash(title + authors) % 10000}\"\n",
    "        \n",
    "    def generate_content_hash(self):\n",
    "        # Simple hash of content\n",
    "        content = json.dumps(self, sort_keys=True)\n",
    "        return f\"hash_{hash(content) % 10000}\"\n",
    "        \n",
    "    @property\n",
    "    def paper_id(self):\n",
    "        return self.get(\"paper_id\", \"\")\n",
    "        \n",
    "    @paper_id.setter\n",
    "    def paper_id(self, value):\n",
    "        self[\"paper_id\"] = value\n",
    "        \n",
    "    @property\n",
    "    def content_hash(self):\n",
    "        return self.get(\"content_hash\", \"\")\n",
    "        \n",
    "    @content_hash.setter\n",
    "    def content_hash(self, value):\n",
    "        self[\"content_hash\"] = value\n",
    "        \n",
    "    @property\n",
    "    def title(self):\n",
    "        return self.get(\"title\", \"\")\n",
    "        \n",
    "    @title.setter\n",
    "    def title(self, value):\n",
    "        self[\"title\"] = value\n",
    "        \n",
    "    @property\n",
    "    def authors(self):\n",
    "        return self.get(\"authors\", \"\")\n",
    "        \n",
    "    @authors.setter\n",
    "    def authors(self, value):\n",
    "        self[\"authors\"] = value\n",
    "        \n",
    "    @property\n",
    "    def abstract(self):\n",
    "        return self.get(\"abstract\", \"\")\n",
    "        \n",
    "    @abstract.setter\n",
    "    def abstract(self, value):\n",
    "        self[\"abstract\"] = value\n",
    "        \n",
    "    @property\n",
    "    def pub_url(self):\n",
    "        return self.get(\"pub_url\", \"\")\n",
    "    \n",
    "    @pub_url.setter\n",
    "    def pub_url(self, value):\n",
    "        self[\"pub_url\"] = value\n",
    "        \n",
    "    @property\n",
    "    def file_urls(self):\n",
    "        return self.get(\"file_urls\", [])\n",
    "    \n",
    "    @file_urls.setter\n",
    "    def file_urls(self, value):\n",
    "        self[\"file_urls\"] = value\n",
    "    \n",
    "    @property\n",
    "    def year(self):\n",
    "        return self.get(\"year\")\n",
    "    \n",
    "    @year.setter\n",
    "    def year(self, value):\n",
    "        self[\"year\"] = value\n",
    "\n",
    "class SelectorMonitor:\n",
    "    \"\"\"Class to monitor selector performance and detect failures\"\"\"\n",
    "    \n",
    "    def __init__(self, selectors=None):\n",
    "        self.selectors = selectors or SELECTOR_CONFIG\n",
    "        self.stats = {\n",
    "            \"total_pages\": 0,\n",
    "            \"total_publications\": 0,\n",
    "            \"selector_success\": {},\n",
    "            \"selector_failure\": {},\n",
    "            \"alerts\": []\n",
    "        }\n",
    "        \n",
    "        # Initialize stats for each selector\n",
    "        for section, section_config in self.selectors.items():\n",
    "            for name, selector_config in section_config.items():\n",
    "                key = f\"{section}.{name}\"\n",
    "                self.stats[\"selector_success\"][key] = 0\n",
    "                self.stats[\"selector_failure\"][key] = 0\n",
    "    \n",
    "    def record_success(self, section, name, used_selector=None):\n",
    "        \"\"\"Record a successful selector use\"\"\"\n",
    "        key = f\"{section}.{name}\"\n",
    "        if key in self.stats[\"selector_success\"]:\n",
    "            self.stats[\"selector_success\"][key] += 1\n",
    "            \n",
    "            # Record which selector was actually used\n",
    "            if used_selector:\n",
    "                if not hasattr(self, \"selector_usage\"):\n",
    "                    self.selector_usage = {}\n",
    "                \n",
    "                if key not in self.selector_usage:\n",
    "                    self.selector_usage[key] = {}\n",
    "                \n",
    "                self.selector_usage[key][used_selector] = self.selector_usage[key].get(used_selector, 0) + 1\n",
    "    \n",
    "    def record_failure(self, section, name):\n",
    "        \"\"\"Record a failed selector use\"\"\"\n",
    "        key = f\"{section}.{name}\"\n",
    "        if key in self.stats[\"selector_failure\"]:\n",
    "            self.stats[\"selector_failure\"][key] += 1\n",
    "            \n",
    "            # Check if failure rate is high enough to trigger alert\n",
    "            total = self.stats[\"selector_success\"][key] + self.stats[\"selector_failure\"][key]\n",
    "            if total >= 5:  # Only check after a minimum sample\n",
    "                failure_rate = self.stats[\"selector_failure\"][key] / total\n",
    "                if failure_rate > 0.5:  # Alert if more than 50% failure\n",
    "                    self.create_alert(section, name, failure_rate)\n",
    "    \n",
    "    def create_alert(self, section, name, failure_rate):\n",
    "        \"\"\"Create an alert for a failing selector\"\"\"\n",
    "        selector_config = self.selectors[section][name]\n",
    "        alert = {\n",
    "            \"timestamp\": None,  # Would use datetime in real implementation\n",
    "            \"selector\": f\"{section}.{name}\",\n",
    "            \"failure_rate\": failure_rate,\n",
    "            \"primary\": selector_config[\"primary\"],\n",
    "            \"fallbacks\": selector_config[\"fallbacks\"],\n",
    "            \"message\": f\"Selector {section}.{name} is failing at a rate of {failure_rate:.2%}\"\n",
    "        }\n",
    "        \n",
    "        # Check if we already have an alert for this selector\n",
    "        existing_alerts = [a for a in self.stats[\"alerts\"] if a[\"selector\"] == alert[\"selector\"]]\n",
    "        if not existing_alerts:\n",
    "            self.stats[\"alerts\"].append(alert)\n",
    "            logger.warning(f\"SELECTOR ALERT: {alert['message']}\")\n",
    "    \n",
    "    def record_page_processed(self):\n",
    "        \"\"\"Record that a page was processed\"\"\"\n",
    "        self.stats[\"total_pages\"] += 1\n",
    "    \n",
    "    def record_publication_processed(self):\n",
    "        \"\"\"Record that a publication was processed\"\"\"\n",
    "        self.stats[\"total_publications\"] += 1\n",
    "    \n",
    "    def check_selector_health(self):\n",
    "        \"\"\"Check the health of all selectors\"\"\"\n",
    "        print(\"\\n🔍 Selector Health Check:\")\n",
    "        \n",
    "        for section, section_config in self.selectors.items():\n",
    "            print(f\"\\n{section.upper()} Selectors:\")\n",
    "            \n",
    "            for name, selector_config in section_config.items():\n",
    "                key = f\"{section}.{name}\"\n",
    "                success = self.stats[\"selector_success\"].get(key, 0)\n",
    "                failure = self.stats[\"selector_failure\"].get(key, 0)\n",
    "                total = success + failure\n",
    "                \n",
    "                if total > 0:\n",
    "                    success_rate = success / total\n",
    "                    status = \"✅ GOOD\" if success_rate >= 0.9 else \"⚠️ WARNING\" if success_rate >= 0.5 else \"❌ FAILING\"\n",
    "                    print(f\"  - {key}: {status} ({success}/{total}, {success_rate:.1%})\")\n",
    "                else:\n",
    "                    print(f\"  - {key}: ⚪ NO DATA\")\n",
    "    \n",
    "    def print_selector_usage(self):\n",
    "        \"\"\"Print which selectors were actually used\"\"\"\n",
    "        if not hasattr(self, \"selector_usage\"):\n",
    "            print(\"\\nNo selector usage data available\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n📊 Selector Usage Statistics:\")\n",
    "        \n",
    "        for key, usage in self.selector_usage.items():\n",
    "            print(f\"\\n{key}:\")\n",
    "            total_uses = sum(usage.values())\n",
    "            \n",
    "            for selector, count in sorted(usage.items(), key=lambda x: x[1], reverse=True):\n",
    "                percentage = count / total_uses * 100\n",
    "                print(f\"  - {selector}: {count} times ({percentage:.1f}%)\")\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a full report of selector performance\"\"\"\n",
    "        print(\"\\n📊 Selector Performance Report\")\n",
    "        print(f\"Pages processed: {self.stats['total_pages']}\")\n",
    "        print(f\"Publications processed: {self.stats['total_publications']}\")\n",
    "        \n",
    "        # Calculate overall selector success rate\n",
    "        total_success = sum(self.stats[\"selector_success\"].values())\n",
    "        total_failure = sum(self.stats[\"selector_failure\"].values())\n",
    "        total_attempts = total_success + total_failure\n",
    "        \n",
    "        if total_attempts > 0:\n",
    "            overall_rate = total_success / total_attempts\n",
    "            print(f\"Overall selector success rate: {overall_rate:.2%}\")\n",
    "        \n",
    "        # Print selector-specific stats\n",
    "        print(\"\\nSelector Performance:\")\n",
    "        for key in sorted(self.stats[\"selector_success\"].keys()):\n",
    "            success = self.stats[\"selector_success\"][key]\n",
    "            failure = self.stats[\"selector_failure\"][key]\n",
    "            total = success + failure\n",
    "            \n",
    "            if total > 0:\n",
    "                rate = success / total\n",
    "                status = \"✅ GOOD\" if rate >= 0.9 else \"⚠️ WARNING\" if rate >= 0.5 else \"❌ FAILING\"\n",
    "                print(f\"  - {key}: {status} {rate:.2%} success ({success}/{total})\")\n",
    "        \n",
    "        # Print selector usage statistics\n",
    "        self.print_selector_usage()\n",
    "\n",
    "# Simplified retry function\n",
    "async def retry_with_backoff(func, *args, max_retries=3, base_delay=1.0, max_delay=30.0, retry_on=Exception, **kwargs):\n",
    "    \"\"\"Retry a function with exponential backoff\"\"\"\n",
    "    retries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            return await func(*args, **kwargs)\n",
    "        except retry_on as e:\n",
    "            if retries >= max_retries:\n",
    "                raise\n",
    "            delay = min(base_delay * (2 ** retries), max_delay)\n",
    "            logger.info(f\"Retrying after error: {e}. Attempt {retries+1}/{max_retries}. Waiting {delay:.2f}s...\")\n",
    "            await asyncio.sleep(delay)\n",
    "            retries += 1\n",
    "\n",
    "# Selector utility functions\n",
    "def find_with_selectors(element, selector_config, monitor=None, section=None, name=None):\n",
    "    \"\"\"Find an element using selectors with fallbacks\n",
    "    \n",
    "    Args:\n",
    "        element: BeautifulSoup element to search within\n",
    "        selector_config: Configuration with primary, fallbacks, xpath\n",
    "        monitor: Optional SelectorMonitor to record success/failure\n",
    "        section: Section name for monitoring\n",
    "        name: Selector name for monitoring\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (element, selector_used) or (None, None) if not found\n",
    "    \"\"\"\n",
    "    # Try primary selector\n",
    "    if selector_config[\"primary\"]:\n",
    "        try:\n",
    "            results = element.select(selector_config[\"primary\"])\n",
    "            if results:\n",
    "                if monitor and section and name:\n",
    "                    monitor.record_success(section, name, selector_config[\"primary\"])\n",
    "                return results[0], selector_config[\"primary\"]\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error with primary selector {selector_config['primary']}: {e}\")\n",
    "    \n",
    "    # Try fallbacks\n",
    "    for fallback in selector_config.get(\"fallbacks\", []):\n",
    "        try:\n",
    "            results = element.select(fallback)\n",
    "            if results:\n",
    "                if monitor and section and name:\n",
    "                    monitor.record_success(section, name, fallback)\n",
    "                return results[0], fallback\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error with fallback selector {fallback}: {e}\")\n",
    "    \n",
    "    # Try XPath as last resort\n",
    "    if \"xpath\" in selector_config and selector_config[\"xpath\"]:\n",
    "        try:\n",
    "            import lxml.html\n",
    "            from lxml import etree\n",
    "            \n",
    "            # Parse the HTML of the element\n",
    "            dom = lxml.html.fromstring(str(element))\n",
    "            xpath_results = dom.xpath(selector_config[\"xpath\"])\n",
    "            \n",
    "            if xpath_results:\n",
    "                if monitor and section and name:\n",
    "                    monitor.record_success(section, name, \"xpath\")\n",
    "                \n",
    "                # Convert back to BeautifulSoup for consistency\n",
    "                result_html = etree.tostring(xpath_results[0])\n",
    "                result_soup = BeautifulSoup(result_html, \"html.parser\")\n",
    "                if result_soup.contents:\n",
    "                    return result_soup.contents[0], \"xpath\"\n",
    "        except ImportError:\n",
    "            logger.debug(\"lxml not available for XPath queries\")\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error with XPath: {e}\")\n",
    "    \n",
    "    # Record failure if we got here\n",
    "    if monitor and section and name:\n",
    "        monitor.record_failure(section, name)\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def find_all_with_selectors(element, selector_config, monitor=None, section=None, name=None):\n",
    "    \"\"\"Find all elements matching selectors with fallbacks\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (elements, selector_used) or ([], None) if none found\n",
    "    \"\"\"\n",
    "    # Try primary selector\n",
    "    if selector_config[\"primary\"]:\n",
    "        try:\n",
    "            results = element.select(selector_config[\"primary\"])\n",
    "            if results:\n",
    "                if monitor and section and name:\n",
    "                    monitor.record_success(section, name, selector_config[\"primary\"])\n",
    "                return results, selector_config[\"primary\"]\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error with primary selector {selector_config['primary']}: {e}\")\n",
    "    \n",
    "    # Try fallbacks\n",
    "    for fallback in selector_config.get(\"fallbacks\", []):\n",
    "        try:\n",
    "            results = element.select(fallback)\n",
    "            if results:\n",
    "                if monitor and section and name:\n",
    "                    monitor.record_success(section, name, fallback)\n",
    "                return results, fallback\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error with fallback selector {fallback}: {e}\")\n",
    "    \n",
    "    # Try XPath\n",
    "    if \"xpath\" in selector_config and selector_config[\"xpath\"]:\n",
    "        try:\n",
    "            import lxml.html\n",
    "            from lxml import etree\n",
    "            \n",
    "            dom = lxml.html.fromstring(str(element))\n",
    "            xpath_results = dom.xpath(selector_config[\"xpath\"])\n",
    "            \n",
    "            if xpath_results:\n",
    "                if monitor and section and name:\n",
    "                    monitor.record_success(section, name, \"xpath\")\n",
    "                \n",
    "                bs_results = []\n",
    "                for result in xpath_results:\n",
    "                    result_html = etree.tostring(result)\n",
    "                    result_soup = BeautifulSoup(result_html, \"html.parser\")\n",
    "                    if result_soup.contents:\n",
    "                        bs_results.append(result_soup.contents[0])\n",
    "                \n",
    "                if bs_results:\n",
    "                    return bs_results, \"xpath\"\n",
    "        except ImportError:\n",
    "            logger.debug(\"lxml not available for XPath queries\")\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error with XPath: {e}\")\n",
    "    \n",
    "    # Record failure\n",
    "    if monitor and section and name:\n",
    "        monitor.record_failure(section, name)\n",
    "    \n",
    "    return [], None\n",
    "\n",
    "# Extract components from citation\n",
    "def extract_from_citation(citation_text):\n",
    "    \"\"\"Extract author, year, and title from a citation string\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    if not citation_text:\n",
    "        return result\n",
    "    \n",
    "    # Try to extract year\n",
    "    year_match = re.search(r'(\\d{4})\\. ', citation_text)\n",
    "    if year_match:\n",
    "        year_pos = year_match.start()\n",
    "        after_year = year_match.end()\n",
    "        \n",
    "        # Author is everything before the year\n",
    "        result[\"authors\"] = citation_text[:year_pos].strip()\n",
    "        if result[\"authors\"].endswith(','):\n",
    "            result[\"authors\"] = result[\"authors\"][:-1]\n",
    "        \n",
    "        # Title is everything after the year until next period or end\n",
    "        title_end = citation_text.find('.', after_year)\n",
    "        if title_end > after_year:\n",
    "            result[\"title\"] = citation_text[after_year:title_end].strip()\n",
    "        else:\n",
    "            result[\"title\"] = citation_text[after_year:].strip()\n",
    "        \n",
    "        # Extract year as integer\n",
    "        try:\n",
    "            result[\"year\"] = int(year_match.group(1))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Function to parse a publication with our selector configuration\n",
    "def parse_publication(element, base_url, monitor=None, year_corrections=None):\n",
    "    \"\"\"Parse a publication using configured selectors with fallbacks\"\"\"\n",
    "    # Initialize the publication\n",
    "    publication = SimpleGrowthLabPublication(\n",
    "        title=None,\n",
    "        authors=None,\n",
    "        year=None,\n",
    "        abstract=None,\n",
    "        pub_url=None,\n",
    "        file_urls=[],\n",
    "        source=\"GrowthLab\"\n",
    "    )\n",
    "    \n",
    "    # Record that we're processing a publication\n",
    "    if monitor:\n",
    "        monitor.record_publication_processed()\n",
    "    \n",
    "    # 1. Find title using selectors\n",
    "    title_element, title_selector = find_with_selectors(\n",
    "        element, \n",
    "        SELECTOR_CONFIG[\"publication\"][\"title\"],\n",
    "        monitor, \"publication\", \"title\"\n",
    "    )\n",
    "    \n",
    "    if title_element:\n",
    "        publication.title = title_element.text.strip()\n",
    "        \n",
    "        # Get publication URL from title link\n",
    "        title_link = title_element.find(\"a\")\n",
    "        if title_link and title_link.get(\"href\"):\n",
    "            pub_url = title_link[\"href\"]\n",
    "            # Ensure URL is absolute\n",
    "            if not pub_url.startswith((\"http://\", \"https://\")):\n",
    "                pub_url = f\"{base_url}{pub_url}\" if not pub_url.startswith(\"/\") else f\"{base_url.rstrip('/')}{pub_url}\"\n",
    "            publication.pub_url = pub_url\n",
    "    \n",
    "    # 2. Find authors\n",
    "    authors_element, authors_selector = find_with_selectors(\n",
    "        element, \n",
    "        SELECTOR_CONFIG[\"publication\"][\"authors\"],\n",
    "        monitor, \"publication\", \"authors\"\n",
    "    )\n",
    "    \n",
    "    if authors_element:\n",
    "        publication.authors = authors_element.text.strip()\n",
    "    \n",
    "    # 3. Extract year from text after authors\n",
    "    if authors_element:\n",
    "        sibling_text = authors_element.next_sibling\n",
    "        if sibling_text:\n",
    "            year_match = re.search(r\"\\b(19|20)\\d{2}\\b\", str(sibling_text))\n",
    "            if year_match:\n",
    "                publication.year = int(year_match.group())\n",
    "    \n",
    "    # 4. Find abstract\n",
    "    abstract_element, abstract_selector = find_with_selectors(\n",
    "        element, \n",
    "        SELECTOR_CONFIG[\"publication\"][\"abstract\"],\n",
    "        monitor, \"publication\", \"abstract\"\n",
    "    )\n",
    "    \n",
    "    if abstract_element:\n",
    "        publication.abstract = abstract_element.text.strip()\n",
    "    \n",
    "    # 5. Find file URLs\n",
    "    file_elements, file_selector = find_all_with_selectors(\n",
    "        element, \n",
    "        SELECTOR_CONFIG[\"publication\"][\"file\"],\n",
    "        monitor, \"publication\", \"file\"\n",
    "    )\n",
    "    \n",
    "    file_urls = set()  # Use set to avoid duplicates\n",
    "    \n",
    "    for elem in file_elements:\n",
    "        if elem.name == \"a\" and elem.get(\"href\"):\n",
    "            # Direct link\n",
    "            file_url = elem[\"href\"]\n",
    "            if not file_url.startswith((\"http://\", \"https://\")):\n",
    "                file_url = f\"{base_url}{file_url}\" if not file_url.startswith(\"/\") else f\"{base_url.rstrip('/')}{file_url}\"\n",
    "            file_urls.add(file_url)\n",
    "        else:\n",
    "            # Container with links\n",
    "            for link in elem.find_all(\"a\"):\n",
    "                if link.get(\"href\"):\n",
    "                    file_url = link[\"href\"]\n",
    "                    if not file_url.startswith((\"http://\", \"https://\")):\n",
    "                        file_url = f\"{base_url}{file_url}\" if not file_url.startswith(\"/\") else f\"{base_url.rstrip('/')}{file_url}\"\n",
    "                    file_urls.add(file_url)\n",
    "    \n",
    "    publication.file_urls = list(file_urls)\n",
    "    \n",
    "    # 6. FALLBACK: If critical fields are still missing, try citation\n",
    "    if not publication.title or not publication.authors or not publication.year:\n",
    "        citation_element, citation_selector = find_with_selectors(\n",
    "            element, \n",
    "            SELECTOR_CONFIG[\"publication\"][\"citation\"],\n",
    "            monitor, \"publication\", \"citation\"\n",
    "        )\n",
    "        \n",
    "        if citation_element:\n",
    "            citation_text = citation_element.text.strip()\n",
    "            citation_data = extract_from_citation(citation_text)\n",
    "            \n",
    "            # Only use citation data for missing fields\n",
    "            if not publication.title and \"title\" in citation_data:\n",
    "                publication.title = citation_data[\"title\"]\n",
    "            if not publication.authors and \"authors\" in citation_data:\n",
    "                publication.authors = citation_data[\"authors\"]\n",
    "            if not publication.year and \"year\" in citation_data:\n",
    "                publication.year = citation_data[\"year\"]\n",
    "    \n",
    "    # 7. Apply year corrections if needed\n",
    "    if publication.pub_url and year_corrections and publication.pub_url in year_corrections:\n",
    "        publication.year = year_corrections[publication.pub_url]\n",
    "    \n",
    "    # Generate stable ID and content hash\n",
    "    publication.paper_id = publication.generate_id()\n",
    "    publication.content_hash = publication.generate_content_hash()\n",
    "    \n",
    "    return publication\n",
    "\n",
    "# Class for scraping Growth Lab website\n",
    "class GrowthLabScraperWithSelectors:\n",
    "    \"\"\"Enhanced Growth Lab scraper with configurable selectors\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"Initialize the scraper with configuration\"\"\"\n",
    "        self.config = config or {\n",
    "            \"base_url\": \"https://growthlab.hks.harvard.edu/publications\",\n",
    "            \"scrape_delay\": 1.0,  # Reduced for testing\n",
    "            \"concurrency_limit\": 2,  # Reduced for testing\n",
    "            \"max_retries\": 3,\n",
    "            \"retry_base_delay\": 1.0,\n",
    "            \"retry_max_delay\": 10.0,\n",
    "        }\n",
    "        \n",
    "        self.base_url = self.config[\"base_url\"]\n",
    "        self.scrape_delay = self.config[\"scrape_delay\"]\n",
    "        self.concurrency_limit = self.config[\"concurrency_limit\"]\n",
    "        self.semaphore = asyncio.Semaphore(self.concurrency_limit)\n",
    "        self.monitor = SelectorMonitor(SELECTOR_CONFIG)\n",
    "        \n",
    "        # Sample year corrections\n",
    "        self.year_corrections = {\n",
    "            \"https://growthlab.hks.harvard.edu/publications/sri-lanka-growth-diagnostic\": 2018,\n",
    "            \"https://growthlab.hks.harvard.edu/publications/recommendations-trade-adjustment-assistance-sri-lanka\": 2017,\n",
    "            \"https://growthlab.hks.harvard.edu/publications/immigration-policy-research\": 2017,\n",
    "            \"https://growthlab.hks.harvard.edu/publications/sri-lanka%E2%80%99s-edible-oils-exports\": 2016,\n",
    "            \"https://growthlab.hks.harvard.edu/publications/economic-complexity-brief\": 2013,\n",
    "            \"https://growthlab.hks.harvard.edu/publications/journey-through-time-story-behind-%E2%80%98eight-decades-changes-occupational-tasks\": 2024,\n",
    "        }\n",
    "    \n",
    "    async def get_max_page_num(self, session):\n",
    "        \"\"\"Get the maximum page number from pagination\"\"\"\n",
    "        self.monitor.record_page_processed() \n",
    "        \n",
    "        async with session.get(self.base_url) as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Find pagination container\n",
    "            pagination, pagination_selector = find_with_selectors(\n",
    "                soup, \n",
    "                SELECTOR_CONFIG[\"pagination\"][\"container\"],\n",
    "                self.monitor, \"pagination\", \"container\"\n",
    "            )\n",
    "            \n",
    "            if not pagination:\n",
    "                logger.warning(\"No pagination container found\")\n",
    "                return 0\n",
    "            \n",
    "            # Find last page link\n",
    "            last_page, last_page_selector = find_with_selectors(\n",
    "                pagination, \n",
    "                SELECTOR_CONFIG[\"pagination\"][\"last_page\"],\n",
    "                self.monitor, \"pagination\", \"last_page\"\n",
    "            )\n",
    "            \n",
    "            if not last_page or not last_page.find(\"a\"):\n",
    "                logger.warning(\"No last page link found\")\n",
    "                return 0\n",
    "            \n",
    "            # Extract page number from URL\n",
    "            last_page_url = last_page.find(\"a\")[\"href\"]\n",
    "            match = re.search(r\"page=(\\d+)\", last_page_url)\n",
    "            \n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "            else:\n",
    "                return 0\n",
    "    \n",
    "    async def fetch_page(self, session, page_num):\n",
    "        \"\"\"Fetch a single page of publications\"\"\"\n",
    "        self.monitor.record_page_processed()\n",
    "        \n",
    "        url = self.base_url if page_num == 0 else f\"{self.base_url}?page={page_num}\"\n",
    "        logger.info(f\"Fetching page {page_num}: {url}\")\n",
    "        \n",
    "        # Use the semaphore to limit concurrency\n",
    "        async with self.semaphore:\n",
    "            async with session.get(url) as response:\n",
    "                if response.status != 200:\n",
    "                    logger.error(f\"Error fetching page {page_num}: {response.status}\")\n",
    "                    return []\n",
    "                \n",
    "                html = await response.text()\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                \n",
    "                # Find all publication containers\n",
    "                containers, container_selector = find_all_with_selectors(\n",
    "                    soup, \n",
    "                    SELECTOR_CONFIG[\"publication\"][\"container\"],\n",
    "                    self.monitor, \"publication\", \"container\"\n",
    "                )\n",
    "                \n",
    "                if not containers:\n",
    "                    logger.warning(f\"No publications found on page {page_num}\")\n",
    "                    return []\n",
    "                \n",
    "                logger.info(f\"Found {len(containers)} publications on page {page_num}\")\n",
    "                \n",
    "                # Parse each publication\n",
    "                publications = []\n",
    "                for container in containers:\n",
    "                    publication = parse_publication(\n",
    "                        container, \n",
    "                        self.base_url.split(\"/publications\")[0],\n",
    "                        self.monitor,\n",
    "                        self.year_corrections\n",
    "                    )\n",
    "                    publications.append(publication)\n",
    "                \n",
    "                # Add a delay to prevent overwhelming the server\n",
    "                await asyncio.sleep(self.scrape_delay)\n",
    "                \n",
    "                return publications\n",
    "    \n",
    "    async def extract_publications(self, max_pages=None):\n",
    "        \"\"\"Extract publications from all pages or a specified number of pages\"\"\"\n",
    "        # Set up a robust session\n",
    "        timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_connect=10, sock_read=10)\n",
    "        \n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        }\n",
    "        \n",
    "        async with aiohttp.ClientSession(headers=headers, timeout=timeout) as session:\n",
    "            # Get the maximum page number\n",
    "            max_page = await self.get_max_page_num(session)\n",
    "            logger.info(f\"Maximum page number: {max_page}\")\n",
    "            \n",
    "            # Limit the number of pages if specified\n",
    "            if max_pages is not None:\n",
    "                max_page = min(max_page, max_pages - 1)  # -1 because we start from page 0\n",
    "            \n",
    "            # Create tasks for fetching pages\n",
    "            tasks = [self.fetch_page(session, page) for page in range(max_page + 1)]\n",
    "            \n",
    "            # Process all pages and collect publications\n",
    "            all_publications = []\n",
    "            for future in asyncio.as_completed(tasks):\n",
    "                try:\n",
    "                    publications = await future\n",
    "                    all_publications.extend(publications)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing page: {e}\")\n",
    "            \n",
    "            logger.info(f\"Total publications extracted: {len(all_publications)}\")\n",
    "            return all_publications\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a report on the scraping results\"\"\"\n",
    "        self.monitor.generate_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc8510c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running comprehensive test across 3 pages\n",
      "🧪 Testing scraper across 3 pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Maximum page number: 21\n",
      "INFO:__main__:Fetching page 0: https://growthlab.hks.harvard.edu/publications\n",
      "INFO:__main__:Fetching page 2: https://growthlab.hks.harvard.edu/publications?page=2\n",
      "INFO:__main__:Fetching page 1: https://growthlab.hks.harvard.edu/publications?page=1\n",
      "INFO:__main__:Found 20 publications on page 0\n",
      "INFO:__main__:Found 20 publications on page 2\n",
      "INFO:__main__:Found 20 publications on page 1\n",
      "ERROR:__main__:Error processing page: can only concatenate str (not \"NoneType\") to str\n",
      "INFO:__main__:Total publications extracted: 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Publication Statistics (Total: 40)\n",
      "  - Missing titles: 0 (0.0%)\n",
      "  - Missing authors: 0 (0.0%)\n",
      "  - Missing year: 0 (0.0%)\n",
      "  - Missing abstract: 0 (0.0%)\n",
      "  - Missing file URLs: 0 (0.0%)\n",
      "\n",
      "✅ Sample of Complete Publications:\n",
      "\n",
      "1. Global Networks, Monetary Policy and Trade\n",
      "   Authors: Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\n",
      "   Year: 2025\n",
      "   Abstract: We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n",
      "   URL: https://growthlab.hks.harvard.edu/publications/global-networks-monetary-policy-and-trade\n",
      "   File URLs: ['https://www.nber.org/system/files/working_papers/w33686/w33686.pdf']\n",
      "\n",
      "2. Industrial policy for competitiveness in the energy transition\n",
      "   Authors: Ahuja, K. & Hausmann, R.\n",
      "   Year: 2025\n",
      "   Abstract: Green objectives have reshaped public policy worldwide since the signing in 2015 of the Paris Agreem...\n",
      "   URL: https://growthlab.hks.harvard.edu/publications/industrial-policy-competitiveness-energy-transition\n",
      "   File URLs: ['https://growthlab.hks.harvard.edu/sites/projects.iq.harvard.edu/files/bruegel_blueprint_34_0.pdf']\n",
      "\n",
      "3. Public-Private Dialogs to Spur Export-led Growth: The Case of Productivity Taskforces in Namibia\n",
      "   Authors: Fortunato, A. & Santos, M.A.\n",
      "   Year: 2025\n",
      "   Abstract: This case study examines the implementation of Namibia's first Productivity Task Force focused on th...\n",
      "   URL: https://growthlab.hks.harvard.edu/publications/public-private-dialogs-spur-export-led-growth-case-productivity-taskforces\n",
      "   File URLs: ['https://growthlab.hks.harvard.edu/sites/projects.iq.harvard.edu/files/public-private-dialogs-to-spur-export-led-growth.pdf']\n",
      "\n",
      "⚠️ Sample of Publications with Missing Fields:\n",
      "\n",
      "📋 Selector Performance Report\n",
      "\n",
      "📊 Selector Performance Report\n",
      "Pages processed: 4\n",
      "Publications processed: 47\n",
      "Overall selector success rate: 98.45%\n",
      "\n",
      "Selector Performance:\n",
      "  - pagination.container: ✅ GOOD 100.00% success (1/1)\n",
      "  - pagination.last_page: ✅ GOOD 100.00% success (1/1)\n",
      "  - publication.abstract: ✅ GOOD 97.87% success (46/47)\n",
      "  - publication.authors: ✅ GOOD 97.87% success (46/47)\n",
      "  - publication.citation: ❌ FAILING 0.00% success (0/1)\n",
      "  - publication.container: ✅ GOOD 100.00% success (3/3)\n",
      "  - publication.file: ✅ GOOD 100.00% success (47/47)\n",
      "  - publication.title: ✅ GOOD 100.00% success (47/47)\n",
      "\n",
      "📊 Selector Usage Statistics:\n",
      "\n",
      "pagination.container:\n",
      "  - ul.pager: 1 times (100.0%)\n",
      "\n",
      "pagination.last_page:\n",
      "  - li.pager-last: 1 times (100.0%)\n",
      "\n",
      "publication.container:\n",
      "  - div.biblio-entry: 3 times (100.0%)\n",
      "\n",
      "publication.title:\n",
      "  - span.biblio-title: 47 times (100.0%)\n",
      "\n",
      "publication.authors:\n",
      "  - span.biblio-authors: 46 times (100.0%)\n",
      "\n",
      "publication.abstract:\n",
      "  - div.biblio-abstract-display: 46 times (100.0%)\n",
      "\n",
      "publication.file:\n",
      "  - span.file: 40 times (85.1%)\n",
      "  - .Z3988+ a: 7 times (14.9%)\n",
      "\n",
      "🧪 Testing critical selectors individually\n",
      "🧪 Testing publication.title selector across 3 pages\n",
      "Testing page 0: https://growthlab.hks.harvard.edu/publications\n",
      "Found 20 containers on page 0\n",
      "✅ Found publication.title for 'Global Networks, Monetary Poli...' using span.biblio-title\n",
      "✅ Found publication.title for 'Industrial policy for competit...' using span.biblio-title\n",
      "✅ Found publication.title for 'Public-Private Dialogs to Spur...' using span.biblio-title\n",
      "✅ Found publication.title for 'De Facto Openness to Immigrati...' using span.biblio-title\n",
      "✅ Found publication.title for 'Serving From Hermosillo: Oppor...' using span.biblio-title\n",
      "✅ Found publication.title for 'Una Estrategia de Crecimiento ...' using span.biblio-title\n",
      "✅ Found publication.title for 'Nearshoring in Hermosillo: Ana...' using span.biblio-title\n",
      "✅ Found publication.title for 'Green Growth Opportunities for...' using span.biblio-title\n",
      "✅ Found publication.title for 'Green Growth Opportunities for...' using span.biblio-title\n",
      "✅ Found publication.title for 'Growth Through Diversification...' using span.biblio-title\n",
      "✅ Found publication.title for 'Childcare Supply in Wyoming...' using span.biblio-title\n",
      "✅ Found publication.title for 'A New Algorithm to Efficiently...' using span.biblio-title\n",
      "✅ Found publication.title for 'How Wyoming’s Exodus of Young ...' using span.biblio-title\n",
      "✅ Found publication.title for 'Escaping from Hardship, Search...' using span.biblio-title\n",
      "✅ Found publication.title for 'Leaving Home: Cumulative Clima...' using span.biblio-title\n",
      "✅ Found publication.title for 'Global Trends in Innovation Pa...' using span.biblio-title\n",
      "✅ Found publication.title for 'Innovation Policies Under Econ...' using span.biblio-title\n",
      "✅ Found publication.title for 'GLocal: A global development d...' using span.biblio-title\n",
      "✅ Found publication.title for 'Diagnosing Wyoming's Workforce...' using span.biblio-title\n",
      "✅ Found publication.title for 'From Products to Capabilities:...' using span.biblio-title\n",
      "Testing page 1: https://growthlab.hks.harvard.edu/publications?page=1\n",
      "Found 20 containers on page 1\n",
      "✅ Found publication.title for 'Export-led Growth...' using span.biblio-title\n",
      "✅ Found publication.title for 'Supply-Side Economics of a Goo...' using span.biblio-title\n",
      "✅ Found publication.title for 'How would dollarisation affect...' using span.biblio-title\n",
      "✅ Found publication.title for 'Harnessing Global Value Chains...' using span.biblio-title\n",
      "✅ Found publication.title for 'What Economic Challenges Does ...' using span.biblio-title\n",
      "✅ Found publication.title for 'Japan's Economic Puzzle...' using span.biblio-title\n",
      "✅ Found publication.title for 'A journey through time: the st...' using span.biblio-title\n",
      "✅ Found publication.title for 'Women Seeking Jobs with Limite...' using span.biblio-title\n",
      "✅ Found publication.title for 'On the Design of Effective San...' using span.biblio-title\n",
      "✅ Found publication.title for 'Grants in Wyoming: Constraints...' using span.biblio-title\n",
      "✅ Found publication.title for 'Crime, inequality and subsidiz...' using span.biblio-title\n",
      "✅ Found publication.title for 'The Missing Economic Diversity...' using span.biblio-title\n",
      "✅ Found publication.title for 'Pandemic-era Inflation Drivers...' using span.biblio-title\n",
      "✅ Found publication.title for 'Towards a Sustainable Recovery...' using span.biblio-title\n",
      "✅ Found publication.title for 'Growth Through Inclusion in So...' using span.biblio-title\n",
      "✅ Found publication.title for 'COVID-19 and emerging markets:...' using span.biblio-title\n",
      "✅ Found publication.title for 'Pretending to be the Law: Viol...' using span.biblio-title\n",
      "✅ Found publication.title for 'Food for Growth: A Diagnostics...' using span.biblio-title\n",
      "✅ Found publication.title for 'Mental health concerns precede...' using span.biblio-title\n",
      "✅ Found publication.title for 'Economic Costs of Friend-shori...' using span.biblio-title\n",
      "Testing page 2: https://growthlab.hks.harvard.edu/publications?page=2\n",
      "Found 20 containers on page 2\n",
      "✅ Found publication.title for 'Housing in Wyoming: Constraint...' using span.biblio-title\n",
      "✅ Found publication.title for 'Gravity with History: On Incum...' using span.biblio-title\n",
      "✅ Found publication.title for 'A more globally-minded Europea...' using span.biblio-title\n",
      "✅ Found publication.title for 'Growth Diagnostics and Competi...' using span.biblio-title\n",
      "✅ Found publication.title for 'Eight Decades of Changes in Oc...' using span.biblio-title\n",
      "✅ Found publication.title for 'The Impact of a Rise in Expect...' using span.biblio-title\n",
      "✅ Found publication.title for 'What is South Africa’s Crop Pr...' using span.biblio-title\n",
      "✅ Found publication.title for 'Scaling Partnerships to Activa...' using span.biblio-title\n",
      "✅ Found publication.title for 'Una historia de la economía de...' using span.biblio-title\n",
      "✅ Found publication.title for 'The Connectivity Trap: Stuck b...' using span.biblio-title\n",
      "✅ Found publication.title for 'The impact of return migration...' using span.biblio-title\n",
      "✅ Found publication.title for 'La Diversidad Económica Faltan...' using span.biblio-title\n",
      "✅ Found publication.title for 'La trampa de conectividad: cóm...' using span.biblio-title\n",
      "✅ Found publication.title for 'Mirar el bosque más allá de su...' using span.biblio-title\n",
      "✅ Found publication.title for 'A Growth Perspective on Wyomin...' using span.biblio-title\n",
      "✅ Found publication.title for 'Seeing the Forest for More tha...' using span.biblio-title\n",
      "✅ Found publication.title for 'The Economic Tale of Two Amazo...' using span.biblio-title\n",
      "✅ Found publication.title for 'Evaluating the Principle of Re...' using span.biblio-title\n",
      "✅ Found publication.title for 'Looking for Virtue in Remotene...' using span.biblio-title\n",
      "✅ Found publication.title for 'Innovation on Wings: Nonstop F...' using span.biblio-title\n",
      "\n",
      "📊 Results for publication.title:\n",
      "  - Success rate: 100.0% (60/60)\n",
      "  - Selectors used:\n",
      "    - span.biblio-title: 60 times (100.0%)\n",
      "🧪 Testing publication.authors selector across 3 pages\n",
      "Testing page 0: https://growthlab.hks.harvard.edu/publications\n",
      "Found 20 containers on page 0\n",
      "✅ Found publication.authors for 'Global Networks, Monetary Poli...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Industrial policy for competit...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Public-Private Dialogs to Spur...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'De Facto Openness to Immigrati...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Serving From Hermosillo: Oppor...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Una Estrategia de Crecimiento ...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Nearshoring in Hermosillo: Ana...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Green Growth Opportunities for...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Green Growth Opportunities for...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Growth Through Diversification...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Childcare Supply in Wyoming...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'A New Algorithm to Efficiently...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'How Wyoming’s Exodus of Young ...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Escaping from Hardship, Search...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Leaving Home: Cumulative Clima...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Global Trends in Innovation Pa...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Innovation Policies Under Econ...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'GLocal: A global development d...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Diagnosing Wyoming's Workforce...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'From Products to Capabilities:...' using span.biblio-authors\n",
      "Testing page 1: https://growthlab.hks.harvard.edu/publications?page=1\n",
      "Found 20 containers on page 1\n",
      "✅ Found publication.authors for 'Export-led Growth...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Supply-Side Economics of a Goo...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'How would dollarisation affect...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Harnessing Global Value Chains...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'What Economic Challenges Does ...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Japan's Economic Puzzle...' using span.biblio-authors\n",
      "❌ Failed to find publication.authors for 'A journey through time: the st...'\n",
      "✅ Found publication.authors for 'Women Seeking Jobs with Limite...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'On the Design of Effective San...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Grants in Wyoming: Constraints...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Crime, inequality and subsidiz...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'The Missing Economic Diversity...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Pandemic-era Inflation Drivers...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Towards a Sustainable Recovery...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Growth Through Inclusion in So...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'COVID-19 and emerging markets:...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Pretending to be the Law: Viol...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Food for Growth: A Diagnostics...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Mental health concerns precede...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Economic Costs of Friend-shori...' using span.biblio-authors\n",
      "Testing page 2: https://growthlab.hks.harvard.edu/publications?page=2\n",
      "Found 20 containers on page 2\n",
      "✅ Found publication.authors for 'Housing in Wyoming: Constraint...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Gravity with History: On Incum...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'A more globally-minded Europea...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Growth Diagnostics and Competi...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Eight Decades of Changes in Oc...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'The Impact of a Rise in Expect...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'What is South Africa’s Crop Pr...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Scaling Partnerships to Activa...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Una historia de la economía de...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'The Connectivity Trap: Stuck b...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'The impact of return migration...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'La Diversidad Económica Faltan...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'La trampa de conectividad: cóm...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Mirar el bosque más allá de su...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'A Growth Perspective on Wyomin...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Seeing the Forest for More tha...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'The Economic Tale of Two Amazo...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Evaluating the Principle of Re...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Looking for Virtue in Remotene...' using span.biblio-authors\n",
      "✅ Found publication.authors for 'Innovation on Wings: Nonstop F...' using span.biblio-authors\n",
      "\n",
      "📊 Results for publication.authors:\n",
      "  - Success rate: 98.3% (59/60)\n",
      "  - Selectors used:\n",
      "    - span.biblio-authors: 59 times (100.0%)\n",
      "🧪 Testing publication.file selector across 3 pages\n",
      "Testing page 0: https://growthlab.hks.harvard.edu/publications\n",
      "Found 20 containers on page 0\n",
      "✅ Found publication.file for 'Global Networks, Monetary Poli...' using .Z3988+ a\n",
      "✅ Found publication.file for 'Industrial policy for competit...' using span.file\n",
      "✅ Found publication.file for 'Public-Private Dialogs to Spur...' using span.file\n",
      "✅ Found publication.file for 'De Facto Openness to Immigrati...' using span.file\n",
      "✅ Found publication.file for 'Serving From Hermosillo: Oppor...' using span.file\n",
      "✅ Found publication.file for 'Una Estrategia de Crecimiento ...' using span.file\n",
      "✅ Found publication.file for 'Nearshoring in Hermosillo: Ana...' using span.file\n",
      "✅ Found publication.file for 'Green Growth Opportunities for...' using span.file\n",
      "✅ Found publication.file for 'Green Growth Opportunities for...' using span.file\n",
      "✅ Found publication.file for 'Growth Through Diversification...' using span.file\n",
      "✅ Found publication.file for 'Childcare Supply in Wyoming...' using span.file\n",
      "✅ Found publication.file for 'A New Algorithm to Efficiently...' using span.file\n",
      "✅ Found publication.file for 'How Wyoming’s Exodus of Young ...' using span.file\n",
      "✅ Found publication.file for 'Escaping from Hardship, Search...' using span.file\n",
      "✅ Found publication.file for 'Leaving Home: Cumulative Clima...' using .Z3988+ a\n",
      "✅ Found publication.file for 'Global Trends in Innovation Pa...' using span.file\n",
      "✅ Found publication.file for 'Innovation Policies Under Econ...' using span.file\n",
      "✅ Found publication.file for 'GLocal: A global development d...' using .Z3988+ a\n",
      "✅ Found publication.file for 'Diagnosing Wyoming's Workforce...' using span.file\n",
      "✅ Found publication.file for 'From Products to Capabilities:...' using span.file\n",
      "Testing page 1: https://growthlab.hks.harvard.edu/publications?page=1\n",
      "Found 20 containers on page 1\n",
      "✅ Found publication.file for 'Export-led Growth...' using span.file\n",
      "✅ Found publication.file for 'Supply-Side Economics of a Goo...' using span.file\n",
      "✅ Found publication.file for 'How would dollarisation affect...' using .Z3988+ a\n",
      "✅ Found publication.file for 'Harnessing Global Value Chains...' using span.file\n",
      "✅ Found publication.file for 'What Economic Challenges Does ...' using .Z3988+ a\n",
      "✅ Found publication.file for 'Japan's Economic Puzzle...' using span.file\n",
      "✅ Found publication.file for 'A journey through time: the st...' using .Z3988+ a\n",
      "✅ Found publication.file for 'Women Seeking Jobs with Limite...' using span.file\n",
      "✅ Found publication.file for 'On the Design of Effective San...' using .Z3988+ a\n",
      "✅ Found publication.file for 'Grants in Wyoming: Constraints...' using span.file\n",
      "✅ Found publication.file for 'Crime, inequality and subsidiz...' using .Z3988+ a\n",
      "✅ Found publication.file for 'The Missing Economic Diversity...' using span.file\n",
      "✅ Found publication.file for 'Pandemic-era Inflation Drivers...' using span.file\n",
      "✅ Found publication.file for 'Towards a Sustainable Recovery...' using span.file\n",
      "✅ Found publication.file for 'Growth Through Inclusion in So...' using span.file\n",
      "✅ Found publication.file for 'COVID-19 and emerging markets:...' using .Z3988+ a\n",
      "✅ Found publication.file for 'Pretending to be the Law: Viol...' using span.file\n",
      "✅ Found publication.file for 'Food for Growth: A Diagnostics...' using span.file\n",
      "✅ Found publication.file for 'Mental health concerns precede...' using .Z3988+ a\n",
      "✅ Found publication.file for 'Economic Costs of Friend-shori...' using .Z3988+ a\n",
      "Testing page 2: https://growthlab.hks.harvard.edu/publications?page=2\n",
      "Found 20 containers on page 2\n",
      "✅ Found publication.file for 'Housing in Wyoming: Constraint...' using span.file\n",
      "✅ Found publication.file for 'Gravity with History: On Incum...' using span.file\n",
      "✅ Found publication.file for 'A more globally-minded Europea...' using span.file\n",
      "✅ Found publication.file for 'Growth Diagnostics and Competi...' using span.file\n",
      "✅ Found publication.file for 'Eight Decades of Changes in Oc...' using span.file\n",
      "✅ Found publication.file for 'The Impact of a Rise in Expect...' using span.file\n",
      "✅ Found publication.file for 'What is South Africa’s Crop Pr...' using span.file\n",
      "✅ Found publication.file for 'Scaling Partnerships to Activa...' using span.file\n",
      "✅ Found publication.file for 'Una historia de la economía de...' using span.file\n",
      "✅ Found publication.file for 'The Connectivity Trap: Stuck b...' using span.file\n",
      "✅ Found publication.file for 'The impact of return migration...' using .Z3988+ a\n",
      "✅ Found publication.file for 'La Diversidad Económica Faltan...' using span.file\n",
      "✅ Found publication.file for 'La trampa de conectividad: cóm...' using span.file\n",
      "✅ Found publication.file for 'Mirar el bosque más allá de su...' using span.file\n",
      "✅ Found publication.file for 'A Growth Perspective on Wyomin...' using span.file\n",
      "✅ Found publication.file for 'Seeing the Forest for More tha...' using span.file\n",
      "✅ Found publication.file for 'The Economic Tale of Two Amazo...' using span.file\n",
      "✅ Found publication.file for 'Evaluating the Principle of Re...' using span.file\n",
      "✅ Found publication.file for 'Looking for Virtue in Remotene...' using span.file\n",
      "✅ Found publication.file for 'Innovation on Wings: Nonstop F...' using span.file\n",
      "\n",
      "📊 Results for publication.file:\n",
      "  - Success rate: 100.0% (60/60)\n",
      "  - Selectors used:\n",
      "    - span.file: 48 times (80.0%)\n",
      "    - .Z3988+ a: 12 times (20.0%)\n",
      "\n",
      "🏆 Final Assessment\n",
      "Critical selectors performance:\n",
      "- Title: 100.0% success\n",
      "- Authors: 98.3% success\n",
      "- File URLs: 100.0% success\n",
      "\n",
      "📚 Sample Publications:\n",
      "\n",
      "1. Global Networks, Monetary Policy and Trade\n",
      "   Authors: Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\n",
      "   Year: 2025\n",
      "   Abstract: We develop a novel framework to study the interaction between monetary...\n",
      "\n",
      "2. Industrial policy for competitiveness in the energy transition\n",
      "   Authors: Ahuja, K. & Hausmann, R.\n",
      "   Year: 2025\n",
      "   Abstract: Green objectives have reshaped public policy worldwide since the signi...\n",
      "\n",
      "3. Public-Private Dialogs to Spur Export-led Growth: The Case of Productivity Taskforces in Namibia\n",
      "   Authors: Fortunato, A. & Santos, M.A.\n",
      "   Year: 2025\n",
      "   Abstract: This case study examines the implementation of Namibia's first Product...\n",
      "\n",
      "4. De Facto Openness to Immigration\n",
      "   Authors: Nedelkoska, L., et al.\n",
      "   Year: 2025\n",
      "   Abstract: Various factors influence why some countries are more open to immigrat...\n",
      "\n",
      "5. Serving From Hermosillo: Opportunities in Cross-Border Trade of Services\n",
      "   Authors: Arcay, G. & O'Brien, T.\n",
      "   Year: 2025\n",
      "   Abstract: Technological advances have increased the general tradability of servi...\n",
      "\n",
      "✅ Demo completed!\n"
     ]
    }
   ],
   "source": [
    "# Function to test the scraper across all pages\n",
    "async def test_scraper_across_pages(max_pages=None):\n",
    "    \"\"\"Run the scraper and generate a report across multiple pages\"\"\"\n",
    "    print(f\"🧪 Testing scraper across {'all' if max_pages is None else max_pages} pages\")\n",
    "    \n",
    "    # Create a scraper instance\n",
    "    scraper = GrowthLabScraperWithSelectors()\n",
    "    \n",
    "    # Extract publications\n",
    "    publications = await scraper.extract_publications(max_pages)\n",
    "    \n",
    "    # Count publications with missing fields\n",
    "    missing_title = sum(1 for pub in publications if not pub.get(\"title\"))\n",
    "    missing_authors = sum(1 for pub in publications if not pub.get(\"authors\"))\n",
    "    missing_year = sum(1 for pub in publications if not pub.get(\"year\"))\n",
    "    missing_abstract = sum(1 for pub in publications if not pub.get(\"abstract\"))\n",
    "    missing_file_urls = sum(1 for pub in publications if not pub.get(\"file_urls\") or len(pub.get(\"file_urls\", [])) == 0)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n📊 Publication Statistics (Total: {len(publications)})\")\n",
    "    print(f\"  - Missing titles: {missing_title} ({missing_title/len(publications)*100:.1f}%)\")\n",
    "    print(f\"  - Missing authors: {missing_authors} ({missing_authors/len(publications)*100:.1f}%)\")\n",
    "    print(f\"  - Missing year: {missing_year} ({missing_year/len(publications)*100:.1f}%)\")\n",
    "    print(f\"  - Missing abstract: {missing_abstract} ({missing_abstract/len(publications)*100:.1f}%)\")\n",
    "    print(f\"  - Missing file URLs: {missing_file_urls} ({missing_file_urls/len(publications)*100:.1f}%)\")\n",
    "    \n",
    "    # Sample of successful publications\n",
    "    print(\"\\n✅ Sample of Complete Publications:\")\n",
    "    complete_pubs = [p for p in publications if p.get(\"title\") and p.get(\"authors\") and p.get(\"year\") and p.get(\"abstract\")]\n",
    "    for i, pub in enumerate(complete_pubs[:3]):\n",
    "        print(f\"\\n{i+1}. {pub.get('title', 'No title')}\")\n",
    "        print(f\"   Authors: {pub.get('authors', 'No authors')}\")\n",
    "        print(f\"   Year: {pub.get('year', 'No year')}\")\n",
    "        print(f\"   Abstract: {pub.get('abstract', 'No abstract')[:100]}...\" if pub.get('abstract') else \"   No abstract\")\n",
    "        print(f\"   URL: {pub.get('pub_url', 'No URL')}\")\n",
    "        print(f\"   File URLs: {pub.get('file_urls', [])}\")\n",
    "    \n",
    "    # Sample of publications with missing fields\n",
    "    print(\"\\n⚠️ Sample of Publications with Missing Fields:\")\n",
    "    incomplete_pubs = [p for p in publications if not p.get(\"title\") or not p.get(\"authors\") or not p.get(\"year\") or not p.get(\"abstract\")]\n",
    "    for i, pub in enumerate(incomplete_pubs[:3]):\n",
    "        print(f\"\\n{i+1}. {pub.get('title', 'No title')}\")\n",
    "        print(f\"   Authors: {pub.get('authors', 'No authors')}\")\n",
    "        print(f\"   Year: {pub.get('year', 'No year')}\")\n",
    "        print(f\"   Abstract: {pub.get('abstract', 'No abstract')[:100]}...\" if pub.get('abstract') else \"   No abstract\")\n",
    "        print(f\"   URL: {pub.get('pub_url', 'No URL')}\")\n",
    "        print(f\"   Missing: \" + \", \".join([\n",
    "            \"title\" if not pub.get(\"title\") else \"\",\n",
    "            \"authors\" if not pub.get(\"authors\") else \"\",\n",
    "            \"year\" if not pub.get(\"year\") else \"\", \n",
    "            \"abstract\" if not pub.get(\"abstract\") else \"\"\n",
    "        ]).replace(\", ,\", \",\").strip(\", \"))\n",
    "    \n",
    "    # Generate selector monitoring report\n",
    "    print(\"\\n📋 Selector Performance Report\")\n",
    "    scraper.generate_report()\n",
    "    \n",
    "    return publications, scraper.monitor\n",
    "\n",
    "# Test functionality to check a specific selector on all pages\n",
    "async def test_specific_selector(selector_section, selector_name, max_pages=3):\n",
    "    \"\"\"Test a specific selector across multiple pages\"\"\"\n",
    "    print(f\"🧪 Testing {selector_section}.{selector_name} selector across {max_pages} pages\")\n",
    "    \n",
    "    # Set up session\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    }\n",
    "    \n",
    "    selector_config = SELECTOR_CONFIG[selector_section][selector_name]\n",
    "    base_url = \"https://growthlab.hks.harvard.edu\"\n",
    "    publications_url = f\"{base_url}/publications\"\n",
    "    \n",
    "    monitor = SelectorMonitor(SELECTOR_CONFIG)\n",
    "    results = {\n",
    "        \"success\": 0,\n",
    "        \"failure\": 0,\n",
    "        \"selectors_used\": {}\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        # First get max page number\n",
    "        async with session.get(publications_url) as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            pagination = soup.select(\"ul.pager\")\n",
    "            max_page = 0\n",
    "            \n",
    "            if pagination:\n",
    "                last_page = pagination[0].select(\"li.pager-last a\")\n",
    "                if last_page:\n",
    "                    last_page_url = last_page[0][\"href\"]\n",
    "                    match = re.search(r\"page=(\\d+)\", last_page_url)\n",
    "                    if match:\n",
    "                        max_page = int(match.group(1))\n",
    "        \n",
    "        # Limit to specified max_pages\n",
    "        max_page = min(max_page, max_pages - 1)\n",
    "        \n",
    "        # Test on multiple pages\n",
    "        for page in range(max_page + 1):\n",
    "            page_url = publications_url if page == 0 else f\"{publications_url}?page={page}\"\n",
    "            print(f\"Testing page {page}: {page_url}\")\n",
    "            \n",
    "            async with session.get(page_url) as response:\n",
    "                html = await response.text()\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                \n",
    "                # Find containers\n",
    "                containers = soup.select(SELECTOR_CONFIG[\"publication\"][\"container\"][\"primary\"])\n",
    "                \n",
    "                if not containers:\n",
    "                    print(f\"No publication containers found on page {page}\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Found {len(containers)} containers on page {page}\")\n",
    "                \n",
    "                # Test the specified selector on each container\n",
    "                for i, container in enumerate(containers):\n",
    "                    # Get title for context\n",
    "                    title_elem = container.select(\"span.biblio-title\")\n",
    "                    title = title_elem[0].text.strip() if title_elem else f\"Publication {i+1}\"\n",
    "                    \n",
    "                    # Try to find the element using our selector with fallbacks\n",
    "                    element, selector_used = find_with_selectors(\n",
    "                        container, \n",
    "                        selector_config,\n",
    "                        monitor, selector_section, selector_name\n",
    "                    )\n",
    "                    \n",
    "                    if element:\n",
    "                        results[\"success\"] += 1\n",
    "                        if selector_used:\n",
    "                            results[\"selectors_used\"][selector_used] = results[\"selectors_used\"].get(selector_used, 0) + 1\n",
    "                        print(f\"✅ Found {selector_section}.{selector_name} for '{title[:30]}...' using {selector_used}\")\n",
    "                    else:\n",
    "                        results[\"failure\"] += 1\n",
    "                        print(f\"❌ Failed to find {selector_section}.{selector_name} for '{title[:30]}...'\")\n",
    "            \n",
    "            # Add a small delay\n",
    "            await asyncio.sleep(1)\n",
    "    \n",
    "    # Print results\n",
    "    total = results[\"success\"] + results[\"failure\"]\n",
    "    success_rate = results[\"success\"] / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\n📊 Results for {selector_section}.{selector_name}:\")\n",
    "    print(f\"  - Success rate: {success_rate:.1%} ({results['success']}/{total})\")\n",
    "    \n",
    "    if results[\"selectors_used\"]:\n",
    "        print(\"  - Selectors used:\")\n",
    "        for selector, count in sorted(results[\"selectors_used\"].items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = count / results[\"success\"] * 100 if results[\"success\"] > 0 else 0\n",
    "            print(f\"    - {selector}: {count} times ({percentage:.1f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test multiple pages and get comprehensive results\n",
    "async def run_comprehensive_test(max_pages=5):\n",
    "    \"\"\"Run a comprehensive test of the scraper\"\"\"\n",
    "    print(f\"🔍 Running comprehensive test across {max_pages} pages\")\n",
    "    \n",
    "    # Test main scraper\n",
    "    publications, monitor = await test_scraper_across_pages(max_pages)\n",
    "    \n",
    "    # Test the most critical selectors specifically\n",
    "    print(\"\\n🧪 Testing critical selectors individually\")\n",
    "    \n",
    "    title_results = await test_specific_selector(\"publication\", \"title\", max_pages)\n",
    "    authors_results = await test_specific_selector(\"publication\", \"authors\", max_pages)\n",
    "    file_results = await test_specific_selector(\"publication\", \"file\", max_pages)\n",
    "    \n",
    "    # Print final assessment\n",
    "    print(\"\\n🏆 Final Assessment\")\n",
    "    print(\"Critical selectors performance:\")\n",
    "    print(f\"- Title: {title_results['success']/(title_results['success']+title_results['failure'])*100:.1f}% success\")\n",
    "    print(f\"- Authors: {authors_results['success']/(authors_results['success']+authors_results['failure'])*100:.1f}% success\")\n",
    "    print(f\"- File URLs: {file_results['success']/(file_results['success']+file_results['failure'])*100:.1f}% success\")\n",
    "    \n",
    "    return publications, monitor\n",
    "\n",
    "# Run a demo on a few pages\n",
    "async def run_demo():\n",
    "    \"\"\"Run a demo of the scraper on a few pages\"\"\"\n",
    "    # Test a few pages for demonstration\n",
    "    MAX_PAGES = 3  # Set the maximum number of pages to test\n",
    "    publications, monitor = await run_comprehensive_test(MAX_PAGES)\n",
    "    \n",
    "    # Print a sample of publications\n",
    "    print(\"\\n📚 Sample Publications:\")\n",
    "    for i, pub in enumerate(publications[:5]):\n",
    "        print(f\"\\n{i+1}. {pub.get('title', 'No title')}\")\n",
    "        print(f\"   Authors: {pub.get('authors', 'No authors')}\")\n",
    "        print(f\"   Year: {pub.get('year', 'No year')}\")\n",
    "        print(f\"   Abstract: {pub.get('abstract', 'No abstract')[:70]}...\" if pub.get('abstract') else \"   No abstract\")\n",
    "        \n",
    "    print(\"\\n✅ Demo completed!\")\n",
    "    return publications, monitor\n",
    "\n",
    "# Execute the demo\n",
    "publications, monitor = await run_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff3e1be",
   "metadata": {},
   "source": [
    "Second section is more on endnote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2a92569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching publication page: https://growthlab.hks.harvard.edu/publications/global-networks-monetary-policy-and-trade\n",
      "Found Endnote URL: https://growthlab.hks.harvard.edu/publications/export/tagged/1942811\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 5: Get Endnote URL for a Publication\n",
    "\n",
    "async def get_endnote_file_url(publication_url):\n",
    "    \"\"\"Find the Endnote file URL for a specific publication\"\"\"\n",
    "    if not publication_url:\n",
    "        print(\"No publication URL provided\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Fetching publication page: {publication_url}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(publication_url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching publication page: {response.status}\")\n",
    "                return None\n",
    "            \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            endnote_link = soup.find(\"li\", class_=\"biblio_tagged\")\n",
    "            \n",
    "            if not endnote_link or not endnote_link.find(\"a\"):\n",
    "                print(\"No Endnote link found on the page\")\n",
    "                return None\n",
    "            \n",
    "            endnote_url = endnote_link.find(\"a\")[\"href\"]\n",
    "            \n",
    "            # Ensure URL is absolute\n",
    "            if not endnote_url.startswith((\"http://\", \"https://\")):\n",
    "                endnote_url = f\"{scraper.base_url.split('/publications')[0]}{endnote_url}\"\n",
    "            \n",
    "            print(f\"Found Endnote URL: {endnote_url}\")\n",
    "            return endnote_url\n",
    "\n",
    "# Run the function if we have a publication\n",
    "if publication and publication['pub_url']:\n",
    "    endnote_url = await get_endnote_file_url(publication['pub_url'])\n",
    "else:\n",
    "    print(\"Cannot get Endnote URL - no publication URL available\")\n",
    "    endnote_url = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a7407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Fetching publications page to find a publication URL...\n",
      "Found publication URL: https://growthlab.hks.harvard.edu/publications/global-networks-monetary-policy-and-trade\n",
      "\n",
      "Step 2: Fetching publication page to find Endnote link...\n",
      "Found Endnote URL: https://growthlab.hks.harvard.edu/publications/export/tagged/1942811\n",
      "\n",
      "Step 3: Fetching Endnote file...\n",
      "Endnote file size: 1731 bytes\n",
      "\n",
      "Raw Endnote Content (first 500 chars):\n",
      "%0 Generic\n",
      "%D 2025\n",
      "%T Global Networks, Monetary Policy and Trade\n",
      "%A Ṣebnem Kalemli-Özcan\n",
      "%A Can Soylu\n",
      "%A Muhammed A. Yildirim\n",
      "%X We develop a novel framework to study the interaction between monetary policy and trade. Our New Keynesian open economy model incorporates international production networks, sectoral heterogeneity in price rigidities, and trade distortions. We decompose the general equilibrium response to trade shocks into distinct channels that account for demand shifts, policy ...\n",
      "\n",
      "Step 4: Parsing Endnote content...\n",
      "\n",
      "Processing Endnote lines:\n",
      "\n",
      "Line: %0 Generic\n",
      "Key: 0\n",
      "Value: Generic\n",
      "\n",
      "Line: %D 2025\n",
      "Key: D\n",
      "Value: 2025\n",
      "Set date: 2025\n",
      "\n",
      "Line: %T Global Networks, Monetary Policy and Trade\n",
      "Key: T\n",
      "Value: Global Networks, Monetary Policy and Trade\n",
      "Set title: Global Networks, Monetary Policy and Trade\n",
      "\n",
      "Line: %A Ṣebnem Kalemli-Özcan\n",
      "Key: A\n",
      "Value: Ṣebnem Kalemli-Özcan\n",
      "Added author: Ṣebnem Kalemli-Özcan\n",
      "\n",
      "Line: %A Can Soylu\n",
      "Key: A\n",
      "Value: Can Soylu\n",
      "Added author: Can Soylu\n",
      "\n",
      "Line: %A Muhammed A. Yildirim\n",
      "Key: A\n",
      "Value: Muhammed A. Yildirim\n",
      "Added author: Muhammed A. Yildirim\n",
      "\n",
      "Line: %X We develop a novel framework to study the interaction between monetary policy...\n",
      "Key: X\n",
      "Value (first 100 chars): We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n",
      "Abstract is plain text, no HTML parsing needed\n",
      "Set abstract (first 100 chars): We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n",
      "\n",
      "Line: %G eng\n",
      "Key: G\n",
      "Value: eng\n",
      "\n",
      "Line: %U https://www.nber.org/system/files/working_papers/w33686/w33686.pdf\n",
      "Key: U\n",
      "Value: https://www.nber.org/system/files/working_papers/w33686/w33686.pdf\n",
      "\n",
      "Final parsed record:\n",
      "- date: 2025\n",
      "- title: Global Networks, Monetary Policy and Trade\n",
      "- author: Ṣebnem Kalemli-Özcan, Can Soylu, Muhammed A. Yildirim\n",
      "- abstract: We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 6: Download and Parse Endnote Content\n",
    "\n",
    "async def download_and_parse_endnote(endnote_url):\n",
    "    \"\"\"Download and parse the content of an Endnote file\"\"\"\n",
    "    if not endnote_url:\n",
    "        print(\"No Endnote URL provided\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Downloading Endnote file from: {endnote_url}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/plain,application/octet-stream\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(endnote_url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error downloading Endnote file: {response.status}\")\n",
    "                return None\n",
    "            \n",
    "            content = await response.text()\n",
    "            print(f\"Downloaded Endnote file: {len(content)} bytes\")\n",
    "            \n",
    "            # Print a sample of the content\n",
    "            print(\"\\nEndnote file content (first 200 chars):\")\n",
    "            print(content[:200] + \"...\" if len(content) > 200 else content)\n",
    "            \n",
    "            # Parse the content (similar to parse_endnote_content method)\n",
    "            record = {}\n",
    "            lines = content.split(\"\\n\")\n",
    "            \n",
    "            print(\"\\nParsing Endnote content...\")\n",
    "            for line in lines[:10]:  # Just show first 10 lines\n",
    "                if line.startswith(\"%\"):\n",
    "                    print(f\"Processing line: {line}\")\n",
    "                    key = line[1]\n",
    "                    value = line[3:].strip() if len(line) > 3 else \"\"\n",
    "                    \n",
    "                    if key == \"A\":  # Author\n",
    "                        name_parts = value.split(\", \")\n",
    "                        if len(name_parts) == 2:\n",
    "                            value = f\"{name_parts[1]} {name_parts[0]}\"\n",
    "                        print(f\"- Author: {value}\")\n",
    "                        record[\"author\"] = record.get(\"author\", []) + [value]\n",
    "                    elif key == \"T\":  # Title\n",
    "                        print(f\"- Title: {value}\")\n",
    "                        record[\"title\"] = value\n",
    "                    elif key == \"D\":  # Date\n",
    "                        print(f\"- Date: {value}\")\n",
    "                        record[\"date\"] = value\n",
    "                    elif key == \"X\":  # Abstract\n",
    "                        print(f\"- Abstract: {value[:50]}...\" if len(value) > 50 else f\"- Abstract: {value}\")\n",
    "                        \n",
    "                        # Check if value contains HTML\n",
    "                        if \"<\" in value and \">\" in value:\n",
    "                            print(\"  Abstract contains HTML, parsing...\")\n",
    "                            soup = BeautifulSoup(value, \"html.parser\")\n",
    "                            for tag in soup.find_all([\"b\", \"strong\"]):\n",
    "                                tag.unwrap()\n",
    "                            \n",
    "                            abstract = \"\\n\".join(\n",
    "                                p.get_text(separator=\" \", strip=True)\n",
    "                                for p in soup.find_all(\"p\")\n",
    "                                if p.get_text(strip=True)\n",
    "                            )\n",
    "                            record[\"abstract\"] = abstract.strip()\n",
    "                        else:\n",
    "                            print(\"  Abstract is plain text, no HTML parsing needed\")\n",
    "                            record[\"abstract\"] = value\n",
    "            \n",
    "            # Join author list\n",
    "            if \"author\" in record:\n",
    "                record[\"author\"] = \", \".join(record[\"author\"])\n",
    "            \n",
    "            return record\n",
    "\n",
    "# Run the function if we have an Endnote URL\n",
    "if endnote_url:\n",
    "    endnote_data = await download_and_parse_endnote(endnote_url)\n",
    "    \n",
    "    if endnote_data:\n",
    "        print(\"\\nParsed Endnote data:\")\n",
    "        for key, value in endnote_data.items():\n",
    "            print(f\"- {key}: {value[:100]}...\" if len(value) > 100 else f\"- {key}: {value}\")\n",
    "else:\n",
    "    print(\"Cannot download Endnote - no URL available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4a555b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Endnote file from: https://growthlab.hks.harvard.edu/publications/export/tagged/1942811\n",
      "Downloaded Endnote file: 1731 bytes\n",
      "\n",
      "Endnote file content (first 200 chars):\n",
      "%0 Generic\n",
      "%D 2025\n",
      "%T Global Networks, Monetary Policy and Trade\n",
      "%A Ṣebnem Kalemli-Özcan\n",
      "%A Can Soylu\n",
      "%A Muhammed A. Yildirim\n",
      "%X We develop a novel framework to study the interaction between mon...\n",
      "\n",
      "Parsing Endnote content...\n",
      "Processing line: %0 Generic\n",
      "Processing line: %D 2025\n",
      "- Date: 2025\n",
      "Processing line: %T Global Networks, Monetary Policy and Trade\n",
      "- Title: Global Networks, Monetary Policy and Trade\n",
      "Processing line: %A Ṣebnem Kalemli-Özcan\n",
      "- Author: Ṣebnem Kalemli-Özcan\n",
      "Processing line: %A Can Soylu\n",
      "- Author: Can Soylu\n",
      "Processing line: %A Muhammed A. Yildirim\n",
      "- Author: Muhammed A. Yildirim\n",
      "Processing line: %X We develop a novel framework to study the interaction between monetary policy and trade. Our New Keynesian open economy model incorporates international production networks, sectoral heterogeneity in price rigidities, and trade distortions. We decompose the general equilibrium response to trade shocks into distinct channels that account for demand shifts, policy effects, exchange rate adjustments, expectations, price stickiness, and input–output linkages. Tariffs act simultaneously as demand and supply shocks, leading to endogenous fragmentation through changes in trade and production network linkages. We show that the net impact of tariffs on domestic inflation, output, employment, and the dollar depends on the endogenous monetary policy response in both the tariff-imposing and tariff-exposed countries, within a global general equilibrium framework. Our quantitative exercise replicates the observed effects of the 2018 tariffs on the U.S. economy and predicts a 1.6 pp decline in U.S. output, a 0.8 pp rise in inflation, and a 4.8% appreciation of the dollar in response to a retaliatory trade war linked to tariffs announced on “Liberation Day.” Tariff threats, even in the absence of actual implementation, are self-defeating— leading to a 4.1% appreciation of the dollar, 0.6% deflation, and a 0.7 pp decline in output, as agents re-optimize in anticipation of future distortions. Dollar appreciates less or even can depreciate under retaliation, tariff threats, and increased global uncertainty.\n",
      "- Abstract: We develop a novel framework to study the interact...\n",
      "  Abstract is plain text, no HTML parsing needed\n",
      "Processing line: %G eng\n",
      "Processing line: %U https://www.nber.org/system/files/working_papers/w33686/w33686.pdf\n",
      "\n",
      "Parsed Endnote data:\n",
      "- date: 2025\n",
      "- title: Global Networks, Monetary Policy and Trade\n",
      "- author: Ṣebnem Kalemli-Özcan, Can Soylu, Muhammed A. Yildirim\n",
      "- abstract: We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 6: Download and Parse Endnote Content\n",
    "\n",
    "async def download_and_parse_endnote(endnote_url):\n",
    "    \"\"\"Download and parse the content of an Endnote file\"\"\"\n",
    "    if not endnote_url:\n",
    "        print(\"No Endnote URL provided\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Downloading Endnote file from: {endnote_url}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "        \"Accept\": \"text/plain,application/octet-stream\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(endnote_url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error downloading Endnote file: {response.status}\")\n",
    "                return None\n",
    "            \n",
    "            content = await response.text()\n",
    "            print(f\"Downloaded Endnote file: {len(content)} bytes\")\n",
    "            \n",
    "            # Print a sample of the content\n",
    "            print(\"\\nEndnote file content (first 200 chars):\")\n",
    "            print(content[:200] + \"...\" if len(content) > 200 else content)\n",
    "            \n",
    "            # Parse the content (similar to parse_endnote_content method)\n",
    "            record = {}\n",
    "            lines = content.split(\"\\n\")\n",
    "            \n",
    "            print(\"\\nParsing Endnote content...\")\n",
    "            for line in lines[:10]:  # Just show first 10 lines\n",
    "                if line.startswith(\"%\"):\n",
    "                    print(f\"Processing line: {line}\")\n",
    "                    key = line[1]\n",
    "                    value = line[3:].strip() if len(line) > 3 else \"\"\n",
    "                    \n",
    "                    if key == \"A\":  # Author\n",
    "                        name_parts = value.split(\", \")\n",
    "                        if len(name_parts) == 2:\n",
    "                            value = f\"{name_parts[1]} {name_parts[0]}\"\n",
    "                        print(f\"- Author: {value}\")\n",
    "                        record[\"author\"] = record.get(\"author\", []) + [value]\n",
    "                    elif key == \"T\":  # Title\n",
    "                        print(f\"- Title: {value}\")\n",
    "                        record[\"title\"] = value\n",
    "                    elif key == \"D\":  # Date\n",
    "                        print(f\"- Date: {value}\")\n",
    "                        record[\"date\"] = value\n",
    "                    elif key == \"X\":  # Abstract\n",
    "                        print(f\"- Abstract: {value[:50]}...\" if len(value) > 50 else f\"- Abstract: {value}\")\n",
    "                        \n",
    "                        # Check if value contains HTML\n",
    "                        if \"<\" in value and \">\" in value:\n",
    "                            print(\"  Abstract contains HTML, parsing...\")\n",
    "                            soup = BeautifulSoup(value, \"html.parser\")\n",
    "                            for tag in soup.find_all([\"b\", \"strong\"]):\n",
    "                                tag.unwrap()\n",
    "                            \n",
    "                            abstract = \"\\n\".join(\n",
    "                                p.get_text(separator=\" \", strip=True)\n",
    "                                for p in soup.find_all(\"p\")\n",
    "                                if p.get_text(strip=True)\n",
    "                            )\n",
    "                            record[\"abstract\"] = abstract.strip()\n",
    "                        else:\n",
    "                            print(\"  Abstract is plain text, no HTML parsing needed\")\n",
    "                            record[\"abstract\"] = value\n",
    "            \n",
    "            # Join author list\n",
    "            if \"author\" in record:\n",
    "                record[\"author\"] = \", \".join(record[\"author\"])\n",
    "            \n",
    "            return record\n",
    "\n",
    "# Run the function if we have an Endnote URL\n",
    "if endnote_url:\n",
    "    endnote_data = await download_and_parse_endnote(endnote_url)\n",
    "    \n",
    "    if endnote_data:\n",
    "        print(\"\\nParsed Endnote data:\")\n",
    "        for key, value in endnote_data.items():\n",
    "            print(f\"- {key}: {value[:100]}...\" if len(value) > 100 else f\"- {key}: {value}\")\n",
    "else:\n",
    "    print(\"Cannot download Endnote - no URL available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2959217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original publication:\n",
      "Title: Global Networks, Monetary Policy and Trade\n",
      "Abstract: We develop a novel framework to study the interaction between monetary policy and trade. Our New Keynesian open economy model incorporates international production networks, sectoral heterogeneity in price rigidities, and trade distortions. We decompose the general equilibrium response to trade shocks into distinct channels that account for demand shifts, policy effects, exchange rate adjustments, expectations, price stickiness, and input–output linkages. Tariffs act simultaneously as demand and supply shocks, leading to endogenous fragmentation through changes in trade and production network linkages. We show that the net impact of tariffs on domestic inflation, output, employment, and the dollar depends on the endogenous monetary policy response in both the tariff-imposing and tariff-exposed countries, within a global general equilibrium framework. Our quantitative exercise replicates the observed effects of the 2018 tariffs on the U.S. economy and predicts a 1.6 pp decline in U.S. output, a 0.8 pp rise in inflation, and a 4.8% appreciation of the dollar in response to a retaliatory trade war linked to tariffs announced on “Liberation Day.” Tariff threats, even in the absence of actual implementation, are self-defeating— leading to a 4.1% appreciation of the dollar, 0.6% deflation, and a 0.7 pp decline in output, as agents re-optimize in anticipation of future distortions. Dollar appreciates less or even can depreciate under retaliation, tariff threats, and increased global uncertainty.\n",
      "Enriching publication: Global Networks, Monetary Policy and Trade\n",
      "- Updated abstract to: We develop a novel framework to study the interact...\n",
      "- Updated content hash: hash_8709\n",
      "\n",
      "Enriched publication:\n",
      "Title: Global Networks, Monetary Policy and Trade\n",
      "Abstract: We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 7: Enrich a Publication with Endnote Data\n",
    "\n",
    "async def enrich_publication_with_endnote(pub, endnote_data):\n",
    "    \"\"\"Update a publication with data from an Endnote file\"\"\"\n",
    "    if not pub or not endnote_data:\n",
    "        print(\"Missing publication or Endnote data\")\n",
    "        return pub\n",
    "    \n",
    "    print(f\"Enriching publication: {pub['title']}\")\n",
    "    \n",
    "    # Update publication with Endnote data if missing\n",
    "    if not pub['title'] and \"title\" in endnote_data:\n",
    "        pub['title'] = endnote_data[\"title\"]\n",
    "        print(f\"- Updated title to: {pub['title']}\")\n",
    "    \n",
    "    if not pub['authors'] and \"author\" in endnote_data:\n",
    "        pub['authors'] = endnote_data[\"author\"]\n",
    "        print(f\"- Updated authors to: {pub['authors']}\")\n",
    "    \n",
    "    if not pub['abstract'] and \"abstract\" in endnote_data:\n",
    "        pub['abstract'] = endnote_data[\"abstract\"]\n",
    "        print(f\"- Updated abstract to: {pub['abstract'][:50]}...\" if len(pub['abstract']) > 50 else f\"- Updated abstract to: {pub['abstract']}\")\n",
    "    \n",
    "    # Regenerate content hash after updates\n",
    "    pub.content_hash = pub.generate_content_hash()\n",
    "    print(f\"- Updated content hash: {pub['content_hash']}\")\n",
    "    \n",
    "    return pub\n",
    "\n",
    "# Run the function if we have both publication and Endnote data\n",
    "if publication and endnote_data:\n",
    "    # Create a copy to show the changes\n",
    "    original_pub = SimpleGrowthLabPublication(**publication)\n",
    "    \n",
    "    # Clear some fields to simulate missing data\n",
    "    publication['abstract'] = None\n",
    "    \n",
    "    print(\"\\nOriginal publication:\")\n",
    "    print(f\"Title: {original_pub['title']}\")\n",
    "    print(f\"Abstract: {original_pub['abstract']}\")\n",
    "    \n",
    "    enriched_pub = await enrich_publication_with_endnote(publication, endnote_data)\n",
    "    \n",
    "    print(\"\\nEnriched publication:\")\n",
    "    print(f\"Title: {enriched_pub['title']}\")\n",
    "    print(f\"Abstract: {enriched_pub['abstract'][:100]}...\" if enriched_pub['abstract'] else \"No abstract\")\n",
    "else:\n",
    "    print(\"Cannot enrich publication - missing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c11f387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing selector parser:\n",
      "div.biblio-entry -> tag: 'div', attrs: {'class': 'biblio-entry'}\n",
      "span.biblio-title -> tag: 'span', attrs: {'class': 'biblio-title'}\n",
      "li.biblio_tagged a -> tag: 'li', attrs: {'class': 'biblio_tagged a'}\n",
      "\n",
      "Publication fetched with configurable selectors:\n",
      "Title: Global Networks, Monetary Policy and Trade\n",
      "Authors: Kalemli-Özcan, Ṣ., Soylu, C. & Yildirim, M.A.\n",
      "Abstract: We develop a novel framework to study the interaction between monetary policy and trade. Our New Key...\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 8: Testing Improved CSS Selectors Configuration\n",
    "\n",
    "# Define selectors in a configuration dictionary\n",
    "selectors = {\n",
    "    \"publication\": {\n",
    "        \"container\": \"div.biblio-entry\",\n",
    "        \"title\": \"span.biblio-title\",\n",
    "        \"authors\": \"span.biblio-authors\",\n",
    "        \"abstract\": \"div.biblio-abstract-display\",\n",
    "        \"file\": \"span.file\",\n",
    "    },\n",
    "    \"endnote\": {\n",
    "        \"link\": \"li.biblio_tagged a\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Helper function to parse CSS selectors\n",
    "def get_selector_parts(selector):\n",
    "    \"\"\"Split a CSS selector into tag and attributes for BeautifulSoup\"\"\"\n",
    "    if \".\" in selector:\n",
    "        parts = selector.split(\".\")\n",
    "        tag = parts[0] or None\n",
    "        attrs = {\"class\": parts[1]} if len(parts) > 1 else {}\n",
    "        return tag, attrs\n",
    "    else:\n",
    "        return selector, {}\n",
    "\n",
    "# Test the selector parser\n",
    "test_selectors = [\n",
    "    \"div.biblio-entry\",\n",
    "    \"span.biblio-title\",\n",
    "    \"li.biblio_tagged a\"\n",
    "]\n",
    "\n",
    "print(\"Testing selector parser:\")\n",
    "for selector in test_selectors:\n",
    "    tag, attrs = get_selector_parts(selector)\n",
    "    print(f\"{selector} -> tag: '{tag}', attrs: {attrs}\")\n",
    "\n",
    "# Fetch a publication using configurable selectors\n",
    "async def fetch_with_configurable_selectors():\n",
    "    \"\"\"Fetch a publication using configurable selectors\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\",\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        async with session.get(scraper.base_url) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Error fetching page: {response.status}\")\n",
    "                return None\n",
    "            \n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Get container selector\n",
    "            container_selector = selectors[\"publication\"][\"container\"]\n",
    "            tag, attrs = get_selector_parts(container_selector)\n",
    "            pub_elements = soup.find_all(tag, attrs)\n",
    "            \n",
    "            if not pub_elements:\n",
    "                print(\"No publications found\")\n",
    "                return None\n",
    "            \n",
    "            # Get first publication\n",
    "            pub_element = pub_elements[0]\n",
    "            \n",
    "            # Extract using configurable selectors\n",
    "            title_selector = selectors[\"publication\"][\"title\"]\n",
    "            tag, attrs = get_selector_parts(title_selector)\n",
    "            title_element = pub_element.find(tag, attrs)\n",
    "            title = title_element.text.strip() if title_element else None\n",
    "            \n",
    "            authors_selector = selectors[\"publication\"][\"authors\"]\n",
    "            tag, attrs = get_selector_parts(authors_selector)\n",
    "            authors_element = pub_element.find(tag, attrs)\n",
    "            authors = authors_element.text.strip() if authors_element else None\n",
    "            \n",
    "            abstract_selector = selectors[\"publication\"][\"abstract\"]\n",
    "            tag, attrs = get_selector_parts(abstract_selector)\n",
    "            abstract_element = pub_element.find(tag, attrs)\n",
    "            abstract = abstract_element.text.strip() if abstract_element else None\n",
    "            \n",
    "            return {\n",
    "                \"title\": title,\n",
    "                \"authors\": authors,\n",
    "                \"abstract\": abstract\n",
    "            }\n",
    "\n",
    "# Run the function\n",
    "config_pub = await fetch_with_configurable_selectors()\n",
    "\n",
    "print(\"\\nPublication fetched with configurable selectors:\")\n",
    "if config_pub:\n",
    "    print(f\"Title: {config_pub['title']}\")\n",
    "    print(f\"Authors: {config_pub['authors']}\")\n",
    "    print(f\"Abstract: {config_pub['abstract'][:100]}...\" if config_pub['abstract'] else \"No abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c68c462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with valid Endnote content:\n",
      "\n",
      "Parsed valid Endnote:\n",
      "- author: John Smith, Jane Doe\n",
      "- title: Sample Article Title\n",
      "- date: 2023\n",
      "- abstract: This is a sample abstract with some formatting .\n",
      "\n",
      "Testing with malformed Endnote content:\n",
      "Malformed Endnote line: %ASmith, John\n",
      "\n",
      "Parsed malformed Endnote:\n",
      "- title: Missing space after %A\n",
      "- abstract: Unclosed tag problem\n",
      "- date: Invalid date\n",
      "\n",
      "Testing with empty Endnote content:\n",
      "Empty Endnote content received\n",
      "\n",
      "Parsed empty Endnote:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 9: Testing Improved Endnote Parser with Error Handling\n",
    "\n",
    "# Sample Endnote content with different formatting issues\n",
    "sample_endnote = \"\"\"%0 Journal Article\n",
    "%A Smith, John\n",
    "%A Doe, Jane\n",
    "%T Sample Article Title\n",
    "%D 2023\n",
    "%X <p>This is a <b>sample</b> abstract with some <strong>formatting</strong>.</p>\n",
    "%J Journal of Sample Studies\n",
    "\"\"\"\n",
    "\n",
    "malformed_endnote = \"\"\"%0 Journal Article\n",
    "%ASmith, John\n",
    "%T Missing space after %A\n",
    "%X <p>Unclosed tag <b>problem</p>\n",
    "%D Invalid date\n",
    "\"\"\"\n",
    "\n",
    "empty_endnote = \"\"\n",
    "\n",
    "async def improved_parse_endnote(content):\n",
    "    \"\"\"Parse Endnote file content with robust error handling\"\"\"\n",
    "    record = {}\n",
    "    \n",
    "    # Validate content format\n",
    "    if not content or not content.strip():\n",
    "        print(\"Empty Endnote content received\")\n",
    "        return record\n",
    "    \n",
    "    # Check for expected format\n",
    "    if not any(line.startswith(\"%\") for line in content.split(\"\\n\")):\n",
    "        print(\"Unexpected Endnote format - missing % line markers\")\n",
    "        return record\n",
    "    \n",
    "    lines = content.split(\"\\n\")\n",
    "    \n",
    "    try:\n",
    "        for line in lines:\n",
    "            if line.startswith(\"%\"):\n",
    "                # Validate line format\n",
    "                if len(line) < 3 or line[2] != ' ':\n",
    "                    print(f\"Malformed Endnote line: {line}\")\n",
    "                    continue\n",
    "                \n",
    "                key = line[1]\n",
    "                value = line[3:].strip()\n",
    "                \n",
    "                if key == \"A\":  # Author\n",
    "                    try:\n",
    "                        name_parts = value.split(\", \")\n",
    "                        if len(name_parts) == 2:\n",
    "                            value = f\"{name_parts[1]} {name_parts[0]}\"\n",
    "                        record[\"author\"] = record.get(\"author\", []) + [value]\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing author: {e}\")\n",
    "                        \n",
    "                elif key == \"T\":  # Title\n",
    "                    record[\"title\"] = value\n",
    "                elif key == \"D\":  # Date\n",
    "                    record[\"date\"] = value\n",
    "                elif key == \"X\":  # Abstract\n",
    "                    try:\n",
    "                        # Check if contains HTML\n",
    "                        if \"<\" in value and \">\" in value:\n",
    "                            soup = BeautifulSoup(value, \"html.parser\")\n",
    "                            # Remove formatting tags\n",
    "                            for tag in soup.find_all([\"b\", \"strong\"]):\n",
    "                                tag.unwrap()\n",
    "                                \n",
    "                            # Extract paragraphs\n",
    "                            paragraphs = soup.find_all(\"p\")\n",
    "                            if paragraphs:\n",
    "                                abstract = \"\\n\".join(\n",
    "                                    p.get_text(separator=\" \", strip=True)\n",
    "                                    for p in paragraphs\n",
    "                                    if p.get_text(strip=True)\n",
    "                                )\n",
    "                            else:\n",
    "                                # Fallback to full text\n",
    "                                abstract = soup.get_text(separator=\" \", strip=True)\n",
    "                                \n",
    "                            record[\"abstract\"] = abstract.strip()\n",
    "                        else:\n",
    "                            # Plain text\n",
    "                            record[\"abstract\"] = value\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing abstract: {e}\")\n",
    "                        # Fallback to raw text\n",
    "                        record[\"abstract\"] = value\n",
    "        \n",
    "        # Join author list\n",
    "        if \"author\" in record:\n",
    "            record[\"author\"] = \", \".join(record[\"author\"])\n",
    "            \n",
    "        return record\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing Endnote content: {e}\")\n",
    "        return record\n",
    "\n",
    "# Test with valid Endnote content\n",
    "print(\"Testing with valid Endnote content:\")\n",
    "valid_result = await improved_parse_endnote(sample_endnote)\n",
    "print(\"\\nParsed valid Endnote:\")\n",
    "for key, value in valid_result.items():\n",
    "    print(f\"- {key}: {value}\")\n",
    "\n",
    "# Test with malformed Endnote content\n",
    "print(\"\\nTesting with malformed Endnote content:\")\n",
    "malformed_result = await improved_parse_endnote(malformed_endnote)\n",
    "print(\"\\nParsed malformed Endnote:\")\n",
    "for key, value in malformed_result.items():\n",
    "    print(f\"- {key}: {value}\")\n",
    "\n",
    "# Test with empty Endnote content\n",
    "print(\"\\nTesting with empty Endnote content:\")\n",
    "empty_result = await improved_parse_endnote(empty_endnote)\n",
    "print(\"\\nParsed empty Endnote:\")\n",
    "print(empty_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepsearch)",
   "language": "python",
   "name": "deepsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
