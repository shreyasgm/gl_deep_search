# Growth Lab Deep Search

An agentic RAG system that helps users query Growth Lab-specific unstructured data.

## ğŸ” Project Overview

Growth Lab Deep Search is an agentic AI system designed to answer complex questions about the Growth Lab's research and publications. The system incorporates:

**Key Features:**

- Automated ETL pipeline for harvesting Growth Lab publications and academic papers
- Advanced OCR processing of PDF documents using modern tools
- Vector embeddings with hybrid search
- Agentic RAG system based on LangGraph

## Project Architecture

### Directory structure

This is a rough outline of the intended directory structure. The actual structure might look different, but this should give an idea of the intended code organization.

```
gl_deep_search/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ etl-pipeline.yml         # Scheduled ETL runs and deployment
â”‚       â”œâ”€â”€ service-deploy.yml       # Service API deployment
â”‚       â””â”€â”€ frontend-deploy.yml      # Frontend deployment
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml                   # Python project config for uv
â”œâ”€â”€ docker-compose.yml               # Local development setup
â”œâ”€â”€ docker-compose.prod.yml          # Production setup
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ Dockerfile                # ETL container configuration
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml        # Local development setup
â”‚   â”‚   â”œâ”€â”€ config.yaml               # Default configuration
â”‚   â”‚   â”œâ”€â”€ .env.example              # Environment variables template
â”‚   â”‚   â”œâ”€â”€ pyproject.toml            # Python dependencies (uv)
â”‚   â”‚   â”œâ”€â”€ main.py                   # ETL orchestration entry point
â”‚   â”‚   â”œâ”€â”€ models.py                 # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ config.py                 # Configuration management
â”‚   â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py               # Abstract scraper interface
â”‚   â”‚   â”‚   â”œâ”€â”€ growthlab.py          # Growth Lab website scraper
â”‚   â”‚   â”‚   â””â”€â”€ openalex.py           # OpenAlex API client
â”‚   â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf_processor.py      # PDF processing and OCR
â”‚   â”‚   â”‚   â””â”€â”€ manifest.py           # Manifest management
â”‚   â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py               # Storage abstraction
â”‚   â”‚   â”‚   â”œâ”€â”€ local.py              # Local filesystem adapter
â”‚   â”‚   â”‚   â””â”€â”€ gcs.py                # Google Cloud Storage adapter
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ id_utils.py           # ID generation utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ async_utils.py        # Async helpers & rate limiting
â”‚   â”‚   â”‚   â”œâ”€â”€ ocr_utils.py          # OCR interface
â”‚   â”‚   â”‚   â””â”€â”€ logger.py             # Logging configuration
â”‚   â”‚   â””â”€â”€ tests/                    # Unit and integration tests
â”‚   â”‚
â”‚   â”œâ”€â”€ service/                      # Main backend service (replaces "agent")
â”‚   â”‚   â”œâ”€â”€ Dockerfile                # Service Docker configuration
â”‚   â”‚   â”œâ”€â”€ .env.example              # Example environment variables
â”‚   â”‚   â”œâ”€â”€ main.py                   # FastAPI entry point
â”‚   â”‚   â”œâ”€â”€ routes.py                 # API endpoints
â”‚   â”‚   â”œâ”€â”€ models.py                 # Data models
â”‚   â”‚   â”œâ”€â”€ config.py                 # Service configuration
â”‚   â”‚   â”œâ”€â”€ graph.py                  # LangGraph definition
â”‚   â”‚   â”œâ”€â”€ tools.py                  # Service tools
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ retriever.py          # Vector retrieval
â”‚   â”‚       â””â”€â”€ logger.py             # Logging and observability
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/                      # Storage configuration
â”‚   â”‚   â”œâ”€â”€ qdrant_config.yaml        # Qdrant vector DB config
â”‚   â”‚   â””â”€â”€ metadata_schema.sql       # Metadata schema if needed
â”‚   â”‚
â”‚   â””â”€â”€ cloud/                        # Cloud deployment configs
â”‚       â”œâ”€â”€ etl-cloudrun.yaml         # ETL Cloud Run config
â”‚       â””â”€â”€ service-cloudrun.yaml     # Service Cloud Run config
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ Dockerfile                    # Frontend Docker configuration
â”‚   â”œâ”€â”€ .env.example                  # Example environment variables
â”‚   â”œâ”€â”€ app.py                        # Single Streamlit application file
â”‚   â””â”€â”€ utils.py                      # Frontend utility functions
â”‚
â””â”€â”€ scripts/                          # Utility scripts
    â”œâ”€â”€ setup.sh                      # Project setup
    â”œâ”€â”€ deploy.sh                     # Deployment to GCP
    â””â”€â”€ storage_switch.sh             # Script to switch between local/cloud storage
```



## Tech Stack

- **ETL Pipeline**: GitHub Actions, Modern OCR tools (Docling/Marker/Gemini Flash 2)
- **Vector Storage**: Qdrant for embeddings, with Cohere for reranking
- **Agent System**: LangGraph for agentic RAG workflows
- **Backend API**: FastAPI, Python 3.12+
- **Frontend**: Streamlit or Chainlit for MVP
- **Deployment**: Google Cloud Run
- **Package Management**: uv

## Getting Started

### Prerequisites

- Docker and Docker Compose
- Python 3.12+
- `uv` for project management (check documentarion [here](https://docs.astral.sh/uv/))
- GCP account and credentials (for production)
- API keys for OpenAI, Anthropic, etc.

### Environment Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/shreyasgm/gl_deep_search.git
   cd gl_deep_search
   ```
2. Run `uv` in the CLI to check that it is available. After this, run `uv sync` to install dependencies and create the virtual environment. This command will only install the core dependencies specified in the `pyproject.toml` file. To install dependencies that belong to a specific component (*i.e.*, optional dependencies) use:
   ```bash
   # For a single optional component
   uv sync --extra etl

   # For multiple optional components
   uv sync --extra etl, frontend, [other groups]
   ```

3. To add new packages to the project, use the following format:
   ```bash
   # Add a package to a specific group (etl, service, frontend, dev, prod)
   uv add package_name --optional group_name

   # Example: Add seaborn to the service group
   uv add seaborn --optional service
   ```

4. Create and configure environment files:
   ```bash
   cp backend/etl/.env.example backend/etl/.env
   cp backend/service/.env.example backend/service/.env
   cp frontend/.env.example frontend/.env
   ```

5. Add your API keys and configuration to the `.env` files

### Docker Development Environment

The project uses Docker for consistent development and deployment environments:

1. Start the complete development stack:
   ```bash
   docker-compose up
   ```

2. Access local services:
   - Frontend UI: http://localhost:8501
   - Backend API: http://localhost:8000
   - API Documentation: http://localhost:8000/docs

3. Run individual components:
   ```bash
   # Run only the ETL service
   docker-compose up etl

   # Run only the backend service
   docker-compose up service

   # Run only the frontend
   docker-compose up frontend
   ```

### Running the ETL Pipeline

The ETL pipeline supports both development and production environments through containerized deployment. Initial data ingestion and processing of historical documents is executed on a High-Performance Computing (HPC) infrastructure using SLURM workload manager. Incremental updates for new documents are handled through Google Cloud Run.

```bash
# Development: Execute ETL pipeline in local environment
docker-compose run --rm etl python main.py

# Production: Initial bulk processing via HPC/SLURM
sbatch scripts/slurm_etl_initial.sh

# Component-specific execution
docker-compose run --rm etl python main.py --component scraper
docker-compose run --rm etl python main.py --component processor
docker-compose run --rm etl python main.py --component embedder
```

Post-initial processing, data is migrated to Google Cloud Storage. Subsequent ETL operations are orchestrated through automated GitHub Actions workflows and executed on Google Cloud Run.

## Deployment

### Local to Production Workflow

1. Development occurs in local Docker environment
2. Code is pushed to GitHub
3. GitHub Actions triggers:
   - Code testing
   - Building and publishing container images
   - Deploying to Cloud Run

### Production Infrastructure

- **ETL Pipeline**: Scheduled Cloud Run jobs triggered by GitHub Actions
- **Backend Service**: Cloud Run with autoscaling
- **Vector Database**: Managed Qdrant instance or Qdrant Cloud
- **Document Storage**: Cloud Storage
- **Frontend**: Streamlit or Chainlit

### Deployment Commands

```bash
# Deploy to development environment
./scripts/deploy.sh dev

# Deploy to production environment
./scripts/deploy.sh prod
```

## ğŸ§ª Development Workflow

### Contributing Guidelines

1. Create a feature branch from `main`
2. Implement your changes with tests
3. Submit a pull request for review

### Testing

```bash
# Run tests
pytest

# Run with coverage
pytest --cov=backend
```

### Deployment

Development and production environments are managed through Docker and GitHub Actions:

```bash
# Deploy to development
./scripts/deploy.sh dev

# Deploy to production
./scripts/deploy.sh prod
```


## ğŸ”’ Security & Configuration

- API keys and secrets are managed via `.env` files (not committed to GitHub)
- Production secrets are stored in GCP Secret Manager
- Access control is implemented at the API level

## License

This project is licensed under CC-BY-NC-SA 4.0. See the LICENSE file for details.
