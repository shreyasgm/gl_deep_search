# Growth Lab Deep Search

An agentic RAG system that helps users query Growth Lab-specific unstructured data.

## ğŸ” Project Overview

Growth Lab Agent is an agentic AI system designed to answer complex questions about the Growth Lab's research and publications. The system incorporates:

**Key Features:**

- Automated ETL pipeline for harvesting Growth Lab publications and academic papers
- Advanced OCR processing of PDF documents using modern tools
- Vector embeddings with hybrid search
- Agentic RAG system based on LangGraph

## Project Architecture

### Directory structure

This is a rough outline of the intended directory structure. The actual structure might look different, but this should give an idea of the intended code organization.

```
growth-lab-agent/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ etl-pipeline.yml         # Scheduled ETL runs and deployment
â”‚       â”œâ”€â”€ service-deploy.yml       # Service API deployment
â”‚       â””â”€â”€ frontend-deploy.yml      # Frontend deployment
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml                   # Python project config for uv
â”œâ”€â”€ docker-compose.yml               # Local development setup
â”œâ”€â”€ docker-compose.prod.yml          # Production setup
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ etl/                         # ETL Pipeline
â”‚   â”‚   â”œâ”€â”€ Dockerfile               # ETL Docker configuration
â”‚   â”‚   â”œâ”€â”€ config.yaml              # ETL configuration
â”‚   â”‚   â”œâ”€â”€ .env.example             # Example environment variables
â”‚   â”‚   â”œâ”€â”€ main.py                  # Main ETL orchestration script
â”‚   â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”‚   â”œâ”€â”€ growth_lab_scraper.py # Growth Lab website scraper
â”‚   â”‚   â”‚   â”œâ”€â”€ openAlex_client.py    # OpenAlex API client
â”‚   â”‚   â”‚   â”œâ”€â”€ process_pdfs.py       # OCR and process PDFs
â”‚   â”‚   â”‚   â”œâ”€â”€ embed_text.py         # Generate embeddings
â”‚   â”‚   â”‚   â””â”€â”€ upload_data.py        # Store in vector DB
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ ocr_utils.py          # OCR with modern tools
â”‚   â”‚   â”‚   â”œâ”€â”€ text_utils.py         # Text processing and chunking
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding_utils.py    # Embedding generation
â”‚   â”‚   â”‚   â”œâ”€â”€ storage_utils.py      # Vector DB interactions
â”‚   â”‚   â”‚   â””â”€â”€ cloud_utils.py        # Utilities for cloud storage
â”‚   â”‚   â””â”€â”€ data/                     # Local data storage during development
â”‚   â”‚       â”œâ”€â”€ raw/                  # Raw downloaded files
â”‚   â”‚       â”œâ”€â”€ intermediate/         # Processed but not final data
â”‚   â”‚       â””â”€â”€ processed/            # Final processed data
â”‚   â”‚
â”‚   â”œâ”€â”€ service/                      # Main backend service (replaces "agent")
â”‚   â”‚   â”œâ”€â”€ Dockerfile                # Service Docker configuration
â”‚   â”‚   â”œâ”€â”€ .env.example              # Example environment variables
â”‚   â”‚   â”œâ”€â”€ main.py                   # FastAPI entry point
â”‚   â”‚   â”œâ”€â”€ routes.py                 # API endpoints
â”‚   â”‚   â”œâ”€â”€ models.py                 # Data models
â”‚   â”‚   â”œâ”€â”€ config.py                 # Service configuration
â”‚   â”‚   â”œâ”€â”€ graph.py                  # LangGraph definition
â”‚   â”‚   â”œâ”€â”€ tools.py                  # Service tools
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ retriever.py          # Vector retrieval
â”‚   â”‚       â””â”€â”€ logger.py             # Logging and observability
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/                      # Storage configuration
â”‚   â”‚   â”œâ”€â”€ qdrant_config.yaml        # Qdrant vector DB config
â”‚   â”‚   â””â”€â”€ metadata_schema.sql       # Metadata schema if needed
â”‚   â”‚
â”‚   â””â”€â”€ cloud/                        # Cloud deployment configs
â”‚       â”œâ”€â”€ etl-cloudrun.yaml         # ETL Cloud Run config
â”‚       â””â”€â”€ service-cloudrun.yaml     # Service Cloud Run config
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ Dockerfile                    # Frontend Docker configuration
â”‚   â”œâ”€â”€ .env.example                  # Example environment variables
â”‚   â”œâ”€â”€ app.py                        # Single Streamlit application file
â”‚   â””â”€â”€ utils.py                      # Frontend utility functions
â”‚
â””â”€â”€ scripts/                          # Utility scripts
    â”œâ”€â”€ setup.sh                      # Project setup
    â”œâ”€â”€ deploy.sh                     # Deployment to GCP
    â””â”€â”€ storage_switch.sh             # Script to switch between local/cloud storage
```



## Tech Stack

- **ETL Pipeline**: GitHub Actions, Modern OCR tools (Dockling/Marker/Gemini Flash 2)
- **Vector Storage**: Qdrant for embeddings, with Cohere for reranking
- **Agent System**: LangGraph for agentic RAG workflows
- **Backend API**: FastAPI, Python 3.11+
- **Frontend**: Streamlit or Chainlit for MVP
- **Deployment**: Google Cloud Run
- **Package Management**: uv

## Getting Started

### Prerequisites

- Docker and Docker Compose
- Python 3.11+
- GCP account and credentials (for production)
- API keys for OpenAI, Anthropic, etc.

### Environment Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/your-org/growth-lab-deep-search.git
   cd growth-lab-deep-search
   ```

2. Create and configure environment files:
   ```bash
   cp backend/etl/.env.example backend/etl/.env
   cp backend/service/.env.example backend/service/.env
   cp frontend/.env.example frontend/.env
   ```
   
3. Add your API keys and configuration to the `.env` files

### Docker Development Environment

The project uses Docker for consistent development and deployment environments:

1. Start the complete development stack:
   ```bash
   docker-compose up
   ```

2. Access local services:
   - Frontend UI: http://localhost:8501
   - Backend API: http://localhost:8000
   - API Documentation: http://localhost:8000/docs

3. Run individual components:
   ```bash
   # Run only the ETL service
   docker-compose up etl
   
   # Run only the backend service
   docker-compose up service
   
   # Run only the frontend
   docker-compose up frontend
   ```

### Running the ETL Pipeline

The ETL pipeline can be run through Docker in both development and production:

```bash
# Development: Run the ETL pipeline locally
docker-compose run --rm etl python main.py

# Production: Initial data processing on SLURM (HPC environment)
sbatch scripts/slurm_etl_initial.sh

# Test specific ETL components
docker-compose run --rm etl python main.py --component scraper
docker-compose run --rm etl python main.py --component processor
docker-compose run --rm etl python main.py --component embedder
```

After the initial SLURM processing, data is transferred to GCP Cloud Storage, and subsequent ETL runs are automatically scheduled through GitHub Actions and Cloud Run.

## Deployment

### Local to Production Workflow

1. Development occurs in local Docker environment
2. Code is pushed to GitHub
3. GitHub Actions triggers:
   - Code testing
   - Building and publishing container images
   - Deploying to Cloud Run

### Production Infrastructure

- **ETL Pipeline**: Scheduled Cloud Run jobs triggered by GitHub Actions
- **Backend Service**: Cloud Run with autoscaling
- **Vector Database**: Managed Qdrant instance or Qdrant Cloud
- **Document Storage**: Cloud Storage
- **Frontend**: Streamlit or Chainlit

### Deployment Commands

```bash
# Deploy to development environment
./scripts/deploy.sh dev

# Deploy to production environment
./scripts/deploy.sh prod
```

## ğŸ§ª Development Workflow

### Contributing Guidelines

1. Create a feature branch from `main`
2. Implement your changes with tests
3. Submit a pull request for review

### Testing

```bash
# Run tests
pytest

# Run with coverage
pytest --cov=backend
```

### Deployment

Development and production environments are managed through Docker and GitHub Actions:

```bash
# Deploy to development
./scripts/deploy.sh dev

# Deploy to production
./scripts/deploy.sh prod
```


## ğŸ”’ Security & Configuration

- API keys and secrets are managed via `.env` files (not committed to GitHub)
- Production secrets are stored in GCP Secret Manager
- Access control is implemented at the API level

## License

This project is licensed under CC-BY-NC-SA 4.0. See the LICENSE file for details.